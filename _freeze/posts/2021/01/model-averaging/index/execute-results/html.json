{
  "hash": "b6e201f152a53eb3e57e228fce904c2d",
  "result": {
    "markdown": "---\ntitle: \"Model averaging methods: how and why to build ensemble models\"\ndescription: |\n  Averaging predictions for fun and profit -- and for dealing with the uncertainty of model selection. With examples in R!\ncategories: [\"R\", \"Data science\"]\nauthor:\n  - name: Mike Mahoney\n    url: {}\n    orcid_id: \"0000-0003-2402-304X\"\ndate: 01-18-2021\nimage: model_avg.jpg\noutput:\n  distill::distill_article:\n    self_contained: false\n    toc: true\n    includes: \n      after_body: \"../../../utterances.html\"\nbibliography: biblio.bib\n---\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](model_avg.jpg){width=3612}\n:::\n:::\n\n\n\n<aside>\nCartoon components are CC-BY 4.0 Allison Horst ([allison_horst on twitter](https://twitter.com/allison_horst/))\n</aside>\n\n## Introduction\n\nBuilding models is hard. Choosing what models to build can be even harder. With \nseemingly infinite different modeling approaches to select between (and somehow\neven more individual implementations), it can be difficult to guess what \nmethods will be the best fit for your data -- particularly if you're working \nwith data that will change over time with new observations or predictors being\nadded to the mix.\n\nUsually, we disclose this sort of uncertainty with things like confidence \nintervals and standard errors. Yet when it comes to selecting a single model,\nwe often don't discuss how confident we are in that model being the _right_ \none -- instead, we present and report only our final choice as if there was\nno chance other candidate models would be as good or even better fits.\n\nEnsemble models prove a way to deal with that uncertainty [@Wintle]. By \naveraging predictions from a handful of candidate models, ensembles acknowledge \nthat there might be multiple models that could be used to describe our data -- \nand by weighting the average we can communicate how confident we are in each \nindividual model's view of the world. Of course, while \n[this is all nice and flowery, it needs to work too](https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/) -- and\nmodel averaging delivers, typically reducing prediction errors beyond even\nabove even the best individual component model [@Dormann].\n\nThere are a ton of approaches to model \naveraging^[See table 1 of Dormann et. al. for a partial list.]. The rest of this \npost will walk through a few of the simplest -- equal-weight averaging, \nfit-based averages, and model-based combinations -- that you can easily \nimplement yourself without needing to worry about slowing down your iteration \ntime or making your modeling code too complex.\n\n### Getting Started\n\nWe'll be using the following libraries for data manipulation and visualization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\n```\n:::\n\n\nAdditionally, we'll be using both `ranger` and `lightgbm` to develop component\nmodels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\nlibrary(lightgbm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lightgbm'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n:::\n\n\nAnd finally, we need the actual data we're modeling. For this example, we'll \nbuild models predicting the arrival delay of the flights included in the \n`nycflights13` package using both flight details and weather data. This next \nchunk of code will preprocess our data into a model-ready format:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- nycflights13::flights\nweather <- nycflights13::weather %>% \n  select(-wind_gust) # About 80% missing values, so we'll drop this column\n\n# combine the two data frames into one complete set\nflight_data <- flights %>% \n  left_join(weather,\n            by = c(\"year\", \"month\", \"day\", \"origin\", \"hour\", \"time_hour\")) %>% \n  drop_na()\n\nflight_data <- flight_data %>% \n  # Drop 37 pretty dramatic outliers\n  filter(arr_delay <= 500) %>% \n  # Get rid of useless predictors -- \n  # these each cause problems with at least one of our regressions\n  select(-year, -time_hour, -minute) %>% \n  # Skip the work of encoding non-numeric values, to save my poor laptop\n  select_if(is.numeric)\n```\n:::\n\n\nAnd for one final pre-processing step, we'll split our data into training, \nvalidation, and testing sets (sticking 20% into both validation and testing and\ndumping the rest into training). We'll be using model performance against the \nvalidation set to determine weights for our averages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n# Generate a random sequence to subset our data into train/validate/test splits\nrow_index <- sample(nrow(flight_data), nrow(flight_data))\n\n# Testing gets the 20% of data with the highest random index values\nflight_testing <- flight_data[row_index >= nrow(flight_data) * 0.8, ]\n\n# Validation gets the next highest 20%\nflight_validation <- flight_data[row_index >= nrow(flight_data) * 0.6 &\n                                   row_index < nrow(flight_data) * 0.8, ]\n\n# Training gets the rest\nflight_training <- flight_data[row_index < nrow(flight_data) * 0.6, ]\n\n# LightGBM requires matrices, rather than data frames and formulas:\nxtrain <- as.matrix(select(flight_training, -arr_delay))\nytrain <- as.matrix(flight_training[[\"arr_delay\"]])\n\nxvalid <- as.matrix(select(flight_validation, -arr_delay))\nxtest <- as.matrix(select(flight_testing, -arr_delay))\n```\n:::\n\n\nSo with that out of the way, it's time to start training our models! \n\n## Component Models\n\n### Linear Model\n\nLet's start off with a simple linear regression model, using all of our \npredictors in the flight dataset to try and estimate arrival delays:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- lm(arr_delay ~ ., flight_training)\nsummary(linear_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = arr_delay ~ ., data = flight_training)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.895  -9.133  -1.538   7.076 159.388 \n\nCoefficients:\n                  Estimate  Std. Error  t value           Pr(>|t|)    \n(Intercept)    -4.61324673  5.99244467   -0.770           0.441394    \nmonth           0.03825040  0.01117238    3.424           0.000618 ***\nday             0.02220492  0.00410860    5.404 0.0000000650770797 ***\ndep_time        0.00009509  0.00027953    0.340           0.733722    \nsched_dep_time -0.00349249  0.00189448   -1.844           0.065257 .  \ndep_delay       1.01251264  0.00106768  948.332            < 2e-16 ***\narr_time        0.00088164  0.00011818    7.460 0.0000000000000868 ***\nsched_arr_time -0.00471343  0.00014783  -31.884            < 2e-16 ***\nflight         -0.00004692  0.00002541   -1.846           0.064863 .  \nair_time        0.75629859  0.00307431  246.006            < 2e-16 ***\ndistance       -0.09791613  0.00039245 -249.500            < 2e-16 ***\nhour            0.59997173  0.18707035    3.207           0.001341 ** \ntemp            0.11726625  0.02231781    5.254 0.0000001487014873 ***\ndewp            0.03632142  0.02404661    1.510           0.130928    \nhumid           0.01860018  0.01228626    1.514           0.130053    \nwind_dir       -0.00607627  0.00040085  -15.158            < 2e-16 ***\nwind_speed      0.19198999  0.00753768   25.471            < 2e-16 ***\nprecip         26.88470146  3.01386317    8.920            < 2e-16 ***\npressure       -0.01634187  0.00561852   -2.909           0.003631 ** \nvisib          -0.46031686  0.03238825  -14.212            < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.82 on 170687 degrees of freedom\nMultiple R-squared:  0.8707,\tAdjusted R-squared:  0.8707 \nF-statistic: 6.048e+04 on 19 and 170687 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nCool! We have our first model -- and it seems to be a pretty ok fit, with an \nR^2 of 0.87. We could probably make this model better by being a bit more \nselective with our terms or throwing in some interaction terms -- but as a first\nstab at a model that we'll incorporate into our average, this is pretty alright.\n\nOf course, we want to make sure this model can generalize outside of the data it\nwas trained with -- let's use it to make predictions against our validation set, \ntoo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflight_validation$lm_pred <- predict(\n  linear_model,\n  newdata = flight_validation\n)\n\nsqrt(mean((flight_validation$lm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lm_pred, flight_validation))$r.squared\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.73962\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8684178\n```\n:::\n:::\n\n\n\nR^2 remains at about 0.87 and RMSE comes in at about 14.74 minutes -- which, \nconsidering delays in the validation set range from -75 to 485 minutes, feels\nnot too bad for a naively implemented linear model. \n\n### Random Forest\n\nSo we have our first model sorted, but we need more than that to take an \naverage! While we could average out a number of linear models with different \nparameters, it feels more interesting to combine a few different algorithms as\ncomponent models. So let's use [ranger](https://github.com/imbs-hl/ranger) to\nimplement a random forest to represent our data -- fair warning, this one takes\na little while to train!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_model <- ranger::ranger(arr_delay ~ ., data = flight_training)\nsqrt(ranger_model$prediction.error)\nranger_model$r.squared\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11.08573\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9276561\n```\n:::\n:::\n\n\nSo this model has an RMSE of 11.09 and an R^2 \nof 0.93 -- an improvement over our linear model! While we could eke out some \nimprovements with careful tuning, it looks like this version is a good enough \nfit to use as an example in our ensemble. As before, we want to check out how \nwell this model generalizes by using it to generate predictions for our \nvalidation set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_predictions <- predict(\n    ranger_model,\n    data = flight_validation\n  )\n\nflight_validation$ranger_pred <- ranger_predictions$predictions\n\nsqrt(mean((flight_validation$ranger_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ ranger_pred, flight_validation))$r.squared\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.96209\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9302306\n```\n:::\n:::\n\n\nOur model actually performs (extremely) slightly better on the validation set!\n\n### GBM\n\nSo that's two models sorted! For completeness sake, let's implement a third and\nfinal component model, this time using the \n[LightGBM](https://github.com/microsoft/LightGBM/tree/master/R-package) package\nto fit a gradient boosting machine. Similar to the last two, we won't do a ton \nto parameterize this model -- the \nonly change I'll make to the model fit defaults is to use 100 rounds, to let the\nboosting algorithm get into the same performance range as our other two models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlightgbm_model <- lightgbm::lightgbm(xtrain, \n                                     ytrain, \n                                     nrounds = 100, \n                                     obj = \"regression\", \n                                     metric = \"rmse\",\n                                     # Suppress output\n                                     force_col_wise = TRUE,\n                                     verbose = 0L)\n```\n:::\n\n\nThe `lightgbm_model` doesn't have the same easy method for evaluating in-bag\nperformance as our linear model and random forests did. We'll skip right to the \nvalidation set instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflight_validation$lightgbm_pred <- predict(\n  lightgbm_model,\n  xvalid\n)\n\nsqrt(mean((flight_validation$lightgbm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lightgbm_pred, flight_validation))$r.squared\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.4088\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9347398\n```\n:::\n:::\n\n\nSo it looks like LightGBM model performs about as well (if not marginally better\nthan) our random forest! For reference, here are the RMSE values from each of \nour candidate models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_values <- flight_validation %>% \n  # Only select our y and y-hat columns\n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots <- prediction_values %>% \n  pivot_longer(cols = -arr_delay) %>% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, levels = c(\"lightgbm\", \"ranger\", \"lm\")))\n\nprediction_plots %>% \n  group_by(Model = name) %>% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %>% \n  arrange(RMSE) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|Model    |     RMSE|\n|:--------|--------:|\n|lightgbm | 10.40880|\n|ranger   | 10.96209|\n|lm       | 14.73962|\n:::\n:::\n\n\nOf course, individual metrics don't tell the whole story -- it can be helpful to\nlook at diagnostic plots of our predictions to try and understand patterns in \nhow our predictions match the data. For instance, \"linear models are about\nfour minutes worse on average\" is all well and good in the abstract, but \ngraphics like the one below can help us see that -- for instance -- linear \nmodels tend to do a bit worse around 0 minute delays (where most of the data is \nclustered) while our random forest performs worse on higher extremes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_plots %>% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ggplot-1.png){width=672}\n:::\n:::\n\n\n## Model Averaging\n\nWith our candidate models in tow, we're now fully ready to move on to model \naveraging methods! We'll walk through basic implementations of three methods \n(equal weighting, fit-based weights, and model-based estimates) and then \nevaluate our ensembles at the end.\n\n### Equal Weights\n\nPerhaps the most obvious way to average models is to take the simple arithmetic\nmean of your model predictions. This method presupposes that each of your models\nare equally good representations of your underlying data; since that isn't the\ncase here, we might expect this method to not substantially reduce error \noverall.\n\nA benefit of this method, though, is that implementation takes no time at all:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_values <- prediction_values %>% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n```\n:::\n\n\n### Fit-Based Weights\n\nA slightly more involved method is to weight models based on some metric of \ntheir performance. Almost any metric with a standard definition across component\nmodels can be used (so for instance, AIC or BIC with nested models or MSE and \nMAPE); as we've been using RMSE so far, we'll use it to weight our errors.\n\nWeighting models based on fit statistics is also relatively easy in the grand\nscheme of data science. First, calculate the fit statistic for each of your\nmodels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_rmse <- vapply(\n  prediction_values,\n  function(x) sqrt(mean((x - prediction_values$arr_delay)^2)),\n  numeric(1)\n  )[1:3] # Only our 3 component models!\nmodel_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      lm_pred   ranger_pred lightgbm_pred \n     14.73962      10.96209      10.40880 \n```\n:::\n:::\n\n\nThen, depending on your statistic, you may need to take the reciprocal of each\nvalue -- as lower RMSEs are better, we need to do so here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_weights <- (1 / (model_rmse))\n```\n:::\n\n\nLastly, calculate your weights as proportion of the whole set of -- you can view\nthese values as the proportion of the ensemble prediction contributed by each\ncomponent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_weights <- rmse_weights / sum(rmse_weights)\nrmse_weights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      lm_pred   ranger_pred lightgbm_pred \n    0.2659099     0.3575422     0.3765479 \n```\n:::\n:::\n\n\nMaking predictions with the ensemble is then relatively easy -- just multiply\neach of your predicted values by their proportion and sum the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_values <- prediction_values %>% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n```\n:::\n\n\n### Model-Based Weights\n\nThe last averaging method we'll walk through is a little more involved, but \nstill pretty comprehensible: take your model outputs, turn around, and use them\nas model inputs.\n\n![](Inception-image.jpeg)\n\nOur toy example here is a pretty good fit for this method -- we already saw in \nour graphics that a strong linear relationship exists between our predictions\nand the true value, and this relationship is a little different for each model:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nFrom this plot, we can guess that a linear model combining our component \npredictions as features will be a good fit^[No pun intended.] for averaging \nthese models. To do so, we simply need to build a linear model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions_model <- lm(arr_delay ~ lm_pred * ranger_pred * lightgbm_pred, \n                        data = prediction_values)\n```\n:::\n\n\nAnd then use it to generate predictions just like our original component linear\nmodel:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_values$model_based_pred <- predict(\n  predictions_model,\n  newdata = prediction_values\n)\n```\n:::\n\n\nNote that if we saw non-linear relationships between our predictions and true \nvalues, we'd want to rely on non-linear methods to average out predictions; it\njust so happens that our models are already pretty strong fits for the \nunderlying data and can be well-represented with simple linear regression.\n\n## How'd We Do?\n\nNow that we have our ensemble models prepared, it's time to evaluate all of our \nmodels out against our testing set!\n\nThe first step is to generate predictions for the test set using our component\nmodels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflight_testing$lm_pred <- predict(\n  linear_model,\n  newdata = flight_testing\n)\n\nranger_predictions <- predict(\n    ranger_model,\n    data = flight_testing\n  )\n\nflight_testing$ranger_pred <- ranger_predictions$predictions\n\nflight_testing$lightgbm_pred <- predict(\n  lightgbm_model,\n  xtest\n)\n```\n:::\n\n\nWe can use those predictions to generate our ensemble predictions. Note that \nwe're still using the weights and models calibrated on the validation data -- \nwe (theoretically) shouldn't know the \"true\" values for the test set, so we \ncan't re-weight our averages now!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflight_testing <- flight_testing %>% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\nflight_testing <- flight_testing %>% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\nflight_testing$model_based_pred <- predict(\n  predictions_model,\n  newdata = flight_testing\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nSo how'd we do? Let's check out the RMSE for each of our models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_values <- flight_testing %>% \n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots <- prediction_values %>% \n  pivot_longer(cols = -arr_delay) %>% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, \n                       levels = c(\"lightgbm\", \"ranger\", \"lm\",\n                                  \"model_based\", \"fit_based\", \"equal_weight\")))\n\nprediction_plots %>% \n  group_by(Model = name) %>% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %>% \n  arrange(RMSE) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|Model        |      RMSE|\n|:------------|---------:|\n|model_based  |  9.492409|\n|lightgbm     | 10.290113|\n|ranger       | 10.968544|\n|fit_based    | 11.057728|\n|equal_weight | 11.311836|\n|lm           | 14.621943|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_plots %>% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nCool -- our model-based ensemble actually performed better than any of the \ncomponents! While the equal weight and fit-based averages were pretty \nmiddle-of-the-road, in other settings these methods can also help to reduce bias\nin predictions and produce estimates with less variance than any of the \ncomponent models.\n\n## Conclusion\n\nModel averaging can be a powerful tool for reducing model bias and addressing \nthe implicit uncertainty in attempting to pick the \"best\" model for a situation.\nWhile plenty of complex and computationally expensive approaches to averaging \nexist -- and can greatly improve model performance -- simpler ensemble methods\ncan provide the same benefits without necessarily incurring the same costs. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}