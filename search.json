[
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Papers\n\n\n\n\nPapers\n\nClick “[PDF]” to download each paper.\n\n\n2023\n\n\nJohnson, L. K., Mahoney, M. J., Desrochers, M. L., and Beier, C. M. In Review. Mapping historical forest biomass for stock-change assessments at parcel to landscape scales. https://doi.org/10.48550/arXiv.2304.02632 [PDF]\n\n\nMahoney, M. J. In Review. waywiser: Ergonomic methods for assessing spatial models. https://arxiv.org/abs/2303.11312 [PDF]\n\n\nMahoney, M. J., Johnson, L. K., Silge, J., Frick, H., Kuhn, M., and Beier, C. M. In Review. Assessing the performance of spatial cross-validation approaches for models of spatially structured data. https://arxiv.org/abs/2303.07334 [PDF]\n\n\nMahoney, M. J., Johnson, L. K., and Beier, C. M. 2023. AI for shrubland identification and mapping. In Sun Z, Cristea N, Rivas P (eds.), Artificial Intelligence in Earth Science, 295-316. Elsevier. ISBN 978-0-323-91737-7. https://doi.org/10.1016/B978-0-323-91737-7.00010-4 [PDF]\n\n\n2022\n\n\nMahoney, M. J., Johnson, L. K., Guinan, A. Z., and Beier, C. M. 2022. Classification and mapping of low‑statured ’shrubland’ cover types in post‑agricultural landscapes of the US Northeast. The International Journal of Remote Sensing, 43(19‑24), 7117‑7138. https://doi.org/10.1080/01431161.2022.2155086 [PDF]\n\n\nJohnson, L. K., Mahoney, M. J., Bevilacqua, E., Stehman, S. V., Domke, G. M., and Beier, C. M. In Review. Fine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages. International Journal of Applied Earth Observation and Geoinformation 114: 103059. https://doi.org/10.1016/j.jag.2022.103059 [PDF]\n\n\nMahoney, M. J., Johnson, L. K., Bevilacqua, E., and Beier, C. M. 2022. Ground noise filtering produces inferior models of forest aboveground biomass. GIScience and Remote Sensing 59(1): 1266-1280. https://doi.org/10.1080/15481603.2022.2103069 [PDF]\n\n\nMahoney, M. J., Beier, CM, and Ackerman, AC. 2022. unifir: A Unifying API for Interacting with Unity from R. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388 [PDF]\n\n\nTamiminia, H., Salehi, B., Mahdianpari, M., Beier, C. M., Johnson, L. K., Phoenix, D. B., and, Mahoney, M. J. 2022. Decision tree-based machine learning models for above-ground biomass estimation using multi-source remote sensing data and object-based image analysis. Geocarto International. https://doi.org/10.1080/10106049.2022.2071475\n\n\nMahoney, M. J., Beier, CM, and Ackerman, AC. 2022. terrainr: An R package for creating immersive virtual environments. Journal of Open Source Software 7(69): 4060. https://doi.org/10.21105/joss.04060 [PDF]\n\n\n2021\n\n\nMahoney, M. J., Beier, CM, and Ackerman, AC. 2021. Interactive landscape simulations for visual resource assessment. VRSC 2021 Conference Proceedings. [PDF]\n\n\n2020\n\n\nMahoney, M. J., and Stella, JC. 2020. Stem size selectivity is stronger than species preferences for beaver, a central place forager. Forest Ecology and Management 475: 118331. https://doi.org/10.1016/j.foreco.2020.118331 [PDF]"
  },
  {
    "objectID": "papers/ground_filtering/index.html",
    "href": "papers/ground_filtering/index.html",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass",
    "section": "",
    "text": "Read the paper!\n\n\nSee the poster!\n\nAirborne LiDAR has become an essential data source for large-scale, high-resolution modeling of forest biomass and carbon stocks, enabling predictions with much higher resolution and accuracy than can be achieved using optical imagery alone. Ground noise filtering – that is, excluding returns from LiDAR point clouds based on simple height thresholds – is a common practice meant to improve the ‘signal’ content of LiDAR returns by preventing ground returns from masking useful information about tree size and condition contained within canopy returns. Although this procedure originated in LiDAR-based estimation of mean tree and canopy height, ground noise filtering has remained prevalent in LiDAR pre-processing, even as modelers have shifted focus to forest aboveground biomass (AGB) and related characteristics for which ground returns may actually contain useful information about stand density and openness. In particular, ground returns may be helpful for making accurate biomass predictions in heterogeneous landscapes that include a patchy mosaic of vegetation heights and land cover types.\nIn this paper, we applied several ground noise filtering thresholds while mapping two study areas in New York (USA), one a forest-dominated area and the other a mixed-use landscape. We observed that removing ground noise via any height threshold systematically biases many of the LiDAR-derived variables used in AGB modeling. By fitting random forest models to each of these predictor sets, we found that that ground noise filtering yields models of forest AGB with lower accuracy than models trained using predictors derived from unfiltered point clouds. The relative inferiority of AGB models based on filtered LiDAR returns was much greater for the mixed land-cover study area than for the contiguously forested study area. Our results suggest that ground filtering should be avoided when mapping biomass, particularly when mapping heterogeneous and highly patchy landscapes, as ground returns are more likely to represent useful ‘signal’ than extraneous ‘noise’ in these cases.\n\n\nFigure 1: Distributions of common LiDAR-derived metrics (including density percentiles, decile heights, L-moments, and quadratic mean height) for the pooled dataset at various levels of ground noise filtering. Filtering reduces the variance in many metrics, reducing the total amount of information available to models.\n\n\nFigure 2: Height threshold-based filtering of LiDAR returns produces inferior models across all landscape types, with more notable impacts in mixed-use landscapes"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Counting the number of times each citation is used in a Quarto document\n\n\n\n\n\n\n\nQuarto\n\n\nTutorials\n\n\nData science\n\n\n\n\nDocumenting this trick here for future me, six months from now.\n\n\n\n\n\n\nApr 10, 2023\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nUsing in-line grouping to fit many models\n\n\n\n\n\n\n\nR\n\n\nTutorials\n\n\nData science\n\n\n\n\nAn alternative to nesting for fitting separate models to multiple groups.\n\n\n\n\n\n\nApr 1, 2023\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nwaywiser is now a part of rOpenSci\n\n\n\n\n\n\n\nR\n\n\nwaywiser\n\n\nSpatial\n\n\nData science\n\n\nR packages\n\n\n\n\nPlus version 0.3.0 now on CRAN, and a new preprint\n\n\n\n\n\n\nMar 23, 2023\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nHow to run R jobs across multiple (local) computers\n\n\n\n\n\n\n\nR\n\n\nTutorials\n\n\nSpatial\n\n\nData science\n\n\n\n\nTIL this extremely, extremely easy thing\n\n\n\n\n\n\nMar 3, 2023\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nCommands I Use\n\n\n\n\n\n\n\nshell\n\n\n\n\nEveryone else was jumping off the bridge, so I did too.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nspatialsample 0.3.0 is now on CRAN\n\n\n\n\n\n\n\nR\n\n\nspatialsample\n\n\ntidymodels\n\n\nR packages\n\n\ngeospatial data\n\n\n\n\nRepeats, standardization, repeats, spatial clustering, repeats, and more.\n\n\n\n\n\n\nJan 17, 2023\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nClassification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast\n\n\n\n\n\n\n\necology\n\n\npapers\n\n\nremote sensing\n\n\nmachine learning\n\n\nearth science\n\n\nenvironmental science\n\n\nshrubland\n\n\n\n\nNew article out in the International Journal of Remote Sensing!\n\n\n\n\n\n\nDec 21, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nLegibility and seats at the table\n\n\n\n\n\n\n\nData science\n\n\necology\n\n\nearth science\n\n\n\n\nReflections on FEMC 2022.\n\n\n\n\n\n\nDec 16, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nThe tools I loved this year\n\n\n\n\n\n\n\nR\n\n\nSpatial\n\n\nData science\n\n\n\n\nThe void yells back.\n\n\n\n\n\n\nDec 12, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nHow to include sf data in R packages\n\n\n\n\n\n\n\nR\n\n\nTutorials\n\n\nSpatial\n\n\nData science\n\n\n\n\nAn extremely opinionated guide for an extremely common problem.\n\n\n\n\n\n\nDec 1, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nFine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages\n\n\n\n\n\n\n\necology\n\n\npapers\n\n\nremote sensing\n\n\nmachine learning\n\n\nearth science\n\n\nAGB\n\n\n\n\nNew (open-access!) article out in International Journal of Applied Earth Observation and Geoinformation!\n\n\n\n\n\n\nNov 7, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nHow rsample keeps memory usage low\n\n\n\n\n\n\n\nR\n\n\nrsample\n\n\ntidymodels\n\n\n\n\nCopy-on-modify is pretty neat.\n\n\n\n\n\n\nOct 4, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nI know what I did last summer\n\n\n\n\n\n\n\nR\n\n\nInternship\n\n\nQuarto\n\n\n\n\nAn RStudio internship retrospective\n\n\n\n\n\n\nAug 12, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nWaywiser version 0.1.0 is now on CRAN!\n\n\n\n\n\n\n\nR\n\n\nwaywiser\n\n\ntidymodels\n\n\nR packages\n\n\ngeospatial data\n\n\n\n\nA new {yardstick} extension package for calculating spatial autocorrelation in model residuals.\n\n\n\n\n\n\nAug 11, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes\n\n\n\n\n\n\n\necology\n\n\npapers\n\n\nremote sensing\n\n\nmachine learning\n\n\n\n\nNew article out in GIScience & Remote Sensing\n\n\n\n\n\n\nAug 10, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nHow to use Quarto for Parameterized Reporting\n\n\n\n\n\n\n\nR\n\n\nTutorials\n\n\nQuarto\n\n\n\n\nYou know. If you wanna.\n\n\n\n\n\n\nAug 5, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nProgress, Purpose, Process\n\n\n\n\n\nReflecting on another year.\n\n\n\n\n\n\nFeb 18, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nterrainr: An R package for creating immersive virtual environments\n\n\n\n\n\nNew open-access paper published in the Journal of Open Source Science.\n\n\n\n\n\n\nJan 14, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nAutomated {drat} uploads with GitHub Actions\n\n\n\n\n\n\n\nR\n\n\nTutorials\n\n\nCI\n\n\nCD\n\n\nGitHub Actions\n\n\n\n\nContinuously deploy your code to personal CRAN-like repos, automatically and for free!\n\n\n\n\n\n\nSep 23, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nInteractive landscape simulations for visual resource assessment\n\n\n\n\n\nNew preprint for the 2021 Visual Resources Stewardship Conference.\n\n\n\n\n\n\nJun 14, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nVirtual environments talk at useR! 2021\n\n\n\n\n\nWith slides and a rough outline available on GitHub.\n\n\n\n\n\n\nJun 12, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s new in terrainr 0.4.0?\n\n\n\n\n\nThe new CRAN release of terrainr improves consistency, CRS logic, and fixes some bugs.\n\n\n\n\n\n\nApr 22, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nterrainr 0.3.0 is out today\n\n\n\n\n\n\n\nR\n\n\nData science\n\n\nterrainr\n\n\n\n\nNew version, who this?\n\n\n\n\n\n\nFeb 17, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nModel averaging methods: how and why to build ensemble models\n\n\n\n\n\n\n\nR\n\n\nData science\n\n\n\n\nAveraging predictions for fun and profit – and for dealing with the uncertainty of model selection. With examples in R!\n\n\n\n\n\n\nJan 18, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nSome Updates\n\n\n\n\n\n\n\nR\n\n\nTwitter\n\n\nphd\n\n\nterrainr\n\n\nbeaver\n\n\n\n\nWhere Do We Come From? What Are We? Where Are We Going?\n\n\n\n\n\n\nOct 16, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nMake a Retweet Bot in R\n\n\n\n\n\n\n\nR\n\n\nTwitter\n\n\necology_tweets\n\n\n\n\nY’know. If you wanna.\n\n\n\n\n\n\nJun 20, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nInstalling the TIG stack on Raspberry Pi\n\n\n\n\n\n\n\nRaspberry Pi\n\n\nTutorials\n\n\nData Visualization\n\n\nMonitoring\n\n\nTelegraf\n\n\nInfluxDB\n\n\nGrafana\n\n\n\n\nA guide to installing InfluxDB, Telegraf, and Grafana on a Raspberry Pi 4 running Raspbian Buster. Unlike every other guide like this on the internet, this one works.\n\n\n\n\n\n\nMay 3, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nA minimalist visualization of Coronavirus rates\n\n\n\n\n\n\n\nR\n\n\n\n\nMade with Shiny and R\n\n\n\n\n\n\nApr 27, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nMaking Excellent Visualizations\n\n\n\n\n\n\n\nData Visualization\n\n\nTutorials\n\n\n\n\nPart 3 in the data visualization series\n\n\n\n\n\n\nApr 22, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nMechanics of Data Visualizations\n\n\n\n\n\n\n\nData Visualization\n\n\nTutorials\n\n\n\n\nPart 2 in the data visualization series\n\n\n\n\n\n\nApr 21, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nTheory of Data Visualizations\n\n\n\n\n\n\n\nData Visualization\n\n\nTutorials\n\n\n\n\nPart 1 in the data visualization series\n\n\n\n\n\n\nApr 20, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nAnnouncing {spacey}, now on CRAN!\n\n\n\n\n\n\n\nR\n\n\nR Packages\n\n\nmaps\n\n\nspacey\n\n\ngeospatial data\n\n\n\n\nUSGS data access and rayshader maps, done cheap.\n\n\n\n\n\n\nMar 24, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nAnnouncing {heddlr}, now on CRAN!\n\n\n\n\n\n\n\nR\n\n\nR Packages\n\n\nR Markdown\n\n\n\n\nWrite less boilerplate, get more done.\n\n\n\n\n\n\nJan 23, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nThesis Now Available in ESF Digital Commons\n\n\n\n\n\n\n\nPublications\n\n\nBeaver\n\n\n\n\nIt’s a real humdinger.\n\n\n\n\n\n\nMar 27, 2019\n\n\nMike Mahoney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cdi/index.html",
    "href": "cdi/index.html",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "",
    "text": "This is an R Markdown document containing code for the workshop “Accessing the USGS National Map and making 3D maps with terrainr”, held virtually on 2021-05-28. If you want to follow along with the workshop as we go, click this link to download the R Markdown notebook.\nIf you’re not familiar with R Markdown, this document lets us write both plain text and code in a single document. Document sections inside three ` marks are called “code chunks”:\nplot(Orange)\nA single line in a code chunk can be run by pressing Control+Enter with your cursor on the line. The entire chunk can be run by pressing Control+Shift+Enter with your cursor inside the chunk."
  },
  {
    "objectID": "cdi/index.html#example",
    "href": "cdi/index.html#example",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "Example",
    "text": "Example\nNow that we’ve seen a basic workflow using terrainr to pull layers from the National Map and plot with ggplot2, let’s have some fun with it! Starting with the latitude and longitude coordinates for a new site, we’ll create an sf object and add a spatial buffer around it like before.\n\n# yosemite NP\nsite &lt;- data.frame(lat = 37.7456, lng = -119.5521)\n\n# convert to sf object and assign coordinate reference system \nsite_sf &lt;- st_as_sf(site, coords = c(\"lng\",\"lat\"), crs = 4326)\n\n# buffer around point\nsite_bbox &lt;- set_bbox_side_length(site_sf, 10, \"km\")\n\nNow, to pull some data using terrainr. In the previous example we looked at elevation and orthoimagery. Other supported services are: nhd, govunits, contours, geonames, NHDPlus_HR, structures, transportation, and wbd. More info here: https://docs.ropensci.org/terrainr/#available-datasets\nLet’s try elevation and contours this time:\n\nwith_progress(\n  site_tiles &lt;- get_tiles(site_bbox, \n                            output_prefix = \"yosemite_np_5m\",\n                            services = c(\"elevation\", \"contours\"),\n                            resolution = 5)\n)\n\nsite_tiles\n\n$elevation\n[1] \"yosemite_np_5m_3DEPElevation_1_1.tif\"\n\n$contours\n[1] \"yosemite_np_5m_contours_1_1.tif\"\n\n\nNow we can convert the elevation and contour layers to plot with ggplot2. The contours layer is an RGBA multi-band raster layer, similar to the orthoimagery.\n\nelevation &lt;- raster(site_tiles$elevation)\ncontours &lt;- stack(site_tiles$contours)\n\n# look at structure of contours raster - has 4 layers\ncontours\n\nclass      : RasterStack \ndimensions : 2000, 1999, 3998000, 4  (nrow, ncol, ncell, nlayers)\nresolution : 0.00005689511, 0.00005689511  (x, y)\nextent     : -119.6089, -119.4952, 37.68869, 37.80248  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nnames      :     lyr.1,     lyr.2,     lyr.3,     lyr.4 \nmin values : 0.6000000, 0.4117647, 0.2235294, 0.0000000 \nmax values :         1,         1,         1,         1 \n\n\nRGBA has 4 channels: red, green, blue, and alpha. The 4th channel, alpha, is like opacity. In this case, alpha is 0 in cells without contour lines and 1 where contours exist.\n\nggplot() + \n  geom_spatial_rgb(data = contours,\n                   mapping = aes(x, y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326) + \n  theme_void()\n\n\n\n\nContours reflect elevation, so let’s make it so! The next chunk stacks elevation and contours together and coverts to a dataframe to plot using geom_raster with ggplot2. Then, using the alpha channel, the dataframe is filtered to just the contour lines, and elevation values are used in the color scale.\n\ncontour_stack &lt;- stack(elevation, contours)\ncontour_df &lt;- as.data.frame(contour_stack, xy = TRUE)\n\nnames(contour_df) &lt;- c(\"x\", \"y\", \"z\", \"r\", \"g\", \"b\", \"a\") # longitude, latitude, elevation, red, green, blue, alpha\n\ncontour_lines &lt;- contour_df[contour_df$a != 0, ]\n\n## plot it!\nggplot(data = contour_lines)+\n  geom_raster(mapping = aes(x, y, fill = z))+\n  theme_void()+\n  theme(plot.background = element_rect(fill=\"black\"),\n        plot.title = element_text(color=\"white\"),\n        legend.position = \"none\")+\n  coord_sf(crs = 4326)+\n  scale_fill_scico(palette=\"imola\", direction = 1)+\n  ggtitle(paste(\"Half dome\", site$lat, site$lng))\n\n\n\n\nThis chart uses the scico package for color scale. Scico (https://github.com/thomasp85/scico) offers a selection of color palettes developed for scientific applications. They are perceptually uniform to represent data fairly, and universally readable by both color-vision deficient and color-blind people.\n\nscico_palette_show() # to see all palette options\n\n\n\n\nFor the purposes of the workshop, we’re avoiding large downloads. But if we were to increase the resolution even more, say to 1m, terrainr might break up the focal region into multiple tiles:\n\n## warning: running this code chunk will take over 5 minutes to complete\nwith_progress(\n  site_tiles_1 &lt;- get_tiles(site_bbox, \n                            output_prefix = \"yosemite_np_10m\",\n                            services = c(\"elevation\", \"contours\"),\n                            resolution = 1)\n)\n\nsite_tiles_1\n\nIn which case they can be merged together using merge_rasters\n\nmerge_el &lt;- terrainr::merge_rasters(site_tiles_1$elevation, \n                          tempfile(fileext = \".tif\"))\n\nmerge_co &lt;- terrainr::merge_rasters(site_tiles_1$contours, \n                          tempfile(fileext = \".tif\"))"
  },
  {
    "objectID": "cdi/index.html#on-your-own",
    "href": "cdi/index.html#on-your-own",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "On your own",
    "text": "On your own\nUse the dataframe below to pull new starting coordinates and recreate the map in a new location, with new layer, or maybe a new color palette. Share what you made in the chat!\n\n# dataframe of cool places in the US\nplaces &lt;- structure(list(name = c(\"half dome\", \"moki dugway\", \"badwater basin\", \n\"mount whitney\", \"monument valley navajo tribal park\", \"devil's lake\", \n\"cadillac mountain\", \"mcafee knob\", \"sleeping bear dunes \", \"grand canyon\", \n\"miguel's pizza at red river gorge\", \"between cumberland and blue ridge mountains\", \n\"missouri river floodplain and bluff\", \"shawangunk mountains\", \n\"lake winnipesaukee, new hampshire\", \"mount washington, new hampshire\", \n\"gravity hill, pa\", \"orcas island\", \"grand canyon of yellowstone\", \n\"mauna loa\", \"tetons\", \"rocky mountain biological station\"), \n    lat = c(37.746036, 37.302306, 36.250278, 36.578581, 36.983333, \n    43.420033, 44.35127, 37.392487, 44.878727, 36.2388, 37.783004, \n    37.1177098, 38.568762, 41.72469, 43.606096, 44.270504, 40.153283, \n    48.654167, 44.895556, 19.479444, 43.75, 38.958611), lng = c(-119.53294, \n    -109.968944, -116.825833, -118.291995, -110.100411, -89.737405, \n    -68.22649, -80.036298, -86.066458, -112.350427, -83.682861, \n    -81.132816, -90.883408, -74.204946, -71.338624, -71.303115, \n    -76.839954, -122.938333, -110.389444, -155.602778, -110.833333, \n    -106.987778)), class = \"data.frame\", row.names = c(NA, -22L\n))\n\n# randomly pick one row from the dataframe above\nsite &lt;- places[sample(nrow(places), 1), ]\n\n# Or set your own by changing the values below and uncommenting\n# site &lt;- data.frame(ï..name = \"skywalk @ grand canyon\", lat = 36.011827, lng = -113.810931) \n\n# convert to sf object and assign coordinate reference system \nsite_sf &lt;- st_as_sf(site, coords = c(\"lng\",\"lat\"), crs = 4326)\n\n# buffer around point\nsite_bbox &lt;- set_bbox_side_length(site_sf, 10, \"km\")\n\n# keep going! Using code from previous steps, make the rest of the map.\n\n# Create site name for use as filenames:\nsite_fn &lt;- gsub(\" \", \"_\", gsub(\"[[:punct:]]\", \"\", site$name))\n\n# Download the appropriate data for your site from The National Map\nwith_progress(\n  site_tiles &lt;- get_tiles(site_bbox, \n                          output_prefix = sprintf(\"%s_5m\", site_fn),\n                          services = c(\"elevation\", \"contours\"),\n                          resolution = 5)\n)\n\n# COnvert to a raster stack\nelevation &lt;- raster(site_tiles$elevation)\ncontours &lt;- stack(site_tiles$contours)\ncontour_stack &lt;- stack(elevation, contours)\ncontour_df &lt;- as.data.frame(contour_stack, xy = TRUE)\nnames(contour_df) &lt;- c(\"x\", \"y\", \"z\", \"r\", \"g\", \"b\", \"a\") # longitude, latitude, elevation, red, green, blue, alpha\n\n# Remove zero elevations\ncontour_lines &lt;- contour_df[contour_df$a != 0, ]\n\n## plot it!\nggplot(data = contour_lines)+\n  geom_raster(mapping = aes(x, y, fill = z))+\n  theme_void()+\n  theme(plot.background = element_rect(fill=\"black\"),\n        plot.title = element_text(color=\"white\"),\n        legend.position = \"none\")+\n  coord_sf(crs = 4326)+\n  # Change the color palette used by editing the `palette` argument below!!\n  scale_fill_scico(palette=\"imola\", direction = 1)+\n  ggtitle(paste(site$name, site$lat, site$lng))\n\n\n\n# If you want to save a higher resolution version (it makes the lines a little sharper)\n# Run this after printing your plot:\n# ggsave(sprintf(\"whimsical_topomap_terrainr_%s.png\", site_fn), width=5, height=8)"
  },
  {
    "objectID": "portfolio/index.html",
    "href": "portfolio/index.html",
    "title": "\nVisuals\n",
    "section": "",
    "text": "Mike Mahoney: Visual Art\n\n\n\n\n\n\n\n\nVisuals\n\n\nClick any image to see a larger version. Click on the right half of the image (or press the right arrow key) to see the next image; click the left half (or press the left arrow) to go back.\n\n\nJump to section: maps (digital maps, plotter drawings, 3D), generative (trees, curves).\n\n\n\n\n\nMaps\n\n\nDigital Maps\n\n\n                  \n\n\n3D Maps\n\n\n        \n\n\nPlotter Drawings\n\n\nDrawn with an AxiDraw V3 pen plotter.\n\n\n \n\n\n\nGenerative\n\n\nHave a Tree\n\n\nGeometric patterns made by growing lines at various angles and decay rates. Play with the system for yourself at this link!\n\n\n    \n\n\nCurves\n\n\n\n\n                                \n\n\nMiscellaneous"
  },
  {
    "objectID": "quotes/index.html",
    "href": "quotes/index.html",
    "title": "Quotes",
    "section": "",
    "text": "I admit that I am perhaps missing part of a larger picture that encourages or even requires the use of cop shit in the classroom. Yet when my students come to me with reams of documentation for a simple cold, or fearful that I will dole out unexcused absences for them taking job interviews, I have to wonder what that larger picture could be. I’ve only taught for a few semesters and I don’t pretend that my experiences are generalizable to the level of empirical data, but my students seem to do perfectly well in the absence—as much as I can make—of cop shit.\nSo why do we have cop shit in our classrooms?\nOne provisional answer is that the people who sell cop shit are very good at selling cop shit, whether that cop shit takes the form of a learning management system or a new pedagogical technique. Like any product, cop shit claims to solve a problem. We might express that problem like this: the work of managing a classroom, at all its levels, is increasingly complex and fraught, full of poorly defined standards, distractions to our students’ attentions, and new opportunities for grift. Cop shit, so cop shit argues, solves these problems by bringing order to the classroom. Cop shit defines parameters. Cop shit ensures compliance. Cop shit gives students and teachers alike instant feedback in the form of legible metrics.\nIn short, cop shit operates according the the logic of datafication.\n[…]\nCop shit is seductive. It makes metrics transparent. It allows for the clear progress toward learning objectives. It also subsumes education within a market logic. “Here,” cop shit says, “you will learn how to do this thing. We will know you learned it by the acquisition of this gold star. But in order for me to award you this gold star, I must parse you, sense you, track you, collect you, and—” here’s the key, “I will presume that you will attempt to flout me at every turn. We are both scamming each other, you and I, and I intend to win.” When a classroom becomes adversarial, of course, as cop shit presumes, then there must be a clear winner and loser. The student’s education then becomes not a victory for their own self-improvement or -enrichment, but rather that the teacher conquered the student’s presumed inherent laziness, shiftiness, etc. to instill some kernel of a lesson.\n— Jeffrey Moro: Against Cop Shit\n\nEngineers working for Evil, Inc. are like engineers anywhere; they include a mix of people inclined to doing quick hardcoded scripts or perhaps writing Fully General Frameworks For Exploitation Of The Global Economy. The first group ends up doing far more damage because the second group, predictably, doesn’t ship much usable software.\n— Patrick McKenzie: Credit cards as a legacy system\n\n1957 - John Backus and IBM create FORTRAN. There’s nothing funny about IBM or FORTRAN. It is a syntax error to write FORTRAN while not wearing a blue tie.\n[…]\n1972 - Dennis Ritchie invents a powerful gun that shoots both forward and backward simultaneously. Not satisfied with the number of deaths and permanent maimings from that invention he invents C and Unix.\n[…]\n1983 - In honor of Ada Lovelace’s ability to create programs that never ran, Jean Ichbiah and the US Department of Defense create the Ada programming language. In spite of the lack of evidence that any significant Ada program is ever completed historians believe Ada to be a successful public works project that keeps several thousand roving defense contractors out of gangs.\n[…]\n1990 - A committee formed by Simon Peyton-Jones, Paul Hudak, Philip Wadler, Ashton Kutcher, and People for the Ethical Treatment of Animals creates Haskell, a pure, non-strict, functional language. Haskell gets some resistance due to the complexity of using monads to control side effects. Wadler tries to appease critics by explaining that “a monad is a monoid in the category of endofunctors, what’s the problem?”\n[…]\n1996 - James Gosling invents Java. Java is a relatively verbose, garbage collected, class based, statically typed, single dispatch, object oriented language with single implementation inheritance and multiple interface inheritance. Sun loudly heralds Java’s novelty.\n2001 - Anders Hejlsberg invents C#. C# is a relatively verbose, garbage collected, class based, statically typed, single dispatch, object oriented language with single implementation inheritance and multiple interface inheritance. Microsoft loudly heralds C#’s novelty.\n— James Iry\n\nClimate is what on average we may expect; weather is what we actually get.\n— Andrew John Herbertson, Outlines of Physiography (1901)\n\nWe are as gods. We may as well get good at it.\n— Stewart Brand\n\nWithout data, you’re just another person with an opinion\n— W. Edwards Deming\n\nWithout data, you’re just another person with an opinion. […] With data, you’re still just another person with an opinion. Expert analysts understand this in their very bones.\n— Cassie Kozyrkov\n\nThe three golden rules to ensure computer security are: do not own a computer; do not power it on; and do not use it.\n— Robert Morris\n\nMy second meta-principle of statistics is the methodological attribution problem, which is that the many useful contributions of a good statistical consultant, or collaborator, will often be attributed to the statistician’s methods or philosophy rather than to the artful efforts of the statistician himself or herself. Don Rubin has told me that scientists are fundamentally Bayesian (even if they do not realize it), in that they interpret uncertainty intervals Bayesianly. Brad Efron has talked vividly about how his scientific collaborators find permutation tests and p-values to be the most convincing form of evidence. Judea Pearl assures me that graphical models describe how people really think about causality. And so on. I am sure that all these accomplished researchers, and many more, are describing their experiences accurately. Rubin wielding a posterior distribution is a powerful thing, as is Efron with a permutation test or Pearl with a graphical model, and I believe that (a) all three can be helping people solve real scientific problems, and (b) it is natural for their collaborators to attribute some of these researchers’ creativity to their methods.\nThe result is that each of us tends to come away from a collaboration or consulting experience with the warm feeling that our methods really work, and that they represent how scientists really think. In stating this, I am not trying to espouse some sort of empty pluralism — the claim that, for example, we would be doing just as well if we were all using fuzzy sets, or correspondence analysis, or some other obscure statistical method. There is certainly a reason that methodological advances are made, and this reason is typically that existing methods have their failings. Nonetheless, I think we all have to be careful about attributing too much from our collaborators’ and clients’ satisfaction with our methods.\n— Andrew Gelman: Bayesian Statistics Then and Now\n\nEven one of the most elementary things taught on a statistics course, the standard deviation, is more complex than it need be, and is considered here as an example of how convenience for mathematical manipulation often over-rides pragmatism in research methods.\n[…]\nIn essence, the claim made for the standard deviation is that we can compute a number (SD) from our observations that has a relatively consistent relationship with a number computed in the same way form the population figures. This claim, in itself, is of no great value. Reliability alone does not make that number of any valid use. For example, if the computation led to a constant whatever figures were used then there would be a perfectly consistent relationship between the parameters for the sample and population. But to what end? Surely the key issue is not how stable the statistic but whether it encapsulates what we want it to.\n[…]\nOf course, much of the rest of traditional statistics is now based on the standard deviation, but it is important to realise that it need not be.\n— Stephen Gorard: Revisiting a 90-Year-Old Debate: The Advantages of the Mean Deviation\n\nOpinionated software is only cool if you have cool opinions\n— Tom MacWright: soapbox: longitude, latitude is the right way\n\nThere was some discussion in the comments thread here about axis labels and zero. Sometimes zero has no particular meaning (for example when graphing degrees Fahrenheit), but usually it has a clear interpretation, in which case it can be pleasant to include it on the graph. On the other hand, if you’re plotting something that varies from, say 184 to 197, it would typically be a bad idea to extend the axis all the way to zero, as this would destroy your visual resolution.\nThe advice we usually give is: If zero is in the neighborhood, invite it in.\n— Andrew Gelman: Graphing advice\n\nYoung man, in mathematics you don’t understand things. You just get used to them.\n— John Von Neumann\n\nTo deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say ‘fourteen’ to yourself very loudly. Everyone does it.\n— Geoffrey Hinton: A geometrical view of perceptrons\n\nDependencies are an open invitation for other people to break your code.\n— Nathan Eastwood: useR 2021 talk on poorman\n\nRAM is cheap and thinking hurts.\n— Uwe Ligges: R-Help\n\nSoftware people are not alone in facing complexity. Physics deals with terribly complex objects even at the “fundamental” particle level. The physicist labors on, however, in a firm faith that there are unifying principles to be found, whether in quarks or in unified field theories. Einstein repeatedly argued that there must be simplified explanations of nature, because God is not capricious or arbitrary.\nNo such faith comforts the software engineer. Much of the complexity he must master is arbitrary complexity, forced without rhyme or reason by the many human institutions and systems to which his interfaces must confirm. These differ from interface to interface, and from time to time, not because of necessity but only because they were designed by different people, rather than by God.\n— Frederick P. Brooks, Jr.: No Silver Bullet —- Essence and Accident in Software Engineering\n\nOn-prem is a lock-in. Cloud is a lock-in. Every single language you program in is a type of lock-in. Python is easy to get started with, but soon you run into packaging issues and are optimizing the garbage collector. Scala is great, but everyone winds up migrating away from it. And on and on.\nEvery piece of code written in a given language or framework is a step away from any other language, and five more minutes you’ll have to spend migrating it to something else. That’s fine. You just have to decide what you’re willing to be locked into.\n— Vicki Boykis: Commit to your lock-in\n\nI believe we need a ‘Digital Earth’. A multi-resolution, three-dimensional representation of the planet, into which we can embed vast quantities of georeferenced data.\n[…]\nImagine, for example, a young child going to a Digital Earth exhibit at a local museum. After donning a head-mounted display, she sees Earth as it appears from space. Using a data glove, she zooms in, using higher and higher levels of resolution, to see continents, then regions, countries, cities, and finally individual houses, trees, and other natural and man-made objects. Having found an area of the planet she is interested in exploring, she takes the equivalent of a ‘magic carpet ride’ through a 3-D visualization of the terrain. Of course, terrain is only one of the many kinds of data with which she can interact.\n[…]\nShe is able to request information on land cover, distribution of plant and animal species, real-time weather, roads, political boundaries, and population.\n— Al Gore: The Digital Earth: Understanding our Planet in the 21st Century, 1998\n\nTo consult the statistician after an experiment is ﬁnished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.\n— R.A. Fisher\n\nHiawatha, mighty hunter,\nHe could shoot ten arrows upward,\nShoot them with such strength and swiftness\nThat the last had left the bow-string\nEre the first to earth descended.\n- This was commonly regarded\nAs a feat of skill and cunning.\nSeveral sarcastic spirits\nPointed out to him, however,\nThat it might be much more useful\nIf he sometimes hit the target.\n“Why not shoot a little straighter\nAnd employ a smaller sample?”\nHiawatha, who at college\nMajored in applied statistics,\nConsequently felt entitled\nTo instruct his fellow man\nIn any subject whatsoever\n[…]\nHiawatha, in a temper,\nQuoted parts of R. A. Fisher,\nQuoted Yates and quoted Finney,\nQuoted reams of Oscar Kempthorne,\nQuoted Anderson and Bancroft\n(practically in extenso)\nTrying to impress upon them\nThat what actually mattered\nWas to estimate the error.\n- Several of them admitted:\n“Such a thing might have its uses;\nStill,” they said, “he would do better\nIf he shot a little straighter.”\n[…]\nIn a corner of the forest\nSits alone my Hiawatha\nPermanently cogitating\nOn the normal law of errors.\nWondering in idle moments\nIf perhaps increased precision\nMight perhaps be sometimes better\nEven at the cost of bias,\nIf one could thereby now and then\nRegister upon a target.\n— . E. Mientka, “Professor Leo Moser - Reflections of a Visit”\n\nFor in much wisdom is much grief: and he that increaseth knowledge increaseth sorrow.\n— Ecclesiastes 1:18\n\nAn extra year of experience has not changed my belief in a disconnect between traditional statistical regression methods and the pure prediction algorithms. Section 8 of the paper, featuring Table 5, makes the case directly in terms of six criteria. Five of the six emerged more or less unscathed from the discussion. Criteria 2, long-time scientific truth versus possibly short-term prediction accuracy, was doubted by FHT and received vigorous push-back from Yu/Barter:\n\n…but in our experience the “truth” that traditional regression methods supposedly represent is rarely justified or validated…\n\nThis is a hard-line Breimanian point of view. That “rarely” is over the top. The truism that begins “all models are wrong” ends with “but some are useful.” Traditional models tend to err on the side of over-simplicity (not enough interactions, etc.) but still manage to capture at least some aspect of the underlying mechanism. “Eternal truth” is a little too much to ask for, but in the Neonate example we did wind up believing that respiratory strength had something lasting to do with the babies’ survival.\n[…]\nThe fathers of statistical theory — Pearson, Fisher, Neyman, Hotelling, Wald — forgot to provide us with a comprehensive theory of optimal prediction. We will have to count on the current generation of young statisticians to fill the gap and put prediction on a principled foundation.\n— Bradley Efron: Rejoinder to Prediction, Estimation, and Attribution\n\nIt’s important to be realistic: most people don’t care about program performance most of the time. Modern computers are so fast that most programs run fast enough even with very slow language implementations. In that sense, I agree with Daniel’s premise: optimising compilers are often unimportant. But “often” is often unsatisfying, as it is here. Users find themselves transitioning from not caring at all about performance to suddenly really caring, often in the space of a single day.\nThis, to me, is where optimising compilers come into their own: they mean that even fewer people need care about program performance. And I don’t mean that they get us from, say, 98 to 99 people out of 100 not needing to care: it’s probably more like going from 80 to 99 people out of 100 not needing to care. This is, I suspect, more significant than it seems: it means that many people can go through an entire career without worrying about performance. Martin Berger reminded me of A N Whitehead’s wonderful line that “civilization advances by extending the number of important operations which we can perform without thinking about them” and this seems a classic example of that at work.\n— Laurence Tratt: What Challenges and Trade-Offs do Optimising Compilers Face?\n\nPay attention to unexpected data that has no natural constituency, and to lack of data that are in high demand.\n— Whitney R. Robinson: More on meta-epistemology: an epidemiologist’s perspective\n\nHalf of what you’ll learn in medical school will be shown to be either dead wrong or out of date within five years of your graduation; the trouble is that nobody can tell you which half–so the most important thing to learn is how to learn on your own.\n— David Sackett (playing off a common phrase)\n\nFile organization and naming are powerful weapons against chaos. (Jenny Bryan)\nYour closest collaborator is you six months ago, but you don’t reply to emails. (Mark Holder)\nI will let the data speak for itself when it cleans itself. (Allison Reichel)\nWorking with data is not about rules to follow but about decisions to make. (Naupaka Zimmerman)\nI’m not worried about being scooped, I’m worried about being ignored. (Magnus Nordborg)\nTeach stats as you would cake baking: make a few before you delve into the theory of leavening agents. (Jenny Bryan)\nThe opposite of “open” isn’t “closed”. The opposite of “open” is “broken”. (John Wilbanks)\n— Collected by Karl Broman\n\nThis was not meant to be an “emperor has no clothes” kind of story, rather “the emperor has nice clothes but they’re not suitable for every occasion.” Where they are suitable, the pure prediction algorithms can be stunningly successful. When one reads an enthusiastic AI-related story in the press, there’s usually one of these algorithms, operating in enormous scale, doing the heavy lifting. Regression methods have come a long and big way since the time of Gauss.\n— Bradley Efron: Prediction, Estimation, and Attribution.\n\nThere are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.\n— Leo Breiman: Statistical Modeling: The Two Cultures.\n\nPS Please excuse my not mailing this — but I don’t know your new address.\n— Richard Feynman: Letter to his dead wife, Arline, 17 Oct 1946\n\nThe more instructions something has, the worse its design. It’s cheaper to add instructions later than to design something well.\n— Scott Berkun: How Design Makes the World\n\nWe prove that last digits are approximately uniform for distributions with an absolutely continuous distribution function. From a practical perspective, that result, of course, is only moderately interesting. For that reason, we derive a result for ‘certain’ sums of lattice-variables as well. That justification is provided in terms of stationary distributions.\n— Stephan Dlugosz and Ulrich Muller-Funk: The value of the last digit: statistical fraud detection with digit analysis\n\nThe first of McIlroy’s dicta is often paraphrased as “do one thing and do it well”, which is shortened from “Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new ‘features.’”\nMcIlroy’s example of this dictum is:\n\nSurprising to outsiders is the fact that UNIX compilers produce no listings: printing can be done better and more flexibly by a separate program.\n\n[…]\nMcIlroy implies that the problem is that people didn’t think hard enough, the old school UNIX mavens would have sat down in the same room and thought longer and harder until they came up with a set of consistent tools that has “unusual simplicity”. But that was never going to scale, the philosophy made the mess we’re in inevitable. It’s not a matter of not thinking longer or harder; it’s a matter of having a philosophy that cannot scale unless you have a relatively small team with a shared cultural understanding, able to to sit down in the same room.\nIf anyone can write a tool and the main instruction comes from “the unix philosophy”, people will have different opinions about what “simplicity” or “doing one thing” means, what the right way to do things is, and inconsistency will bloom, resulting in the kind of complexity you get when dealing with a wildly inconsistent language, like PHP. People make fun of PHP and javascript for having all sorts of warts and weird inconsistencies, but as a language and a standard library, any commonly used shell plus the collection of widely used *nix tools taken together is much worse and contains much more accidental complexity due to inconsistency even within a single Linux distro and there’s no other way it could have turned out. If you compare across Linux distros, BSDs, Solaris, AIX, etc., the amount of accidental complexity that users have to hold in their heads when switching systems dwarfs PHP or javascript’s incoherence. The most widely mocked programming languages are paragons of great design by comparison.\nTo be clear, I’m not saying that I or anyone else could have done better with the knowledge available in the 70s in terms of making a system that was practically useful at the time that would be elegant today. It’s easy to look back and find issues with the benefit of hindsight. What I disagree with are comments from Unix mavens speaking today; comments like McIlroy’s, which imply that we just forgot or don’t understand the value of simplicity, or Ken Thompson saying that C is as safe a language as any and if we don’t want bugs we should just write bug-free code. These kinds of comments imply that there’s not much to learn from hindsight; in the 70s, we were building systems as effectively as anyone can today; five decades of collective experience, tens of millions of person-years, have taught us nothing; if we just go back to building systems like the original Unix mavens did, all will be well. I respectfully disagree.\n— Dan Luu: The growth of command line options, 1979-Present\n\nThis isn’t to say there’s no cost to adding options – more options means more maintenance burden, but that’s a cost that maintainers pay to benefit users, which isn’t obviously unreasonable considering the ratio of maintainers to users. This is analogous to Gary Bernhardt’s comment that it’s reasonable to practice a talk fifty times since, if there’s a three hundred person audience, the ratio of time spent watching to the talk to time spent practicing will still only be 1:6. In general, this ratio will be even more extreme with commonly used command line tools.\n— Dan Luu: The growth of command line options, 1979-Present\n\nobjects: Everything that exists in R is an object.\nfunctions: Everything that happens in R is a function call.\ninterfaces: Interfaces to other languages are a part of R.\n— John Chambers - S, R, and Data Science\n\nPHP is an embarrassment, a blight upon my craft. It’s so broken, but so lauded by every empowered amateur who’s yet to learn anything else, as to be maddening. It has paltry few redeeming qualities and I would prefer to forget it exists at all.\n[…]\nDo not tell me that “good developers can write good code in any language”, or bad developers blah blah. That doesn’t mean anything. A good carpenter can drive in a nail with either a rock or a hammer, but how many carpenters do you see bashing stuff with rocks? Part of what makes a good developer is the ability to choose the tools that work best.\n[…]\nPHP is built to keep chugging along at all costs. When faced with either doing something nonsensical or aborting with an error, it will do something nonsensical. Anything is better than nothing.\n— PHP: A fractal of bad design\n\nThe joy in mathematics is often the receding of the pain.\n[…]\nClaim: You can’t control the use of knowledge.\nThe choice is not between “peaceful” and “warlike” science. For most of science, the only choice is “relevant” and “irrelevant”.\nThe only way to make sure your scientific output will not be used by the military for war: Make sure it irrelevant, or wrong, or better: Both.\nAlmost anything that will give one side an advantage will eventually be used in warfare.\nNonetheless: The individual still has the responsibility to judge what they are working on. This is often not easy.\n[…]\nAre better weapons bad?\nPeople are inclined to believe that better weapons are (morally) bad.\nI visited Japan in early 2016 - both Hiroshima and Tanegashima. I will not talk about Hiroshima…. Tanegashima is an island in the south of Japan.\n1467: Japan’s Feudal system collapses. Sengoku period starts, permanent internal warfare. No side could get an upper hand.\n76 years later, the first Musket arrives in Japan (1543). Full adoption into the battle around 1570s; decisive in battle from 1575.\nFrom 1615 onward: Japan unified, century of peace. 108 years of war, ended in 40 years.\nProlonged warfare is terrible.\nSuperior weaponry does not always mean more civilian casualties.\nSome people argue that the terrible power of nuclear weapons are what enabled the last 75 years without World Wars.\nAre better weapons bad?\nI do not have an answer. I think about the history of Tanegashima, with little result.\n[…]\nBob Morris Sr. asked me when I met what I do.\n“I study math.”\n“For whom?”\n[…]\nIt is difficult to get a man to understand something when his salary depends upon his not understanding it. People tend to pick their ideologies by function.\n— Halvar Flake - OffensiveCon 2020 Keynote\n\nData analysis is hard, and part of the problem is that few people can explain how to do it. It’s not that there aren’t any people doing data analysis on a regular basis. It’s that the people who are really good at it have yet to enlighten us about the thought process that goes on in their heads.\nImagine you were to ask a songwriter how she writes her songs. There are many tools upon which she can draw. We have a general understanding of how a good song should be structured: how long it should be, how many verses, maybe there’s a verse followed by a chorus, etc. In other words, there’s an abstract framework for songs in general. Similarly, we have music theory that tells us that certain combinations of notes and chords work well together and other combinations don’t sound good. As good as these tools might be, ultimately, knowledge of song structure and music theory alone doesn’t make for a good song. Something else is needed.\nIn Donald Knuth’s legendary 1974 essay Computer Programming as an Art, Knuth talks about the difference between art and science. In that essay, he was trying to get across the idea that although computer programming involved complex machines and very technical knowledge, the act of writing a computer program had an artistic component. In this essay, he says that\n\nScience is knowledge which we understand so well that we can teach it to a computer.\n\nEverything else is art.\nAt some point, the songwriter must inject a creative spark into the process to bring all the songwriting tools together to make something that people want to listen to. This is a key part of the art of songwriting. That creative spark is difficult to describe, much less write down, but it’s clearly essential to writing good songs. If it weren’t, then we’d have computer programs regularly writing hit songs. For better or for worse, that hasn’t happened yet.\nMuch like songwriting (and computer programming, for that matter), it’s important to realize that data analysis is an art. It is not something yet that we can teach to a computer.\n— Roger D. Peng and Elizabeth Matsui: The Art of Data Science\n\nThere are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models andtreats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.\n— Leo Breiman: Statistical Modeling: The Two Cultures\n\nAt first glance Leo Breiman’s stimulating paper looks like an argument against parsimony and scientific insight, and in favor of black boxes with lots of knobs to twiddle. At second glance it still looks that way, but the paper is stimulating, and Leo has some important points to hammer home\n[…]\nRule 1. New methods always look better than old ones. Neural nets are better than logistic regression, support vector machines are better than neural nets, etc. In fact it is very difficult to run an honest simulation comparison, and easy to inadvertently cheat by choosing favorable examples, or by not putting as much effort into optimizing the dull old standard as the exciting new challenger.\nRule 2. Complicated methods are harder to criticize than simple ones. By now it is easy to check the efficiency of a logistic regression, but it is no small matter to analyze the limitations of a support vector machine.\n— Brad Efron: Comment on Statistical Modeling: The Two Cultures\n\nOccasionally, one sees frequentism defined in careerist terms, e.g., “A statistician who always rejects null hypotheses at the 95% level will over time make only 5% errors of the first kind.” This is not a comforting criterion for the statistician’s clients.\n[…]\nSomething important changed in the world of statistics in the new millennium. Twentieth-century statistics, even after the heated expansion of itslate period, could still be contained within the classic Bayesian–frequentist–Fisherian inferential triangle (Figure 14.1). This is not so in the twenty-first century. Some of the topics discussed in Part III—false-discovery rates,post-selection inference, empirical Bayes modeling, the lasso—fit within the triangle but others seem to have escaped, heading south from the frequentist corner, perhaps in the direction of computer science.\n— Bradley Efron & Trevor Hastie: Computer Age Statistical Inference\n\nPeople sometimes think (or complain) that working with quantitative data like this inures you to the reality of the human lives that lie behind the numbers. Numbers and measures are crude; they pick up the wrong things; they strip out the meaning of what’s happening to real people; they make it easy to ignore what can’t be counted. There’s something to those complaints. But it’s mostly a lazy critique. In practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture—what they mean, and for whom. Those regular spikes in the driving data are the pulse of everyday life as people go out to have a good time at the weekend. That peak there is the Mardi Gras parade in New Orleans. That bump in Detroit was a Garth Brooks concert. Right across the country, that is the sudden shock of the shutdown the second weekend in March. It was a huge collective effort to buy time that, as it turns out, the federal government has more or less entirely wasted. And now through May here comes the gradual return to something like the baseline level of activity from January, proceeding much more quickly in some cities than in others.\nI sit at my kitchen-counter observatory and look at the numbers. Before my coffee is ready, I can quickly pull down a few million rows of data courtesy of a national computer network originally designed by the government to be disaggregated and robust, because they were convinced that was what it would take for communication to survive a nuclear war. I can process it using software originally written by academics in their spare time, because they were convinced that sophisticated tools should be available to everyone for free. Through this observatory I can look out without even looking up, surveying the scale and scope of the country’s ongoing, huge, avoidable failure. Everything about this is absurd.\n— Kieran Healy: The Kitchen Counter Observatory\n\nSurgisphere appears to be the Theranos, or possibly the Cornell Food and Brand Lab, of medical research, and Lancet is a serial enabler of research fraud (see this news article by Michael Hiltzik), and it’s easy to focus on that. But remember all the crappy papers these journals publish that don’t get retracted, cos they’re not fraudulent, they’re just crappy. Retracting papers just cos they’re crappy—no fraud, they’re just bad science—I think that’s never ever ever gonna happen. Retraction is taken as some kind of personal punishment meted out to an author and a journal. This frustrates me to no end. What’s important is the science, not the author. But it’s not happening. So, when we hear about glamorous/seedy stories of fraud, remember the bad research, the research that’s not evilicious but just incompetent, maybe never even had a chance of working. That stuff will stay in the published literature forever, and journals love publishing it.\nAs we say in statistics, the shitty is the enemy of the good.\n\nOpen code, open data, open review . . .\n\nSo, you knew I’d get to this…\nJust remember, honesty and transparency are not enuf. Open data and code don’t mean your work is any good. A preregistered study can be a waste of time. The point of open data and code is that it makes it easier to do post-publication review. If you’re open, it makes it easier for other people to find flaws in your work. And that’s a good thing.\nAn egg is just a chicken’s way of making another egg.\nAnd the point of science and policy analysis is not to build beautiful careers. The purpose is to learn about and improve the world.\n— Andrew Gelman: bla bla bla PEER REVIEW bla bla bla\n\nSometimes somebody says something to me, like a whisper of a hint of an echo of something half-forgotten, and it lands on me like an invocation. The mania sets in, and it isn’t enough to believe; I have to know.\n[…]\nSo: the technical reason we started counting arrays at zero is that in the mid-1960’s, you could shave a few cycles off of a program’s compilation time on an IBM 7094. The social reason is that we had to save every cycle we could, because if the job didn’t finish fast it might not finish at all and you never know when you’re getting bumped off the hardware because the President of IBM just called and fuck your thesis, it’s yacht-racing time.\nThere are a few points I want to make here.\nThe first thing is that as far as I can tell nobody has ever actually looked this up.\nWhatever programmers think about themselves and these towering logic-engines we’ve erected, we’re a lot more superstitious than we realize. We tell and retell this collection of unsourced, inaccurate stories about the nature of the world without ever doing the research ourselves, and there’s no other word for that but “mythology”. Worse, by obscuring the technical and social conditions that led humans to make these technical and social decisions, by talking about the nature of computing as we find it today as though it’s an inevitable consequence of an immutable set of physical laws, we’re effectively denying any responsibility for how we got here. And worse than that, by refusing to dig into our history and understand the social and technical motivations for those choices, by steadfastly refusing to investigate the difference between a motive and a justification, we’re disavowing any agency we might have over the shape of the future. We just keep mouthing platitudes and pretending the way things are is nobody’s fault, and the more history you learn and the more you look at the sad state of modern computing the the more pathetic and irresponsible that sounds.\n— mhoye: Citation Needed\n\nOf course, someone has to write for loops. It doesn’t have to be you.\n— Jenny Bryan: Row Oriented Workflows in R With the Tidyverse\n\nAn over-simplified and dangerously reductive diagram of a data system might look like this:\nCollection → Computation → Representation\nWhenever you look at data — as a spreadsheet or database view or a visualization, you are looking at an artifact of such a system. What this diagram doesn’t capture is the immense branching of choice that happens at each step along the way. As you make each decision — to omit a row of data, or to implement a particular database structure or to use a specific colour palette you are treading down a path through this wild, tall grass of possibility. It will be tempting to look back and see your trail as the only one that you could have taken, but in reality a slightly divergent you who’d made slightly divergent choices might have ended up somewhere altogether different. To think in data systems is to consider all three of these stages at once.\n— Jer Thorp: You Say Data, I Say System\n\nThe thing that is most alluring about the Gini coefficient also turns out to be its greatest shortcoming. By collapsing the whole rainbow of the income distribution into a single statistical point of white light, it necessarily conceals much of great interest. That is of course true of any single summary measure… The best measures are those that match our purpose, or pick up on the places where important changes are happening. We should pick and mix with that in mind.\n— Angus Deaton and Angela Case\n\nAfter describing the perverse interaction between wealth gaps, education, mortality trends and political economy, Case and Deaton note that “You could crunch the Gini coefficient to as many decimal places as you like, and you’d learn next to nothing about what’s really going on here.” Quite so, but surely it is unreasonable to demand of one measure of inequality along one dimension that it shed light on complex social interactions or detect causal relationships.\nThat would be like urging us to abandon the Centigrade scale as a measure of temperature because it so badly fails to inform us about how climate change plays havoc with rainfall patterns, the incidence of extreme weather events, or the extent of sea level rises. You could calculate average global temperature increases to as many decimal places in degrees Celsius as you liked, and you would be none the wiser about the diversity of consequences of climate change around the world.\n— Francisco Ferreira\n\nSoftware has been around since the 1940s. Which means that people have been faking their way through meetings about software, and the code that builds it, for generations. Now that software lives in our pockets, runs our cars and homes, and dominates our waking lives, ignorance is no longer acceptable. The world belongs to people who code. Those who don’t understand will be left behind. (Josh Tyrangiel, header)\n[…]\nA computer is a clock with benefits. They all work the same, doing second-grade math, one step at a time: Tick, take a number and put it in box one. Tick, take another number, put it in box two. Tick, operate (an operation might be addition or subtraction) on those two numbers and put the resulting number in box one. Tick, check if the result is zero, and if it is, go to some other box and follow a new set of instructions.\n[…]\nIt’s a good and healthy exercise to ponder what your computer is doing right now. Maybe you’re reading this on a laptop: What are the steps and layers between what you’re doing and the Lilliputian mechanisms within? When you double-click an icon to open a program such as a word processor, the computer must know where that program is on the disk. It has some sort of accounting process to do that. And then it loads that program into its memory—which means that it loads an enormous to-do list into its memory and starts to step through it. What does that list look like?\nMaybe you’re reading this in print. No shame in that. In fact, thank you. The paper is the artifact of digital processes. Remember how we put that “a” on screen? See if you can get from some sleepy writer typing that letter on a keyboard in Brooklyn, N.Y., to the paper under your thumb. What framed that fearful symmetry?\nThinking this way will teach you two things about computers:\nOne, there’s no magic, no matter how much it looks like there is. There’s just work to make things look like magic.\nAnd two, it’s crazy in there.\n[…]\nYou can tell how well code is organized from across the room. Or by squinting or zooming out. The shape of code from 20 feet away is incredibly informative. Clean code is idiomatic, as brief as possible, obvious even if it’s not heavily documented. Colloquial and friendly. As was written in Structure and Interpretation of Computer Programs (aka SICP), the seminal textbook of programming taught for years at MIT, “A computer language is not just a way of getting a computer to perform operations – it is a novel formal medium for expressing ideas about methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute.” A great program is a letter from current you to future you or to the person who inherits your code. A generous humanistic document.\nOf course all of this is nice and flowery; it needs to work, too.\n— Paul Ford: What is Code?\n\nThe Postulates of Mathematics Were Not on the Stone Tablets that Moses Brought Down from Mt. Sinai. It is necessary to emphasize this. We begin with a vague concept in our minds, then we create various sets of postulates, and gradually we settle down to one particular set. In the rigorous postulational approach, the original concept is now replaced by what the postulates define. This makes further evolution of the concept rather difficult and as a result tends to slow down the evolution of mathematics. It is not that the postulation approach is wrong, only that its arbitrariness should be clearly recognized, and we should be prepared to change postulates when the need becomes apparent.\n— Richard Hamming\n\nTeachers should prepare the student for the student’s future, not for the teacher’s past.\n[…]\nEducation is what, when, and why to do things. Training is how to do it.\n[…]\nIn science, if you know what you are doing, you should not be doing it. In engineering, if you do not know what you are doing, you should not be doing it. Of course, you seldom, if ever, see either pure state.\n—Richard Hamming: The Art of Doing Science and Engineering\n\nI note with fear and horror that even in 1980, language designers and users have not learned this lesson [mandatory run-time checking of array bounds]. In any respectable branch of engineering, failure to observe such elementary precautions would have long been against the law.\n[…]\nI conclude that there are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult. It demands the same skill, devotion, insight, and even inspiration as the discovery of the simple physical laws which underlie the complex phenomena of nature. It also requires a willingness to accept objectives which are limited by physical, logical, and technological constraints, and to accept a compromise when conflicting objectives cannot be met. No committee will ever do this until it is too late.\n— C.A.R. Hoare: The Emperor’s Old Clothes\n\nI like this definition: A tool addresses human needs by amplifying human capabilities. That is, a tool converts what we can do into what we want to do. A great tool is designed to fit both sides.\n— Bret Victor: A Brief Rant on the Future of Interaction Design\n\nDiscoverability is often cited as npm’s biggest flaw. Many blog posts – scratch that, entire websites – have been created to try and mitigate the difficulty of finding what you need on npm. Everyone has an idea about how to make it easier to find needles in the haystack, but no-one bothers to ask what all this hay is doing here in the first place.\n— Rich Harris: Small modules: it’s not quite that simple\n\nBrave to write it, all by itself, and then brave to show it. It is like opening your ribcage, and letting someone see the little bird you have inside. What if they don’t love the bird? It’s not like you can change it. I mean.. That’s your bird.\n— Tycho: Fan Fiction\n\nLook, you have two choices. You can say, “I’m a pessimist, nothing’s gonna work, I’m giving up, I’ll help ensure that the worst will happen.” Or you can grasp onto the opportunities that do exist, the rays of hope that exist, and say, “Well, maybe we can make it a better world.” It’s not a much of a choice.\n— Noam Chomskey: Interview\n\nThere’s really only one important question worth asking, which is: what is a life well-lived?… That’s a question that can’t be answered, but one thing we can say, with a lot of certainty, is that a life well-lived is not going to be a life in which every moment is scrutinized.\n— Frank Lantz: Hearts and Minds\n\nI hate abstraction. Here are some examples.\n— Adam Cadre: Fatal abstraction\n\nI find that the single thing which inhibits young professionals, new students most severely, is their acceptance of standards that are too low. If I ask a student whether her design is as good as Chartres, she often smiles tolerantly at me as if to say, “Of course not, that isn’t what I am trying to do…. I could never do that.”\nThen, I express my disagreement, and tell her: “That standard must be our standard. If you are going to be a builder, no other standard is worthwhile. That is what I expect of myself in my own buildings, and it is what I expect of my students.” Gradually, I show the students that they have a right to ask this of themselves, and must ask this of themselves. Once that level of standard is in their minds, they will be able to figure out, for themselves, how to do better, how to make something that is as profound as that.\n— Christopher Alexander: Foreward to Patterns of Software\n\nThe Stone Age didn’t end for lack of stone, and the Oil Age will end long before the world runs out of oil.\n— Sheik Ahmed Zaki Yamani\n\n[I told the fourth-graders] I was thinking of a number between 1 and 10,000. … They still cling stubbornly to the idea that the only good answer is a yes answer. This, of course, is the result of miseducation in which “right answers” are the only ones that pay off. They have not learned how to learn from a mistake, or even that learning from mistakes is possible. If they say, “Is the number between 5,000 and 10,000?” and I say yes, they cheer; if I say no, they groan, even though they get exactly the same amount of information in either case. The more anxious ones will, over and over again, ask questions that have already been answered, just for the satisfaction of hearing a yes.\n— John Holt: How Children Fail\n\nIan is a game design teacher and a professional skeptic. People call him a “curmudgeon”, but they don’t really understand how much love, how much actual faith, that kind of skepticism takes. On a pretty regular basis one of us will IM the other something like “help” or “fuck” or “people are terrible”.\nOnly when you fully believe in how wonderful something is supposed to be does every little daily indignity start to feel like some claw of malaise. At least, that’s how I explain Ian to other people.\n— Leigh Alexander: The Unearthing\n\nQ: So it’s fine to say, everybody should learn a little bit about how to program and this way of thinking because it’s valuable and important. But then maybe that’s just not realistic. Donald Knuth told me that he thinks two percent of the population have brains wired the right way to think about programming.\nA: That same logic would lead you to say that one percent of the US’s population is wired to understand Mandarin. The reasoning there is equivalent.\n— [Hal Abselson: Interview]\n\nMany professions require some form of programming. Accountants program spreadsheets; musicians program synthesizers; authors program word processors; and web designers program style sheets.\n[…]\nThe typical course on programming teaches a “tinker until it works” approach. When it works, students exclaim “It works!” and move on. Sadly, this phrase is also the shortest lie in computing, and it has cost many people many hours of their lives.\n[…]\nBy “good programming,” we mean an approach to the creation of software that relies on systematic thought, planning, and understanding from the very beginning, at every stage, and for every step. To emphasize the point, we speak of systematic program design and systematically designed programs. Critically, the latter articulates the rationale of the desired functionality. Good programming also satisfies an aesthetic sense of accomplishment; the elegance of a good program is comparable to time-tested poems or the black-and-white photographs of a bygone era. In short, programming differs from good programming like crayon sketches in a diner from oil paintings in a museum. No, this book won’t turn anyone into a master painter. But, we would not have spent fifteen years writing this edition if we didn’t believe that\neveryone can design programs\nand\neveryone can experience the satisfaction that comes with creative design.\n[…]\nWhen you were a small child, your parents taught you to count and perform simple calculations with your fingers: “1 + 1 is 2”; “1 + 2 is 3”; and so on. Then they would ask “what’s 3 + 2?” and you would count off the fingers of one hand. They programmed, and you computed. And in some way, that’s really all there is to programming and computing. Now it is time to switch roles.\n— How to Design Programs\n\nYou can’t sell someone the solution before they’ve bought the problem.\n— Chip Morningstar: Smart people can rationalize anything\n\nwhen you don’t create things, you become defined by your tastes rather than ability. your tastes only narrow & exclude people. so create\n— why the lucky stiff\n\nWhen you believe you have a future, you think in terms of generations and years. When you do not, you live not just by the day – but by the minute.\n— Iris Chang – Suicide Note\n\nIt seems like most people ask: “How can I throw my life away in the least unhappy way?”\n— Stewart Brand\n\nQ: Are you ever afraid of someone stealing your thunder?\nA: I think anybody really good is going to want to do their own thing. Anybody who’s not really good, you don’t have to worry too much about.\n— Chris Hecker: Interview\n\nLiving organisms are shaped by evolution to survive, not necessarily to get a clear picture of the universe. For example, frogs’ brains are set up to recognize food as moving objects that are oblong in shape. So if we take a frog’s normal food – flies – paralyze them with a little chloroform and put them in front of the frog, it will not notice them or try to eat them.\nIt will starve in front of its food! But if we throw little rectangular pieces of cardboard at the frog it will eat them until it is stuffed! The frog only sees a little of the world we see, but it still thinks it perceives the whole world.\nNow, of course, we are not like frogs! Or are we?\n— Alan Kay: The Center of “Why?”\n\nPick a plane or a cave wall to project the shadow of the Real World onto, and tell a story about the outlines it makes. The trick is to shrug and smile and pick another plane and do it all again to get a completely different shadow, until you find the one most useful for the day. It’s a magic trick for most folks.\n— Down is just the most common way out\n\nTo an architect, imagination is mostly about the future. To invent the future, one must live in it, which means living (at least partly) in a world that does not yet exist. Just as a driver whizzing along a highway pays more attention to the front window than the rear, the architect steers by looking ahead. This can sometimes make them seem aloof or absent-minded, as if they are someplace else. In fact, they are. For them, the past is a lesson, the present is fleeting; but the future is real. It is infinite and malleable, brimming with possibility.\n— Danny Hillis: The Power of Conviction\n\nIf the first line of your [R] script is setwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\"), I will come into your lab and SET YOUR COMPUTER ON FIRE.\n— Jenny Bryan: here, here\n\nIt becomes important in such a climate of opinion to emphasize that books do not store knowledge. They contain symbolic codes that can serve us as external mnemonics for knowledge. Knowledge can exist only in living human minds.\n— Kieran Egan: The Educated Mind: How Cognitive Tools Shape Our Understanding\n\nPeople are much smarter when they can use their full intellect and when they can relate what they are learning to situations or phenomena which are real to them. The natural reaction, when someone is having trouble understanding what you are explaining, is to break up the explanation into smaller pieces and explain the pieces one by one. This tends not to work, so you back up even further and fill in even more details.\nBut human minds do not work like computers: it is harder, not easier, to understand something broken down into all the precise little rules than to grasp it as a whole. It is very hard for a person to read a computer assembly language program and figure out what it is about…\nStudying mathematics one rule at a time is like studying a language by first memorizing the vocabulary and the detailed linguistic rules, then building phrases and sentences, and only afterwards learning to read, write, and converse. Native speakers of a language are not aware of the linguistic rules: they assimilate the language by focusing on a higher level, and absorbing the rules and patterns subconsciously. The rules and patterns are much harder for people to learn explicitly than is the language itself.\n— William Thurston: Mathematical Education\n\nA lot of the stuff going on [in AI] is not very ambitious. In machine learning, one of the big steps that happened in the mid-’80s was to say, “Look, here’s some real data – can I get my program to predict accurately on parts of the data that I haven’t yet provided to it?” What you see now in machine learning is that people see that as the only task.\n— Stuart Russell\n\nLike most readers, I had functionally consigned [our game] to the furnace. I had let it float away on one of those little lantern boats in a way that brought me closure, if no one else. Insufficient.\nFucking insufficient.\nYou have to get back on the horse. Somehow, and I don’t know how this kind of thing starts, we have started to lionize horseback-not-getting-on: these casual, a priori assertions of inevitable failure, which is nothing more than a gauze draped over your own pulsing terror. Every creative act is open war against The Way It Is. What you are saying when you make something is that the universe is not sufficient, and what it really needs is more you. And it does, actually; it does. Go look outside. You can’t tell me that we are done making the world.\n— Tycho: A Matter of Scale\n\nQ: At the [NYU] Game Center, we’re interested in the role of the university as an alternate place for thinking about games… What in your opinion are some of the big interesting problems that students should be working on?\nA: My advice for students is… I question the question. I don’t think there are problems that students should be working on. I think students should be making games that are interesting and push the boundaries, and those will generate the problems.\n— Chris Hecker\n\nA complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.\n— John Gall: Systemantics\n\nWe have this limited bubble of experience. We can only have so many experiences in our lifetime to build models from, and we’re abstracting from that data. We’ve found, through evolution, two ways to get more data, to build more elaborate models of the world. One is to have toy experiences, little counterfeit experiences. The other one is to learn from the experience of others. When somebody tells you a story, you can actually learn from that story, incorporate it into your model of the world to make your model more accurate based upon that data that you got from somebody else. So over time, we have come to call one of these things “play” and the other one “storytelling”. These are both fundamentally educational technologies that allow us to build more elaborate models of the world around us, by supplanting our limited experience with other experiences.\n— Will Wright: Gaming Reality\n\nJohn [McCarthy]’s world is a world of ideas, a world in which ideas don’t belong to anyone, and when an idea is wrong, just the idea - not the person - is wrong. A world in which ideas are like young birds, and we catch them and proudly show them to our friends. The bird’s beauty and the hunter’s are distinct….\nSome people won’t show you the birds they’ve caught until they are sure, certain, positive that they - the birds, or themselves - are gorgeous, or rare, or remarkable. When your mind can separate yourself from your bird, you will share it sooner, and the beauty of the bird will be sooner enjoyed. And what is a bird but for being enjoyed?\n— Richard Gabriel: The Design of Parallel Programming Languages"
  },
  {
    "objectID": "software_citations.html",
    "href": "software_citations.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "On this page I collect citations to software I maintain or am an author of. Software citations are particularly poorly captured by things like Google Scholar, both because software exists outside of standard citation formats and because software citations are often poorly formatted in publications. For that reason, I find it useful to keep my own list of projects which have cited my work.\nIf you cited any of my packages in a publication, please let me know – either on Mastodon or via email at mike.mahoney.218+site@gmail.com . I can’t guarantee I’ll add them to this list, for secret reasons, but it always makes me happy to see."
  },
  {
    "objectID": "software_citations.html#software-citations",
    "href": "software_citations.html#software-citations",
    "title": "Mike Mahoney",
    "section": "",
    "text": "On this page I collect citations to software I maintain or am an author of. Software citations are particularly poorly captured by things like Google Scholar, both because software exists outside of standard citation formats and because software citations are often poorly formatted in publications. For that reason, I find it useful to keep my own list of projects which have cited my work.\nIf you cited any of my packages in a publication, please let me know – either on Mastodon or via email at mike.mahoney.218+site@gmail.com . I can’t guarantee I’ll add them to this list, for secret reasons, but it always makes me happy to see."
  },
  {
    "objectID": "software_citations.html#how-to-cite-software",
    "href": "software_citations.html#how-to-cite-software",
    "title": "Mike Mahoney",
    "section": "How to cite software",
    "text": "How to cite software\nThe rOpenSci blog has a fantastic post on how to cite R and R packages, which I quickly summarize below. I highly recommend their post, however, even if you aren’t an R user; most of the topic generalizes to other software pretty easily.\nA software citation serves two functions for your readers:\n\nTells them what you did; for instance, if I see someone cite the sf package, I suddenly know more about the details of their spatial calculations than I can probably get from the text alone.\nTells them how you did it; package version information and source information helps others to replicate your work, and also can help readers assess an analysis. If you use sf &gt;= 1.0.0, for instance, I know that you probably didn’t run into issues doing distance calculations with geographic coordinates, so I don’t need to consider that when I’m reading your results.\n\nSoftware citations also help the developers of your packages justify developing packages further; if you’re a user of scientific software and want there to still be scientific software in the future, you should cite your software. It also makes me, personally, feel great to see people benefiting from my work, and to see them credit me for any help my software gave them!\nAs such, you should cite any package that was relevant to the study you performed. If uninstalling the package (or deleting the relevant function calls) would change your results or make your code not run, you should cite it.\nIf you are an R user, you can get the citations for both R and your packages using the citation() function:\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2023). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation(\"rsample\")\n\nTo cite package 'rsample' in publications use:\n\n  Frick H, Chow F, Kuhn M, Mahoney M, Silge J, Wickham H (2022).\n  _rsample: General Resampling Infrastructure_. R package version\n  1.1.1, &lt;https://CRAN.R-project.org/package=rsample&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {rsample: General Resampling Infrastructure},\n    author = {Hannah Frick and Fanny Chow and Max Kuhn and Michael Mahoney and Julia Silge and Hadley Wickham},\n    year = {2022},\n    note = {R package version 1.1.1},\n    url = {https://CRAN.R-project.org/package=rsample},\n  }\n\n\nIf you aren’t an R user, many scientific software libraries will still provide information on how to cite them – for instance, here’s the instructions for citing numpy. If Google and such return nothing, then I think your best option is to style citations after the official R citation: mimic book citation style, using authors, year, title, publication location, and an accessible URL."
  },
  {
    "objectID": "software_citations.html#how-i-track-citations",
    "href": "software_citations.html#how-i-track-citations",
    "title": "Mike Mahoney",
    "section": "How I track citations",
    "text": "How I track citations\nAs suggested in a different rOpenSci post, I find the best general tool I have for finding software citations is to set up a Google Scholar alert for my packages and investigate each alert as it comes in. This is not a foolproof method, and I’m sure I miss things; as mentioned above, if you cited any of my packages in a publication, please let me know either on Mastodon or via email at mike.mahoney.218+site@gmail.com .\nI only track citations for packages I’m an author on, once I’m an author on them. In cases where a paper mentions the package but doesn’t cite it, I only list the article if it was published after I became an author on the package.\nWhile I track both citations and mentions of my packages, I really want to stress that citations are fantastic, help me justify maintaining open source scientific software (both to my boss and to myself), and make me really happy. Mentions do not. If you are going to mention one of these packages, please give it a full citation.\nThe citations below are in a variety of formats; I’ll confess to copying journal “cite this article” citations without reformatting for this list. If I messed up my citation for your article, please get in touch to let me know."
  },
  {
    "objectID": "software_citations.html#software-citations-1",
    "href": "software_citations.html#software-citations-1",
    "title": "Mike Mahoney",
    "section": "Software citations",
    "text": "Software citations\n\nterrainr\n\nFox, N., Serrano-Vergel, R., Van Berkel, D., Lindquist, M. 2022. Towards Gamified Decision Support Systems: In-game 3D Representation of Real-word Landscapes from GIS Datasets. Journal of Digital Landscape Architecture. https://doi.org/10.14627/537724035\nTamiminia, H., Salehi, B., Mahdianpari, M., Beier, C. M., Johnson, L. 2022. Mapping Two Decades of New York State Forest Aboveground Biomass Change Using Remote Sensing. Remote Sensing 14(16): 4097. https://doi.org/10.3390/rs14164097\n\nThe following are self-citations:\n\nJohnson, L. K., Mahoney, M. J., Desrochers, M. L., & Beier, C. M. 2023. Mapping historical forest biomass for stock-change assessments at parcel to landscape scales. arXiv:2304.02632. https://doi.org/10.48550/arXiv.2304.02632\nJohnson, L. K., Mahoney, M. J., Bevilacqua, E., Stehman, S. V., Domke, G. M., & Beier, C. M. (2022). Fine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages. International Journal of Applied Earth Observation and Geoinformation, 114, 103059. https://doi.org/10.1016/j.jag.2022.103059\nMahoney, M. J., Johnson, L. K., Guinan, A. Z., & Beier, C. M. (2022). Classification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast. International Journal of Remote Sensing, 43(19-24), 7117-7138. https://doi.org/10.1080/01431161.2022.2155086\nMahoney et al., (2022). unifir: A Unifying API for Working with Unity in R. Journal of Open Source Software, 7(73), 4388, https://doi.org/10.21105/joss.04388\nTamiminia, H., Salehi, B., Mahdianpari, M., Beier, C. M., Johnson, L., Phoenix, D. B., and Mahoney, M. 2022. Decision tree-based machine learning models for above-ground biomass estimation using multi-source remote sensing data and object-based image analysis. Geocarto International 37(26): 12763-12791. https://doi.org/10.1080/10106049.2022.2071475\n\n\ncitation(\"terrainr\")\n\nThe United States Geological Survey provides guidelines for citing USGS\ndata products (as downloaded from 'get_tiles') at:\nhttps://www.usgs.gov/faqs/how-should-i-cite-datasets-and-services-national-map\n\nTo cite terrainr in publications please use:\n\n  Mahoney M. J., Beier C. M., and Ackerman, A. C. (2022). terrainr: An\n  R package for creating immersive virtual environments. Journal of\n  Open Source Software, 7(69), 4060,\n  https://doi.org/10.21105/joss.04060\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {69},\n    pages = {4060},\n    author = {Michael J. Mahoney and Colin M. Beier and Aidan C. Ackerman},\n    title = {{terrainr}: An R package for creating immersive virtual environments},\n    journal = {Journal of Open Source Software},\n    doi = {10.21105/joss.04060},\n    url = {https://doi.org/10.21105/joss.04060},\n  }\n\n\n\n\nunifir\n\ncitation(\"unifir\")\n\nWarning in meta$Date: partial match of 'Date' to 'Date/Publication'\n\n\nTo cite unifir in publications please use:\n\n  Mahoney M. J., Beier C. M., and Ackerman, A. C. (2022). unifir: A\n  Unifying API for Working with Unity in R. Journal of Open Source\n  Software, 7(73), 4388, https://doi.org/10.21105/joss.04388\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {73},\n    pages = {4388},\n    author = {Michael J. Mahoney and Colin M. Beier and Aidan C. Ackerman},\n    title = {{unifir:} A Unifying {API} for Working with {Unity} in {R}},\n    journal = {Journal of Open Source Software},\n    doi = {10.21105/joss.04388},\n    url = {https://doi.org/10.21105/joss.04388},\n  }\n\n\n\n\nwaywiser\n\ncitation(\"waywiser\")\n\nTo cite waywiser in publications please use:\n\n  Mahoney M. J. (2023). waywiser: Ergonomic Methods for Assessing\n  Spatial Models. arXiv:2303.11312 [cs.MS].\n  https://doi.org/10.48550/arXiv.2303.11312\n\nA BibTeX entry for LaTeX users is\n\n  @Misc{,\n    title = {waywiser: Ergonomic Methods for Assessing Spatial Models},\n    author = {Michael J Mahoney},\n    year = {2023},\n    eprint = {2303.11312},\n    archiveprefix = {arXiv},\n    primaryclass = {cs.MS},\n    doi = {10.48550/arXiv.2303.11312},\n    url = {https://arxiv.org/abs/2303.11312},\n  }\n\n\n\n\nspatialsample\nI took over maintenance of spatialsample in mid-2022, after spending the summer interning with the tidymodels crew at Posit working on spatialsample and rsample. I’m responsible for the majority of package functionality. I only track citations here for versions of the package I’m an author on (&gt;= 0.2.0).\n\nPebesma, E., Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). 314 pages. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016\nde Lara, A., Mieno, T., Luck, J.D. et al. Predicting site-specific economic optimal nitrogen rate using machine learning methods and on-farm precision experimentation. Precision Agric (2023). https://doi.org/10.1007/s11119-023-10018-8\nTutland, Niko J., Rodman, Kyle C., Andrus, Robert A., and Hart, Sarah J. 2023. “Overlapping Outbreaks of Multiple Bark Beetle Species are Rarely More Severe than Single-Species Outbreaks.” Ecosphere 14(3): e4478. https://doi.org/10.1002/ecs2.4478\nSkinner, E.B., Glidden, C.K., MacDonald, A.J. et al. Human footprint is associated with shifts in the assemblages of major vector-borne diseases. Nat Sustain (2023). https://doi.org/10.1038/s41893-023-01080-1\n\nThe following are self-citations:\n\nMahoney, MJ In Review. waywiser: Ergonomic methods for assessing spatial models. https://arxiv.org/abs/2303.11312\n\n\ncitation(\"spatialsample\")\n\nTo cite spatialsample in publications please use:\n\n  Mahoney M. J., Johnson, L. K., Silge, J., Frick, H., Kuhn, M., and\n  Beier C. M. (2023). Assessing the performance of spatial\n  cross-validation approaches for models of spatially structured data.\n  arXiv. https://doi.org/10.48550/arXiv.2303.07334\n\nA BibTeX entry for LaTeX users is\n\n  @Misc{,\n    title = {Assessing the performance of spatial cross-validation approaches for models of spatially structured data},\n    author = {Michael J Mahoney and Lucas K Johnson and Julia Silge and Hannah Frick and Max Kuhn and Colin M Beier},\n    year = {2023},\n    eprint = {2303.07334},\n    archiveprefix = {arXiv},\n    primaryclass = {stat.CO},\n    doi = {10.48550/arXiv.2303.07334},\n    url = {https://arxiv.org/abs/2303.07334},\n  }\n\n\n\n\nrsample\nI am an author on rsample after implementing functions for grouped resampling and clustered cross-validation (as well as the “common resampling patterns” vignette and a few other improvements). I only track citations here for versions of the package I’m an author on (&gt;= 1.1.1).\n\nLundell, J. 2023. EZtune: A Package for Automated Hyperparameter Tuning in R. https://arxiv.org/abs/2303.12177\nYing R., Monteiro, F. M., Wilson, J. D., and Schmidt, D. N. 2023. ForamEcoGEnIE 2.0: incorporating symbiosis and spine traits into a trait-based global planktic foraminiferal model. Geosci. Model Dev., 16, 813–832, https://doi.org/10.5194/gmd-16-813-2023\nLyu H, Grafton M, Ramilan T, Irwin M, Sandoval E. Assessing the Leaf Blade Nutrient Status of Pinot Noir Using Hyperspectral Reflectance and Machine Learning Models. Remote Sensing. 2023; 15(6):1497. https://doi.org/10.3390/rs15061497\n\nThe following are self-citations:\n\nMahoney, MJ, Johnson, L. K., Silge, J., Frick, H., Kuhn, M., and Beier, C. M. In Review. Assessing the performance of spatial cross-validation approaches for models of spatially structured data. https://arxiv.org/abs/2303.07334\nMahoney, MJ In Review. waywiser: Ergonomic methods for assessing spatial models. https://arxiv.org/abs/2303.11312\n\nrsample is used in (but not cited in) Wang et al 2023; Lienard et al 2022; Adu-Oppong et al 2022; Zappaterra et al 2022; Mull et al 2022; Dematheis et al 2022; Bellows et al 2022; Yates et al; Stevelink et al 2022; Bartz-Beielstein et al 2023; Han Nh 2022; Yan et al 2023; dos Santos et al 2022; Badonyi et al 2022; Badonyi et al 2022; Peguero et al 2023; Howard et al 2023.\n\ncitation(\"rsample\")\n\nTo cite package 'rsample' in publications use:\n\n  Frick H, Chow F, Kuhn M, Mahoney M, Silge J, Wickham H (2022).\n  _rsample: General Resampling Infrastructure_. R package version\n  1.1.1, &lt;https://CRAN.R-project.org/package=rsample&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {rsample: General Resampling Infrastructure},\n    author = {Hannah Frick and Fanny Chow and Max Kuhn and Michael Mahoney and Julia Silge and Hadley Wickham},\n    year = {2022},\n    note = {R package version 1.1.1},\n    url = {https://CRAN.R-project.org/package=rsample},\n  }\n\n\n\n\ngeojsonio\nI took over maintaining geojsonio in mid-2022, in order to keep the package on CRAN. Most functionality was either contributed by Scott and Andy or by community contributors; I primarily fix issues to keep the package on CRAN and shepherd community contributions prior to merging them into the project. As with rsample, I only track citations since I joined the project.\ngeojsonio is used in (but not cited in) Chauhan et al 2022; Wang 2023; de Souza et al 2022.\n\ncitation(\"geojsonio\")\n\nTo cite package 'geojsonio' in publications use:\n\n  Chamberlain S, Teucher A, Mahoney M (2023). _geojsonio: Convert Data\n  from and to 'GeoJSON' or 'TopoJSON'_. R package version 0.11.0,\n  &lt;https://CRAN.R-project.org/package=geojsonio&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {geojsonio: Convert Data from and to 'GeoJSON' or 'TopoJSON'},\n    author = {Scott Chamberlain and Andy Teucher and Michael Mahoney},\n    year = {2023},\n    note = {R package version 0.11.0},\n    url = {https://CRAN.R-project.org/package=geojsonio},\n  }"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "I’m a PhD candidate at SUNY-ESF, working on new tools to make complex, broad-scale systems easier for humans to understand. Right now, that means I split my time between predictive modeling (to monitor forest carbon sequestration across New York State, track the development of early-successional forests, and more) and visualization (including using game engines as a GIS and making it easier to make reproducible VR environments for research). By training I’m either an ecologist (specializing in landscape and translational ecology) or environmental scientist; professionally, I’ve worked as a data analyst, software engineer, and chicken farmer.\nOn this site I keep a list of my publications, presentations, and my CV, as well as a technical blog.\n\n\n\n\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes. Mahoney, MJ, Johnson, LK, Bevilacqua, E, and Beier, CM. 2022. GIScience & Remote Sensing 59(1): 1266-1280. https://doi.org/10.1080/15481603.2022.2103069\nClassification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast. Mahoney, MJ, Johnson, LK, Guinan, AZ, and Beier, CM. 2022. International Journal of Remote Sensing 43(19-24): 7117-7138. https://doi.org/10.1080/01431161.2022.2155086\nunifir: A Unifying API for Interacting with Unity from R. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388\nterrainr: An R package for creating immersive virtual environments. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software, 7(69): 4060. https://doi.org/10.21105/joss.04060\n\n\n\nwaywiser | Ergonomic Methods for Assessing Spatial Models | 2023\nspatialsample | Spatial Resampling Infrastructure | 2022\nunifir | A Unifying API for Working with Unity in R | 2022\nterrainr | Retrieve Data from the USGS National Map and Transform it for 3D Landscape Visualizations | 2021"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Mike Mahoney",
    "section": "",
    "text": "I’m a PhD candidate at SUNY-ESF, working on new tools to make complex, broad-scale systems easier for humans to understand. Right now, that means I split my time between predictive modeling (to monitor forest carbon sequestration across New York State, track the development of early-successional forests, and more) and visualization (including using game engines as a GIS and making it easier to make reproducible VR environments for research). By training I’m either an ecologist (specializing in landscape and translational ecology) or environmental scientist; professionally, I’ve worked as a data analyst, software engineer, and chicken farmer.\nOn this site I keep a list of my publications, presentations, and my CV, as well as a technical blog."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes. Mahoney, MJ, Johnson, LK, Bevilacqua, E, and Beier, CM. 2022. GIScience & Remote Sensing 59(1): 1266-1280. https://doi.org/10.1080/15481603.2022.2103069\nClassification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast. Mahoney, MJ, Johnson, LK, Guinan, AZ, and Beier, CM. 2022. International Journal of Remote Sensing 43(19-24): 7117-7138. https://doi.org/10.1080/01431161.2022.2155086\nunifir: A Unifying API for Interacting with Unity from R. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388\nterrainr: An R package for creating immersive virtual environments. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software, 7(69): 4060. https://doi.org/10.21105/joss.04060\n\n\n\nwaywiser | Ergonomic Methods for Assessing Spatial Models | 2023\nspatialsample | Spatial Resampling Infrastructure | 2022\nunifir | A Unifying API for Working with Unity in R | 2022\nterrainr | Retrieve Data from the USGS National Map and Transform it for 3D Landscape Visualizations | 2021"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Presentations\n\n\n\n\nPresentations\n\nAs lead/presenting author:\n\n2023\n\n\n\nspatialsample: A tidy approach to spatial cross-validation.\n\n\nRegular talk, Greater Boston useR Group, Boston, MA, April 5 2023.\n\n\nSlides available at https://mm218.dev/boston_useR_2023\n\n\n\n\nMake reproducibility easy.\n\n\nInvited talk, Federation of Earth Science Information Partners (ESIP) January Meeting, Severna Park, Maryland (Virtual), January 23 2023.\n\n\nSlides available at https://mm218.dev/esip2023\n\n\n\n2022\n\n\n\nVirtual Environments for Communicating Changing Forests.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, Forest Ecosystem Monitoring Cooperative Conference, Burlington, VT, December 15 2022.\n\n\nSlides available at https://mm218.dev/femc2022\n\n\n\n\nBuilding trust in AI outputs: Approaches from NY’s forest carbon monitoring program.\n\n\nInvited talk, Federation of Earth Science Information Partners (ESIP) July Meeting, Pittsburgh, PA, July 22 2022.\n\n\nSlides available at https://mm218.dev/esip2022\n\n\n\n\nDetecting regenerating forestland at a landscape level.\n\n\nWith Lucas K. Johnson, and Colin M. Beier\n\n\nRegular talk, Ecological Society of America and Canadian Society for Ecology and Evolution Joint Annual Meeting, Montreal, Quebec, Canada, August 15 2022.\n\n\n\n\nunifir: A Unifying API for Working with Unity in R.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, useR! 2022 (Virtual), June 22 2022.\n\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass.\n\n\nWith Lucas K. Johnson, and Colin M. Beier\n\n\nRegular talk, North American Forest Ecology Workshop (Virtual), June 20 2022.\n\n\n\n\nIt’s not what it looks like: Learning to question assumptions when debugging ML models.\n\n\nRegular talk, Data Mishaps Night (Virtual), February 24 2022.\n\n\n\n\nUsing AI/ML to help New York manage lands for net zero carbon.\n\n\nInvited talk, Federation of Earth Science Information Partners (ESIP) January Meeting, Annapolis, MD (Virtual), January 20 2022.\n\n\nRecording available at https://youtu.be/k8AqtJFpYEk?t=3065\n\n\n\n2021\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass.\n\n\nWith Lucas K. Johnson, Eddie Bevilacqua, and Colin M. Beier\n\n\nPoster presentation, American Geophysical Union Fall Meeting, New Orleans LA, December 15 2021.\n\n\nPoster available at https://www.mm218.dev/papers/ground_filtering/poster.pdf\n\n\n\n\nInteractive landscape simulations for visual resource assessment.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, Visual Resources Stewardship Conference, SUNY-ESF, Syracuse, NY, October 21 2021.\n\n\nSlides available at https://mikemahoney218.github.io/2021-10-21-vrs/\n\n\n\n\nProducing Interactive 3D Landscape Visualizations in Unity Using Terrainr R Package.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nInvited workshop, Visual Resources Stewardship Conference, SUNY-ESF, Syracuse, NY, October 19 2021.\n\n\nMaterials available at https://mikemahoney218.github.io/2021-10-19-vrs-workshop/2021-10-19-vrs-workshop.html\n\n\n\n\nterrainr: Spatial Data Access and Visualization in R.\n\n\nInvited talk, Earth Science Information Partners, Severna Park MD, September 9 2021.\n\n\nSlides available at https://mm218.dev/esip2021.\n\n\nVideo recording available at https://youtu.be/xWZ7QQMr_AQ\n\n\n\n\nVirtual Environments: Using R as a Frontend for 3D Rendering of Digital Landscapes.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, useR! 2021, ETH Zürich, Zürich, Switzerland, July 6 2021.\n\n\nSlides available at https://mm218.dev/user2021.\n\n\nVideo recording available at https://www.youtube.com/watch?t=2202&v=tbt8ZsHm5eA\n\n\n\n\nAccessing the USGS National Map and Making 3D Maps with terrainr.\n\n\nWith Colleen Nell (lead author) and Lindsay Platt\n\n\nInvited workshop, USGS Center for Data Integration, Lakewood, CO, May 28 2021.\n\n\nMaterials available at https://www.mm218.dev/cdi/cdi.html.\n\n\n\n\nterrainr: Landscape Visualizations Using Data from the National Map.\n\n\nInvited talk, USGS National Geospatial Technical Operations Center, Denver, CO, February 5 2021.\n\n\n\n2018\n\n\n\nBeaver Foraging Preferences and Impacts on Forest Structure in the Adirondack Mountains of New York.\n\n\nWith John C. Stella\n\n\nRegular talk, Forest Ecosystem Monitoring Collective Conference, University of Vermont, Burlington, VT, December 2018.\n\n\n\n\nBeaver Foraging Preferences and Impacts on Forest Structure in the Adirondack Mountains of New York.\n\n\nWith John C. Stella\n\n\nRegular talk, Rochester Academy of Sciences Fall Scientific Paper Session, Geneseo, NY, November 2018.\n\n\n\nAs co-/non-presenting author:\n\n2022\n\n\n\nA Map‑based Stock Change Approach for Fine‑scale Biomass and Carbon Accounting in NYS.\n\n\nWith Lucas K. Johnson (lead author), and Colin M. Beier\n\n\nRegular talk, Forest Ecosystem Monitoring Cooperative Conference, Burlington, VT, December 15 2022.\n\n\n\n\nHistorical Time Series Biomass Modeling: To Train on Plots or Pixels?.\n\n\nWith Lucas K. Johnson (lead author), and Colin M. Beier\n\n\nRegular talk, American Geophysical Union Fall Meeting, Chicago, IL, December 14 2022.\n\n\n\n2021\n\n\n\nBroad‑scale forest biomass mapping: generating contiguous high‑resolution predictions using a spatio‑temporal patchwork of LiDAR coverages across a mixed‑use landscape.\n\n\nWith Lucas K. Johnson (lead author), Eddie Bevilacqua, and Colin M. Beier\n\n\nRegular talk, American Geophysical Union Fall Meeting, New Orleans LA, December 15 2021.\n\n\n\n\nGreening Up Before Growing Up: Challenges in Modeling Forest Biomass Recovery Post‑Harvest Using Satellite Imagery.\n\n\nWith Lucas K. Johnson (lead author) and Colin M. Beier\n\n\nRegular talk, Society of American Foresters National Convention, Virtual, November 2021.\n\n\n\n2019\n\n\n\nNutritional Impacts on Invasive Beech Scale Quantification in Beech Bark Disease Aftermath Forests.\n\n\nWith Gretchen A. Dillion (lead author), Stephanie Chase, and  Mariann Johnston.\n\n\nPoster presentation, New York Society of American Foresters Annual Meeting, Syracuse, NY, November 2019.\n\n\n\n2017\n\n\n\nAn Investigation of Nutritional Effects on Beech Bark Disease Causal Organisms.\n\n\nWith Gretchen A. Dillon (lead author),  Mariann Johnston, Vizma Leimanis, and Jason Stoodley\n\n\nPoster presentation, Forest Ecosystem Monitoring Collective Conference, University of Vermont, Burlington, VT, December 2017.\n\n\n\n\nAn Investigation of Nutritional Effects on Beech Bark Disease Causal Organisms.\n\n\nWith Gretchen A. Dillon (lead author),  Mariann Johnston, Vizma Leimanis, and Jason Stoodley\n\n\nPoster presentation, Rochester Academy of Sciences Fall Scientific Paper Session, Geneseo, NY, November 2017."
  },
  {
    "objectID": "posts/2022-02-18-progress-purpose-process/index.html",
    "href": "posts/2022-02-18-progress-purpose-process/index.html",
    "title": "Progress, Purpose, Process",
    "section": "",
    "text": "Today is my birthday, so please forgive me one of the most self-indulgent things I will ever write.\nTwo years ago I was living in Boston, working in tech, and a good bit bored. I was recognizing that I had spent most of the past year focusing on goals I had because they were the obvious next step, not because they were building me a life I wanted to live. I had lost a sense of agency over the future.\nLuckily, at the time, I had just come across this podcast called “Cortex”. Over the holiday season, the hosts had discussed how they structure their goals around a “yearly theme” – not New Years’ Resolutions, not specific targets that you either achieve or fail at, but a broad concept that they use to inform their decisions over the course of the year. The point is to gently nudge your life in the direction you want it to go. Here’s a video with more information about this; I recommend it.\nSo I decided that 23 would be my year of Progress; that I’d try and make a habit of making decisions that helped build momentum and kept me moving forward. I quit my job, started a PhD, moved states, and started building the personal and professional networks I rely on today. It wasn’t all perfect – about a month in there was a global pandemic I really wish I could have done something to prevent – but it nudged my life’s trajectory in a direction I’ve been happy with.\nSo when 24 came and it was time to change my theme, I decided to make it the year of Purpose. The year of Progress had involved saying yes to every opportunity that came my way; with Purpose, I wanted to make sure that I was being intentional with how I spent my time and making explicit decisions about where I’d invest my energy. I’m still not great at it, but I’ve gotten better at saying no when needed. I spent time improving my organization and time management to help me efficiently work through the things I said yes to. And I made a point of learning to consciously decide when to take time off, when to disconnect, and when to purposefully relax, so that I can now make those decisions without worrying about all the things I “should” be doing instead.\nNow it’s 25, and it’s time for a new theme. I think my plan is for the year of Process: I like where I am with my life right now, and I like how I spend my time. But I could be better and more careful about how I approach both of those things. I’m going to try to get better with details, to improve my memory around small things.\nI told you this would be indulgent. I know this sounds froofy. But it’s worked for me for two years now and I’m betting on round three. And I’m looking forward to seeing where this year takes me."
  },
  {
    "objectID": "posts/2023-03-21-waywiser-ropensci/index.html",
    "href": "posts/2023-03-21-waywiser-ropensci/index.html",
    "title": "waywiser is now a part of rOpenSci",
    "section": "",
    "text": "I’m thrilled to share that waywiser, my R package focused on providing framework-agnostic (but tidymodels-friendly) methods for assessing models fit to spatial data,1 has passed peer review and been accepted to rOpenSci. As always, the review process improved the package immensely, thanks to the thoughtful reviews of Virgilio Gómez-Rubio and Jakub Nowosad2, as well as the shepherding of Anna Krystalli and Paula Moraga as editors.\nAs of Monday, the reviewed version has officially made its way to CRAN. This is a huge update, bringing in a ton of new functions and improving consistency and speed across the package, and I’m excited to have it officially released.\nI’m also very excited to have put out a preprint describing the package, which goes a bit deeper into the logic of why and how the package implements the features it does. This is my first solo-authored paper,3 and by far and away the one with the most equations.4 Despite both of those, I’m pretty pleased with how this paper turned out; I think it’s a useful addition to the package documentation for users who want a more thorough explication of the scholarly background of package features."
  },
  {
    "objectID": "posts/2023-03-21-waywiser-ropensci/index.html#footnotes",
    "href": "posts/2023-03-21-waywiser-ropensci/index.html#footnotes",
    "title": "waywiser is now a part of rOpenSci",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI use this terminology in a number of places throughout the documentation to try and emphasize that nothing about the models themselves need to be spatial. The models don’t need to incorporate spatial information at all for these methods to be useful. I’ll admit that it’s a bit clunky, though, and have a habit of lapsing back to “spatial models”.↩︎\nOnly a little intimidating having my package reviewed by the authors of two of the best books on using R for spatial analysis!↩︎\nAnd frankly, I don’t intend to make it a habit. I highly appreciate working with collaborators, particularly with my current research group, and know that my work is usually better for having gone through multiple rounds of revisions before being released. I really missed that process while working on this paper!↩︎\n23 numbered equations! My undergraduate degree is in ecology, where each additional equation per page is associated with 28% fewer citations; four years ago I’d never have expected I’d be writing a paper that looks like this↩︎"
  },
  {
    "objectID": "posts/2023-03-03-parallel/index.html",
    "href": "posts/2023-03-03-parallel/index.html",
    "title": "How to run R jobs across multiple (local) computers",
    "section": "",
    "text": "I’ve got a small homelab going at the moment, where in addition to my daily workstation I’ve also got a NUC, Raspberry Pi, and Synology NAS running on my local network.1 I primarily use these other machines as always-on servers and storage for things like Telegraf, Influx and Grafana, mealie and paperless, but every so often it’s useful to run a long-running, high CPU job on the NUC, rather than tying up my main workstation. In those situations, I tend to use VS Code’s Remote Server, at least for anything too complex for just ssh’ing into the NUC and running commands in the CLI.\nAt the moment I’m working with some extremely long-running jobs which will take a few weeks to complete and are blockers for my other work. As a result, I’m not really worried about tying up my main workstation, if it means the jobs will run faster – in fact, I’d like to tie up as many computers as possible, if it means the jobs execute any faster.\nIn the past, I’ve tried to manually split up jobs into smaller pieces and run them independently on the different computers. This is a pain. I’ve also shifted to using targets for most analysis projects these days, in order to take advantage of its automatic DAG creation and state-saving. Manual splitting-and-execution really undermines targets’ automatic orchestration abilities, so I’ve needed to find a better way to split workloads across computers.\nIt turns out that better way is extremely straightforward,2 and I’m kicking myself for not finding it out earlier. Say you’ve got two machines with internal IP addresses of 192.168.1.001 and 192.168.1.002, each with a user some_user who has key-based access to ssh into the other machine. If you’re using future, setting a plan to work across both computers takes two function calls:\nAnd that’s it! Any future-enabled functions you use3 will be split across your machines. For my targets-based workflow, I just run targets::tar_make_future(workers = 2) to split the jobs up.\nTo push things further, you can also run multiple clusters on a single machine:\nOr set jobs to use multiple cores, either by nesting futures or using other forms of parallelism; for instance, my current job is primarily using terra for a lot of raster predictions, so by setting cores = future::availableCores() - 1 inside of terra::predict() I’m able to more-or-less max out both machines I’m running on."
  },
  {
    "objectID": "posts/2023-03-03-parallel/index.html#footnotes",
    "href": "posts/2023-03-03-parallel/index.html#footnotes",
    "title": "How to run R jobs across multiple (local) computers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith external access via tailscale.↩︎\nFor a value of straightforward that includes “maintaining multiple machines with similar-enough R environments, access to shared storage if necessary, and ssh access to each other on a private subnet”.↩︎\nHighly recommend either future.apply or furrr for ease-of-use, by the way.↩︎"
  },
  {
    "objectID": "posts/2023-04-01-fitting_models/index.html",
    "href": "posts/2023-04-01-fitting_models/index.html",
    "title": "Using in-line grouping to fit many models",
    "section": "",
    "text": "Tim Tiefenbach has a post on using dplyr and other tidymodels tools to fit many models in relatively few lines of code. Tim’s post walks through a lot of interesting functions for more complicated model fitting, which I’m not going to talk about at all. What I want to talk about is that I recently realized I don’t do much nesting in R anymore!\nThe building block in Tim’s post uses the new-ish dplyr::nest_by() to create a nested data frame, then fits models using the new data list column:\ndplyr::storms |&gt; \n  dplyr::nest_by(status) |&gt; \n  dplyr::mutate(\n    mod = list(lm(wind ~ pressure, data = data))\n  )\n\n# A tibble: 9 × 3\n# Rowwise:  status\n  status                                data mod   \n  &lt;fct&gt;                  &lt;list&lt;tibble[,12]&gt;&gt; &lt;list&gt;\n1 disturbance                     [146 × 12] &lt;lm&gt;  \n2 extratropical                 [2,068 × 12] &lt;lm&gt;  \n3 hurricane                     [4,684 × 12] &lt;lm&gt;  \n4 other low                     [1,405 × 12] &lt;lm&gt;  \n5 subtropical depression          [151 × 12] &lt;lm&gt;  \n6 subtropical storm               [292 × 12] &lt;lm&gt;  \n7 tropical depression           [3,525 × 12] &lt;lm&gt;  \n8 tropical storm                [6,684 × 12] &lt;lm&gt;  \n9 tropical wave                   [111 × 12] &lt;lm&gt;\nThis is a pattern I used to use all the time1 – my undergrad thesis and eventual first pub from the same were built from strings of nesting and unnesting data and models. But over the years, I’ve realized that you can often get the same results using grouped data frames in the place of nested ones, and have shifted to using dplyr::summarise() and friends instead of nesting:\ndplyr::storms |&gt; \n  dplyr::summarise(\n    mod = list(lm(wind ~ pressure, data = dplyr::pick(dplyr::everything()))),\n    .by = status\n  )\n\n# A tibble: 9 × 2\n  status                 mod   \n  &lt;fct&gt;                  &lt;list&gt;\n1 tropical depression    &lt;lm&gt;  \n2 tropical storm         &lt;lm&gt;  \n3 extratropical          &lt;lm&gt;  \n4 hurricane              &lt;lm&gt;  \n5 subtropical storm      &lt;lm&gt;  \n6 subtropical depression &lt;lm&gt;  \n7 disturbance            &lt;lm&gt;  \n8 other low              &lt;lm&gt;  \n9 tropical wave          &lt;lm&gt;\nNow, the obvious downside of using a grouped data frame instead of a nested one is that future function calls no longer have access to your raw data frame. The slightly less obvious downside is that the output of dplyr::nest_by() is a rowwise data frame, which makes it easy to pass our model objects directly to other functions like broom::tidy():\ndplyr::storms |&gt; \n  dplyr::nest_by(status) |&gt; \n  dplyr::mutate(\n    mod = list(lm(wind ~ pressure, data = data)),\n    res = list(broom::tidy(mod))\n  )\n\n# A tibble: 9 × 4\n# Rowwise:  status\n  status                                data mod    res             \n  &lt;fct&gt;                  &lt;list&lt;tibble[,12]&gt;&gt; &lt;list&gt; &lt;list&gt;          \n1 disturbance                     [146 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n2 extratropical                 [2,068 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n3 hurricane                     [4,684 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n4 other low                     [1,405 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n5 subtropical depression          [151 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n6 subtropical storm               [292 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n7 tropical depression           [3,525 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n8 tropical storm                [6,684 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n9 tropical wave                   [111 × 12] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\nThe outputs of the grouped data frame method are not rowwise data frames, which means we need to use another function to iterate through each element of mod. I usually use purrr::map() for this:\ndplyr::storms |&gt; \n  dplyr::summarise(\n    mod = list(lm(wind ~ pressure, data = dplyr::pick(dplyr::everything()))),\n    res = purrr::map(mod, broom::glance),\n    .by = status\n  )\n\n# A tibble: 9 × 3\n  status                 mod    res              \n  &lt;fct&gt;                  &lt;list&gt; &lt;list&gt;           \n1 tropical depression    &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n2 tropical storm         &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n3 extratropical          &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n4 hurricane              &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n5 subtropical storm      &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n6 subtropical depression &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n7 disturbance            &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n8 other low              &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\n9 tropical wave          &lt;lm&gt;   &lt;tibble [1 × 12]&gt;\nPart of the reason I use purrr for this is that purrr provides plenty of other helper functions for working with list columns; for instance, I tend to use purrr::chuck() to extract model fit statistics:\ndplyr::storms |&gt; \n  dplyr::summarise(\n    mod = list(lm(wind ~ pressure, data = dplyr::pick(dplyr::everything()))),\n    res = purrr::map(mod, broom::glance),\n    rsquared = purrr::map_dbl(res, purrr::chuck, \"r.squared\"),\n    .by = status\n  )\n\n# A tibble: 9 × 4\n  status                 mod    res               rsquared\n  &lt;fct&gt;                  &lt;list&gt; &lt;list&gt;               &lt;dbl&gt;\n1 tropical depression    &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.0369\n2 tropical storm         &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.485 \n3 extratropical          &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.631 \n4 hurricane              &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.807 \n5 subtropical storm      &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.473 \n6 subtropical depression &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.214 \n7 disturbance            &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.299 \n8 other low              &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.332 \n9 tropical wave          &lt;lm&gt;   &lt;tibble [1 × 12]&gt;   0.126\nAnyway – there’s nothing better or worse (as far as I’m concerned) with either nesting or grouping for fitting many models; I just think it’s interesting that my personal style has shifted over time to use much more grouping, and much less nesting.\nAnd as I said at the start, Tim’s post walks through a lot of interesting functions for more complicated model fitting, which I’m not going to talk about at all; here’s the link again if you’re interested."
  },
  {
    "objectID": "posts/2023-04-01-fitting_models/index.html#footnotes",
    "href": "posts/2023-04-01-fitting_models/index.html#footnotes",
    "title": "Using in-line grouping to fit many models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing tidyr::nest(), though.↩︎"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "",
    "text": "terrainr version 0.4.0 is now on CRAN! This version is a relatively minor update that shouldn’t impact most workflows, but makes some changes to improve the logic and consistency of the package. The rest of this post runs through the changes you can expect if you update.packages() any time soon!"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#whats-a-terrainr",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#whats-a-terrainr",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "What’s a terrainr?",
    "text": "What’s a terrainr?\nterrainr is an R package for the retrieval and visualization of spatial data. It provides functions to download elevation data and basemap tiles for points within the United States (using public domain data from the USGS National Map), visualize them in R via ggplot2, and process them for 3D visualization using the Unity 3D engine. You can see the GitHub repo here!"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#merge_rasters-can-handle-tiles-with-different-numbers-of-bands",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#merge_rasters-can-handle-tiles-with-different-numbers-of-bands",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "merge_rasters can handle tiles with different numbers of bands",
    "text": "merge_rasters can handle tiles with different numbers of bands\nThe old implementation of merge_rasters was very bulky, read all your map tiles into memory at once, and was a bit of a mess to maintain thanks to the large number of paths you could theoretically take through the code. The commit (suggested via rOpenSci review!) replacing it with gdalwarp via sf is probably the single best code improvement I’ve made to this repo. Unfortunately, the old method could also handle merging rasters with differing numbers of bands, while the simple gdalwarp fix couldn’t.\nSo the old implementation is back as an internal method while I look for a better solution to this problem. merge_rasters will now attempt to use gdalwarp to merge your input files and then fall back to (a massively simplified version of) the older version if gdalwarp fails.\nAs for why you’d want to automatically merge rasters with different numbers of bands, well…"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#get_tiles-doesnt-auto-download-transparency-values-for-naip",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#get_tiles-doesnt-auto-download-transparency-values-for-naip",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "get_tiles doesn’t auto-download transparency values for NAIP",
    "text": "get_tiles doesn’t auto-download transparency values for NAIP\nNAIP orthophotography provides fantastic continuous 1-meter images for the continental United States. When downloading these photos with the argument transparency = true, which used to be the default, most photos don’t have any transparent pixels to talk about and as such are returned and saved as 3 band rasters (RGB images). Some photos, however, do have such pixels and are returned with a 4th alpha band. This causes problems with gdalwarp as well as image editing software, and the majority of the time users are not better served by these pixels being transparent.\nAs a result, this version changes the default transparency argument for get_tiles and hit_national_map_api to false when downloading NAIP images (no other data source is impacted). This is one of the reasons this version gets a 0.x.0 number – while it should be a small change, the same inputs to functions no longer returns the same outputs (though I doubt people would notice), so I’m counting this as a breaking change.\nThere’s a slightly more impactful breaking change worth noting though:"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#functions-pay-attention-to-the-provided-crs",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#functions-pay-attention-to-the-provided-crs",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "Functions pay attention to the provided CRS",
    "text": "Functions pay attention to the provided CRS\nThis header is actually about two distinct changes.\nFirst, another new behavior with get_tiles is that rather than assuming the provided data and downloaded image should both be using the WGS84 CRS (EPSG 4326), get_tiles will now infer the EPSG CRS from any provided sf or Raster object. If the numeric code is missing, this function will still assume 4326.\nSimilarly, rather than specifying target_crs in vector_to_overlay, this function will now return an overlay projected to match reference_raster’s CRS. Missing CRS are handled slightly differently here – if the error_crs argument is NULL, this function will warn; if FALSE it will assume 4326, and if TRUE it will interrupt the function with an error.\nThose are the major changes to this iteration! On top of these there are some minor changes to the package internals, slowly removing dead code paths and simplifying things behind the scenes. If you have any problems (bugs or missing features) with the package, feel free to open an issue!"
  },
  {
    "objectID": "posts/2022-08-15-last-summer/index.html",
    "href": "posts/2022-08-15-last-summer/index.html",
    "title": "I know what I did last summer",
    "section": "",
    "text": "I’m wrapping up three months as an open-source intern with Posit (née RStudio). I wanted to write a small post talking a bit about what it was like and what we got done in that timespan. So, well, this post1."
  },
  {
    "objectID": "posts/2022-08-15-last-summer/index.html#spatialsample",
    "href": "posts/2022-08-15-last-summer/index.html#spatialsample",
    "title": "I know what I did last summer",
    "section": "spatialsample",
    "text": "spatialsample\nThe main thrust (and originally, the entire scope) of the internship was writing the spatialsample package for spatial cross-validation. While not an entirely new package (the first version hit CRAN in 2021), by the end of June we had added support for:\n\nsf objects (and more correct distance calculations with geographic coordinates)\nexclusion buffers around assessment folds\ninclusion radii around assessment folds\nspatial clustering with more than just k-means clustering\nspatially buffered V-fold CV\nleave-location-out CV\nspatial block CV\nautomatic plotting\n\n\n\n\nThe results of calling autoplot() on a spatial clustering CV object, for the city of Boston.\n\n\nThat meant a pretty huge update to the package, and that version was released on CRAN as version 0.2.0. A small patch to speed up buffering, work better with simulated data, and make me the official package maintainer as Julia moves to focus more on ModelOps just hit CRAN last Friday.\nI also wrote a package vignette on spatial buffering, and an article for tidymodels.org on how to use spatialsample for multi-scale model assessment.\nWe chose to not release spatialsample 1.0.0 as part of the broader tidymodels 1.0.0 release because the package is still very young – most of its features have only been on CRAN for a month or so, and we want more feedback on what else the package should do and any areas that people think we got wrong. However, I feel really good about where the package is right now, and am really excited to see people use it (and to use it in my own work going forward)."
  },
  {
    "objectID": "posts/2022-08-15-last-summer/index.html#rsample",
    "href": "posts/2022-08-15-last-summer/index.html#rsample",
    "title": "I know what I did last summer",
    "section": "rsample",
    "text": "rsample\nAfter the spatialsample release, I had the opportunity to start poking around the open issues on the rsample package. As rsample is a much more established package, it was important to focus on only the changes we could make without breaking people’s code; with that said, we were able to add a few highly-requested features:\n\nImproved support for grouped resampling, adding functions for grouped Monte Carlo CV, grouped bootstrap resampling, creating validation sets with groups, and creating initial training/testing splits with groups. This was the most-requested feature still missing from rsample, and it’s a personal point of pride that I was able to implement it (in a way that doesn’t seem awful to maintain).\nAdded reverse_splits() to “swap” the analysis and assessment sets of an rsplit or rset object.\nAdded reshuffle_rset() to re-generate an rset, using all the same parameters that were used to create it the first time (but potentially, a different random seed). This was the oldest open issue on rsample, and even though it wasn’t in particularly high demand I was very happy to close both the oldest and most requested issues on the package in a single release.\nStandardized the classes and attributes attached to the objects returned by rsample functions.\n\nI also added a new overview vignette covering some of the most commonly used functions in the package, which I think will be a good first point of entry for a lot of users.\nAll of this hit CRAN as rsample 1.1.0 on Monday. I’m particularly excited for the grouped resampling functions to hit CRAN, since I think those will solve a very common use-case for a lot of people working with repeated measures data."
  },
  {
    "objectID": "posts/2022-08-15-last-summer/index.html#waywiser",
    "href": "posts/2022-08-15-last-summer/index.html#waywiser",
    "title": "I know what I did last summer",
    "section": "waywiser",
    "text": "waywiser\nOn top of these resampling projects, I’ve also been noodling on a new package in my personal namespace, called waywiser. This is a yardstick extension package4 focused on helping people assess spatial autocorrelation in model residuals.\nVersion 0.1.0 is inching along to CRAN at the moment, as I clean off a few last new package checks, and should be viewed as mostly experimental – I’m currently figuring out how I can get spatial information from inside of tune::fit_resamples(), and the default neighbor lists and weights matrices will probably be outsourced to sfdep in the next version. But it exists!"
  },
  {
    "objectID": "posts/2022-08-15-last-summer/index.html#footnotes",
    "href": "posts/2022-08-15-last-summer/index.html#footnotes",
    "title": "I know what I did last summer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy the way, the title of this post is a joke about a 25 year old horror film: https://en.wikipedia.org/wiki/I_Know_What_You_Did_Last_Summer↩︎\nI was in the middle of moving to Boston at this point, and this work week took place towards the end of May. So I did a very funny thing where I drove from Syracuse to Boston, stayed in a beautiful hotel and went to the office every day, then drove back to Syracuse on Sunday… only to drive back to Boston two days later, move into a slightly less beautiful apartment, and never go to the office again.↩︎\nI feel the need to clarify this is a joke; next year’s conference will be posit::conf() following the company’s name change. I had been joking that Hadley should kick off the announcement with “I do have some major news to share: this will be the last ever RStudio::conf()”, but that would probably have unleashed a purely irresponsible level of panic on Twitter.↩︎\nAnd “waywiser” is an old name for a surveyor’s wheel, get it?↩︎\nWhich is not itself actually affiliated with arXiv at all, but rather a modified version of the NeurIPS template published by an individual researcher. This is confusing; I decided that continuing to refer to it as an “arXiv template” was the least confusing option, but maybe the template should actually be called “preprint template” instead. It’s a very nice template, I like it a lot.↩︎\nThis link doesn’t work at the time of publication, but will eventually point to the article once the online version gets published.↩︎"
  },
  {
    "objectID": "posts/2021-06-14-interactive-landscape-simulations-for-visual-resource-assessment/index.html",
    "href": "posts/2021-06-14-interactive-landscape-simulations-for-visual-resource-assessment/index.html",
    "title": "Interactive landscape simulations for visual resource assessment",
    "section": "",
    "text": "I’ll be speaking at the 2021 Visual Resources Stewardship Conference, presenting a paper (“Interactive landscape simulations for visual resource assessment”, with Colin Beier and Aidan Ackerman) talking about how the use of game engines for spatial visualization can help in visual resource assessment projects.\n\n\n\n\n\nEverything the light touches can see & be seen by the red dot. Users can interactively walk around the landscape to see for themselves if the viewshed algorithm gets things exactly right.\n\n\n\n\nThis was a really fun project, forcing me to push these visualizations in a new direction. The conference itself is in October, and I’ve posted a preprint of the paper at this link.\n[Update - 2021-06-24] And the code and manuscript used for this paper are now on Github at this link."
  },
  {
    "objectID": "posts/2022-01-13-terrainr-an-r-package-for-creating-immersive-virtual-environments/index.html",
    "href": "posts/2022-01-13-terrainr-an-r-package-for-creating-immersive-virtual-environments/index.html",
    "title": "terrainr: An R package for creating immersive virtual environments",
    "section": "",
    "text": "I’ve got a new paper out at the Journal of Open Source Science, documenting the “terrainr” package I’ve been developing since late 2020. JOSS is a fantastic journal and is entirely open-source, so that first link should work for everyone.\n\n\n\n\n\nLeft: Where typical geodata-to-IVE workflows leave you. Right: Where terrainr gets you.\n\n\n\n\nThe JOSS workflow is absolutely fascinating. I made a git branch in the terrainr repo with the files needed to generate the paper, then (through a web form) opened a pre-review issue on GitHub. The editors used that issue to assign a handling editor, then moved things over to a review issue, where the paper was processed and prepared for submission via an automatically generated PR. The whole thing was incredibly efficient, and also entirely transparent – the logs of the paper processing workflow will be available until GitHub finally shuts down, a refreshing change compared to standard peer review.\nThe paper itself is a general overview of the terrainr package, trying to be a new enough contribution that it’s not just repeating the package documentation but not adding enough to spin off into an entirely separate research project. It’s also very short – a nice feature of JOSS papers – so rather than summarize here I’ll just suggest you read it yourself."
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html",
    "href": "posts/2022-12-12-tools/index.html",
    "title": "The tools I loved this year",
    "section": "",
    "text": "One of the problems with open source development is that it’s the engineering equivalent of yelling into the void. Once your code is released, it’s really, really hard to know if anyone actually uses your tool. You might be able to track download numbers,1 but there’s almost no way to tell how many of those downloads are actual users, and how many are from automated webscrapers, unofficial mirrors, or people who will try to use your software once before giving up and moving on to something else.2 The best information about how many people use your software is, basically, how much you hear people talking about your software.\nThe problem is that, for the majority of open source projects, you’ll only hear people talk about your software when it’s pissing someone off. People don’t generally open issues on GitHub to tell you they love your work,3 but rather to tell you that your work messed up in a big enough way that they’re going to spend time writing you letters about it. On my more meditative days, I’m able to look at issues as “someone cares enough about this project to spend their time telling me what they think would improve it”; on my more human ones, it can feel like someone sharing a public list of my mistakes.4\nSo, with all this in mind and full up on the holiday spirit, I’m putting here a non-exhaustive list of the open-source tools that I loved this year, in no particular order, with a bent towards independent developers and smaller projects. These are primarily R projects, with a few other tools thrown in. Apologies to anyone I’ve forgotten; please know that you would not know I had ever used your software if I didn’t love it."
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html#workflow",
    "href": "posts/2022-12-12-tools/index.html#workflow",
    "title": "The tools I loved this year",
    "section": "Workflow",
    "text": "Workflow\n\ntargets by Will Landau\nEvery so often you stumble across a project that makes it obviously, blindingly clear that you’ve been doing things wrong this whole time. Sometimes this means finding out that your efforts have been wrong, and you’ve been doing things incorrectly up until now; more exciting are the times you realize you haven’t been doing anything wrong per se, but could be doing things so much better.\nThe targets package is a peak example of the latter. The ways analyses are usually structured – cells of a single notebook, shell scripts that run a smattering of individual files, cludgy Make files – are fine, and serve a purpose, and there’s nothing wrong with them. There’s just a better way. And targets is that better way.\nRather than making you define the dependencies between your analysis steps yourself, targets will automatically calculate your project’s DAG and run your scripts in the order they need to be executed. If you change a script, or files change on disk, targets will invalidate only the steps that need to be re-run under the new conditions, and will run only those steps on your next go. This alone is incredible.\nBut targets does so much more than that. My personal needs have meant I’ve spent a lot of time with targets’ dynamic branching mechanism, where you can tell targets to execute a script against various combinations of input parameters, massively reducing the amount of actual code you need to write. There’s plenty of additional bells and whistles attached for dealing with distributed computing and cloud data environments. Plus, there’s tons of metrics and instrumentation available to watch your DAG as it executes, and the visuals produced by the package are beautiful:\n\nPart of the joy of targets is just how well-documented the package is. There’s a website. There’s an entire book. Will has set an incredibly high bar for the rest of us.\n\n\nPaperless-ng by Jonas Winkler\nA docker container runs on a small server in my living room,5 running my instance of Paperless-ng. When I save a new article to its “input” folder, the container OCR’s the document and moves it to a safe storage location, giving it the appropriate “tags” and making it accessible with the rest of the papers I’ve ever shown an interest in.\n\n\n\nI swear I’ve read (most of) these, I just am not great at updating my tags.\n\n\nPaired with a VPN6, this means I can access my entire reference library from anywhere I go, from any device I own, with access to full-text search and any notes I’ve taken. This is one of the outright best hacks I have ever found for literature searches; I have access to a personally-curated Google Scholar filled with the sources that I have found relevant in the past, which makes finding the proper citation or reference document a breeze. If you’re a certain breed of nerdy academic, I can’t recommend it enough.\n\n\nfuture and progressr by Henrik Bengtsson\nMy research group is split across folks who use Windows, MacOs, and Linux, and we need to share code a lot. That makes relying on system features, like multi-core processing via mclapply() in the parallel package, rather fraught.\nOur solution to that problem is the future package, which provides functions for parallel computing which function across OSes. That alone merits a spot on this list. But even more than that, I’m a huge fan of the philosophy behind the future package.\nSay for instance you’re developing a package – or writing a script for a coworker, or generally writing code that you will not be executing yourself. Your code looks something like this:\n\nintense_function &lt;- function(...) {\n  list_to_iterate_over &lt;- list(...)\n  lapply(\n    list_to_iterate_over,\n    some_intensive_processing_function\n  )\n}\n\nYou have no idea what else your user will be doing when you execute this. Maybe they’re busy running a more important job that needs most of their RAM, or maybe your script can safely use all their computing resources; while writing your function, you can’t know. For that reason, you don’t want to enable parallelism by default.\nSo what do you do? Let the user decide how parallel they want your code to be. If you write your code using packages from the future family, your users can use the plan() function to specify how much parallelism they’re comfortable with your script using. And gaining that amount of flexibility requires a one-line change:\n\nintense_function &lt;- function(...) {\n  list_to_iterate_over &lt;- list(...)\n  future.apply::future_lapply(\n    list_to_iterate_over,\n    some_intensive_processing_function\n  )\n}\n\nYou don’t need to write any logic around parallelization; all of that work gets outsourced to future.\nFor similar reasons, I really like the progressr package that’s part of the future ecosystem. Adding progress bars is as simple as specifying how many iterations you’re expecting to do, and when the bar should update:\n\niteration_function &lt;- function(n = 10) {\n  p &lt;- progressr::progressor(n)\n  lapply(\n    seq_len(n),\n    \\(x) {\n      p(message = sprintf(\"On iteration %f\", x))\n      Sys.sleep(0.1)\n    }\n  )\n  invisible(NULL)\n}\n\nAnd then all the rest is left to the user. They can use whatever sort of progress bar they find useful, or none at all – you don’t need to be a part of that decision whatsoever:\n\nprogressr::handlers(\"txtprogressbar\")\nprogressr::with_progress(\n  iteration_function()\n)\n\n\n\nunits by Edzer Pebesma\nI’m gonna be honest, this is a weird one for the workflow section. But at the same time, the units package has become something I load in almost every project I work on. If you’re doing anything that might have unit conversions, this package can be a lifesaver.\nFor instance, let’s take an actual example from a project I worked on last week. Say you’ve run some samples through an ICP machine, and need to calculate how much of a given analyte7 is in your sample.\nTo do that, you’ve got three numbers: the concentration of the analyte in the solution you ran through the ICP in units of milligrams per liter, the amount of solution you used in units of milliliters, and the amount of sample in the solution in grams. Your target is the amount of analyte per amount of sample, in units of milligrams per gram. This is simple dimensional analysis8, and you set up your equations like so:\n\nconcentration_mg_per_L &lt;- 1.68\nsample_weight_g &lt;- 0.29\nsolution_volume_mL &lt;- 53.54\n\nsolution_volume_L &lt;- solution_volume_mL / 1000\nconcentration_mg &lt;- concentration_mg_per_L / solution_volume_L\nconcentration_mg / sample_weight_g\n\n[1] 108.2014\n\n\nSee the bug?\nThis is the sort of thing that can kill an analysis; silent logic bugs that execute fine but invalidate all of your results. If we tag our measurements with the units they represent, however, it’s a little bit easier to see what went wrong here:\n\nconcentration_mg_per_L &lt;- units::set_units(1.68, \"mg / L\")\nsample_weight_g &lt;- units::set_units(0.29, \"g\")\nsolution_volume_mL &lt;- units::set_units(53.54, \"mL\")\n\nconcentration_mg = concentration_mg_per_L / solution_volume_mL\nconcentration_mg / sample_weight_g\n\n0.0001082014 [1/L/mL]\n\n\nThe units package automatically converts between units, which means we can get rid of the manual conversion from milliliters of solution into liters. The package also tells us what units our results are in following calculation. Here we can see that our results aren’t in the units we’re after – we want a concentration (amount of analyte per unit of sample), not something measured in “1 per liter per milliliter”.\nThis tips us off that we made a mistake in our conversions somewhere, and indeed we flipped an operand – rather than dividing our concentration by the amount of solution, we need to multiply it:\n\nconcentration_mg = concentration_mg_per_L * solution_volume_mL\nconcentration_mg / sample_weight_g\n\n0.0003101628 [1]\n\n\nThat gives us our desired concentration: the number of grams of analyte per gram of sample. We can easily convert that to other forms using the units package; for instance, we tend to work with concentrations in units of milligram of analyte per gram of sample:\n\n(concentration_mg / sample_weight_g) |&gt; units::set_units(\"mg / g\")\n\n0.3101628 [mg/g]\n\n\nBut we can be even more explicit than that. The units package lets us install our own units, which lets us tag what these numbers are a gram of. That means that we can automatically see that our results are natively a ratio of total analyte per unit of sample:\n\nunits::install_unit(\"analyte\")\nunits::install_unit(\"sample\")\nunits::install_unit(\"solution\")\n\nconcentration_mg_per_L &lt;- units::set_units(1.68, \"(mg * analyte) / (L * solution)\")\nsample_weight_g &lt;- units::set_units(0.29, \"g * sample\")\nsolution_volume_mL &lt;- units::set_units(53.54, \"mL * solution\")\n\nconcentration_mg = concentration_mg_per_L * solution_volume_mL\n\n(concentration_mg / sample_weight_g)\n\n0.0003101628 [analyte/sample]\n\n\nAnd we can purposefully convert that to units of miligrams of analyte per grams of sample, in a very explicit way:\n\n(concentration_mg / sample_weight_g) |&gt; \n  units::set_units(\"(mg * analyte) / (g * sample)\")\n\n0.3101628 [analyte*mg/g/sample]\n\n\nThis erases an entire category of bugs, and has been big for me this year. The tidyverse team talks a lot about the idea of trying to help people fall into a “pit of success” – that tools should make it very easy to do things right, and very hard to do things wrong. The units package feels like a fantastic example of the concept.9"
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html#visualization",
    "href": "posts/2022-12-12-tools/index.html#visualization",
    "title": "The tools I loved this year",
    "section": "Visualization",
    "text": "Visualization\n\nggdist by Matthew Kay\nI think I am extremely, extremely late to the ggdist party. But that’s okay, because ggdist is extremely, extremely good; there’s plenty to party about.\nI really feel like the best advertisement for this package is the package website; the basic demonstration graphs on that page are beautiful. The toy example plots look better than some10 of my published figures.\n\ndplyr::starwars |&gt; \n  tidyr::unnest(films) |&gt; \n  ggplot2::ggplot(\n    ggplot2::aes(x = height, y = films, fill = films)\n  ) + \n  ggdist::stat_slab(scale = 0.5) + \n  ggdist::stat_dotsinterval(\n    side = \"bottom\", \n    scale = 0.5, \n    interval_alpha = 0, \n    point_alpha = 0, \n    slab_size = NA\n  ) +\n  ggplot2::scale_fill_brewer(palette = \"Set2\") + \n  ggdist::theme_ggdist()\n\n\n\n\nggdist is a late addition to my arsenal; I used it to make graphs while revising a manuscript and was immediately hooked. The toolkit ggdist provides is incredibly flexible, enabling both dozens of radically different representations of the same data as well as the infinite infinitesimal tweaks that we all obsess over to get our graphs just right. I’m a huge fan so far.\n\n\nkableExtra by Hao Zhu\nA confession: every two months11 I try a new package for making tables,12 use it for a week, and immediately go back to kableExtra. There’s just really nothing like it for consistent, professional, beautiful, and controllable tables, particularly for PDF documents.\n\nmtcars |&gt; \n  head() |&gt; \n  kableExtra::kbl() |&gt; \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    fixed_thead = TRUE\n  )\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\nColumn widths! Consistent APIs to control header rows, grouping rows, data rows! Consistent APIs to control columns! Consistent APIs to control individual cells! kableExtra is a well-designed package. Plus, kableExtra ships with incredible documentation, which is extremely easy to search; my experience is that this package is written the way that you intuitively want a table-making package to be written. Every table in every paper I’ve written13 has been run through kableExtra, and I’m expecting that trend to continue going forward.\n\n\nggsflabel by Hiroaki Yutani\nPutting labels on maps is a recurring meme among GIS users. This is one of those things that feels like it should not be that hard, and turns out to actually be impossible.\nBut somehow, ggsflabel gets it… right? Almost every time? It’s magic. There’s literally no other explanation than magic. You could sell this to any university with an ArcMap subscription for thousands of dollars, but instead I installed it for free from GitHub.\n\nnc &lt;- sf::read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nggplot2::ggplot(nc) + \n  ggplot2::geom_sf() + \n  ggsflabel::geom_sf_label_repel(ggplot2::aes(label = NAME), size = 2)\n\n\n\n\n\n\nggspatial by Dewey Dunnington\nJust like label placement, every other part of making a map is surprisingly hard. Making coordinate reference systems play nicely with plotting libraries is hard, adding directionally-aware elements to a map is hard, adding scale bars and other distance-aware elements to a map is hard.\nThe ggspatial package makes it easier. My research group uses it extensively for our north arrows and scale bars, but the entire package is a gem. It solves a problem and does it well.\n\nnc &lt;- sf::read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nggplot2::ggplot(nc) + \n  ggplot2::geom_sf() + \n  ggspatial::annotation_north_arrow(location = \"br\") + \n  ggspatial::annotation_scale()\n\n\n\n\n\n\npatchwork by Thomas Lin Pedersen\nFor the longest time, it was surprisingly tricky to create multi-panel plots in R. The implementations that did exist required you to think a little bit too much about how R thinks about drawing graphics, which has never been a strong suit of mine. Dealing with legends and other annotations was also often a pain.\nEnter patchwork, which makes combining plot objects together an absolute breeze:\n\nlibrary(patchwork)\n\np1 &lt;- ggplot2::ggplot(Orange, ggplot2::aes(age, circumference, color = Tree)) + \n  ggplot2::geom_line()\n\np2 &lt;- ggplot2::ggplot(mtcars, ggplot2::aes(wt, drat)) + \n  ggplot2::geom_point()\n\np1 + p2 + \n  plot_annotation(tag_levels = 'A') & \n  ggplot2::theme(plot.tag = ggplot2::element_text(size = 8))\n\n\n\n\nJust about all of my multi-panel figures are made with patchwork these days, because I find it to be exactly as flexible as I need for my visuals. Perhaps the best compliment I can give patchwork is that I barely ever need to think about it – when I need to create multi-panel graphs, I load patchwork and then things Just Work from there."
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html#spatial",
    "href": "posts/2022-12-12-tools/index.html#spatial",
    "title": "The tools I loved this year",
    "section": "Spatial",
    "text": "Spatial\n\nterra by Robert Hijmans\nI’m not going to lie: I was dreading the R spatial migration. I have so many legacy projects relying on raster and friends,14 and was expecting the transition to be an incredible headache without bringing any real benefits to my work.\nI could not have been more wrong. Switching workloads to terra has been a fantastic investment across our research group. The terra package is faster than raster, and benefits from over a decade15 of lessons learned from the raster package. The breadth of operations implemented is incredible as well; a weekly conversation in my lab involves someone16 asking “how do I do X?”, where X is some complex calculation that would be incredibly difficult to implement, and someone17 answering “oh, use this one-liner from terra.”\nThe terra package implements the functions you expect for a raster toolkit, with the names you expect for a raster toolkit, which work in the way you expect for a raster toolkit. For a relatively new package (the first CRAN release was in 2020), terra feels incredibly mature, and is an absolute joy to work with.\n\nloi &lt;- tmaptools::geocode_OSM(\"Hyampom California\")$coords\nloi &lt;- data.frame(x = loi[[\"x\"]], y = loi[[\"y\"]])\nloi &lt;- sf::st_as_sf(loi, coords = c(\"x\", \"y\"), crs = 4326)\nloi_file &lt;- tempfile()\noutput_tiles &lt;- terrainr::get_tiles(\n  terrainr::set_bbox_side_length(loi, 8000),\n  resolution = 30,\n  output_prefix = loi_file\n)\n\nterra::rast(output_tiles$elevation) |&gt; \n  terra::plot()\n\n\n\n\n\n\nGDAL by Frank Warmerdam, Even Rouault, and others\nFor me, 2022 was the year of CLI GDAL commands. I have now written two papers entirely on the back of shell scripts calling gdal_calc and gdalwarp.\nFor those with normal hobbies, GDAL is a software library that describes itself as a “translator library” between raster and vector formats. In practice, however, GDAL is a full-featured raster toolkit with pretty decent vector support; a huge amount of common raster operations can be run by chaining together GDAL commands. And that’s huge, because GDAL is fast and can handle much, much larger data than R can.\nRecommending GDAL to people who work with spatial data is a little bit like recommending breathing air,18 given its dominant position in the spatial software ecosystem. But still, this year in particular, I found myself really, really appreciating having this tool at my disposal.\n\n\nsf by Edzer Pebesma\nPerhaps unsurprisingly, the sf package continues to be an absolute delight. sf is the 14th most popular package when counting CRAN downloads; if you work with spatial data, you know about sf. Heck, odds are, if you don’t work with spatial data, you probably also know about sf. I used it in the last three code examples, because it is that core to how I think about doing spatial data analysis in R.\nsf is another package that has, with shocking regularity, already implemented the thing you’re trying to do. The dplyr integration is fantastic; the ggplot2 integration is fantastic; the ability to call directly to GDAL is fantastic. I remain a huge fan.\n\n\nlandscapemetrics by Maximilian Hesselbarth et al.\nImagine, if you will, that everyone – quite literally every single person – in your field uses tool X. X is mostly focused on calculating statistics, and because of its dominance most of those statistics are known primarily as “the X set of statistics”. Most people in your field don’t know how to calculate the statistics without X and aren’t particularly interested in trying; a “correct” statistic is one that agrees with tool X.\nNow, imagine tool X is closed-source, only runs on Windows, and was first released in 1995, so doesn’t exactly integrate with other software. In order to address those drawbacks, a team of scientists develop an R package that calculates the same statistics as X. This is already incredibly impressive; I cannot stress enough that everyone uses X and expects your results to match it exactly, and sometimes you just can’t figure out how to precisely match the closed-source Windows-only software. This was a big job.\nNow imagine that two years later, the person who wrote tool X retires and every trace of tool X is erased from the internet. This suddenly becomes a much bigger job.\nThat’s, as best as I can tell from the outside, what happened to the team behind landscapemetrics. FRAGSTATS existed and was the standard reference for a whole boat of statistics;19 landscapemetrics provided an open-source implementation; FRAGSTATS suddenly no longer existed. I don’t want to sound like I’m criticizing FRAGSTATS here – for a very long time, that software provided an incredible service for free to a huge number of researchers, and I don’t think releasing something on the Internet creates an infinite obligation to make sure the download links never expire.\n\nlandscapemetrics::calculate_lsm(landscapemetrics::landscape, level = \"patch\")\n\n\n\n  \n\n\n\nBut it still creates big shoes for landscapemetrics to fill – and landscapemetrics fills them admirably. This package was the engine behind the landscape structure section of my ground filtering paper this year, and is so user-friendly. I’m very grateful that this tool exists at all now that FRAGSTATS is no longer with us; that the package is good is more than we deserve."
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html#devops",
    "href": "posts/2022-12-12-tools/index.html#devops",
    "title": "The tools I loved this year",
    "section": "DevOps",
    "text": "DevOps\n\nPi-hole\nIf you’re nerdy enough to have a sever kicking around, you owe it to yourself to set up a Pi-hole. A comical amount of the total payload size for any website is made up of advertisements and tracking code that is annoying at best and actively malicious at worst; letting that unwanted data into your network and onto your machine is a real security risk. You should use an ad blocker in your browser for this reason (I personally use Ad Nauseum), but adding an extra layer of security to your network is also a great idea.\nPi-holes are relatively easy to set up (in comparison to, let’s say, Python), will help your websites load faster, and will help prevent malicious code from affecting your machines. I can’t recommend them enough.\n\n\nplumber by Barret Schloerke, pool by Joe Cheng, and memoise by Hadley Wickham and Jim Hester (among others)\nI’m cheating, because technically these next three packages belong in the next section, but also a huge amount of my work this year has been helped by having these packages around. The plumber package makes it comically easy to set up a REST API using R, and we lean on it to run the data delivery infrastructure for our research group. I don’t know how they did it, but the team behind plumber managed to make it so writing API code feels like writing analysis or processing code, even though it’s a completely different problem space. It’s fantastic.\nThis year in particular we’ve been focused on speeding up our API. For that purpose, we’ve turned to pool and memoise: the pool package caches database connections so that you don’t need to spend the time re-establishing connections for each new request that comes through; the memoise package caches function calls, so that you don’t need to spend the time re-calculating the results of a function that you recently executed. By making our data retrieval API lean heavily on these two packages, we were able to cut our average download times in half – not the server time, not the individual function calls we altered, but time from user making a request to having all of the data they required. And the changes required were painless – we were already using DBI, so using pool took roughly no work; using memoise took maybe an hour of my time. These packages make working with APIs a treat, even in a “statistical computing and graphics” language.\n\n\nopenmetrics by Aaron Jacobs and logger by Gergely Daróczi\nSpeaking of APIs, these two packages have been absolute workhorses for us over the past year. The openmetrics package writes metrics about your API to a format that’s understandable by Prometheus, which in turn is easily queried via Grafana; combined, these tools have formed our main observability stack for our data retrieval API ever since we stood it up. The logger package, meanwhile, does exactly what you’d expect: it emits logs, of various levels, to wherever you direct it. Having good tooling for monitoring and logging has been incredibly helpful as we need to debug and otherwise alter our production API, and I’ve been extremely happy with our setup."
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html#the-posit-empire",
    "href": "posts/2022-12-12-tools/index.html#the-posit-empire",
    "title": "The tools I loved this year",
    "section": "The Posit Empire",
    "text": "The Posit Empire\nLast, but certainly not least, there’s a huge suite of tools I use nearly every day from Posit, PBC (née RStudio). Because this post is rather long, and I wanted to focus on smaller projects, I’m going to write a bit less about each; that is not because I like them any less.\n\n\nQuarto\nI’ve been a huge Quarto booster since it was released; heck, I went to (I think) the first public Quarto talk at the Boston RUG meetup. This site is written in Quarto; my last three papers have been written in Quarto; the talk I’m giving Wednesday is written in Quarto. I’ve written three Quarto formats. A huge amount of my job is “writing” – writing code, papers, talks, and so on – and being able to treat each form of writing in the same way, without needing to switch my tooling up is a huge help when I need to switch across tasks.\n\n\nrecipes\nI think I need to write a longer post about recipes at some point, because this package is great. In isolation, recipes makes it extremely easy to do feature engineering in a clean, ergonomic way; I teach with it even when I’m not teaching with tidymodels. But when combined with the rest of the tidymodels ecosystem, recipes really shines, helping you avoid data leakage and keeping your data preprocessing pipelines consistent across iterations.\n\n\nyardstick\nWant to calculate RMSE of two numeric columns in a data frame?\n\nyardstick::rmse(Orange, age, circumference)\n\n\n\n  \n\n\n\nDecide you want to calculate MAE instead?\n\nyardstick::mae(Orange, age, circumference)\n\n\n\n  \n\n\n\nWant to calculate a ton of metrics all at once?\n\nyardstick::metric_set(\n  yardstick::rmse, \n  yardstick::mae,\n  yardstick::rsq,\n  yardstick::mape,\n  yardstick::msd\n)(Orange, age, circumference)\n\n\n\n  \n\n\n\nyardstick is consistent in inputs and outputs, extremely clean, and a joy to work with. I’ve been extending it recently in waywiser and enjoying the process a lot. Plus, just like with recipes, it integrates beautifully with the rest of tidymodels. Fantastic.\n\n\nggplot2\nI mean, come on.\n\n\ndplyr\nI mean, come on.\n\n\ntidyr\nI fell in love with tidyr back in 2017,20 because it let me do things like this:\n\nOrange |&gt; \n  tidyr::nest(data = -Tree) |&gt; \n  dplyr::mutate(\n    model = purrr::map(data, \\(x) lm(circumference ~ age, x)),\n    rsq = purrr::map_dbl(model, \\(x) summary(x)$r.squared)\n  )\n\n\n\n  \n\n\n\nI don’t know if dplyr has changed since then or if I’ve just gotten more familiar with it, but I’ve since realized I can use dplyr for this sort of thing instead:\n\nOrange |&gt; \n  dplyr::group_by(Tree) |&gt; \n  dplyr::summarise(\n    model = list(lm(circumference ~ age, dplyr::cur_data())),\n    rsq = purrr::map_dbl(model, \\(x) summary(x)$r.squared)\n  )\n\n\n\n  \n\n\n\nBut for my more complicated split-apply-combine workflows, tidyr still reigns supreme.\n\n\ndevtools, testthat, pkgdown\nLast but not least, the tools I probably use more than any other package. It is incredible how effectively devtools and testthat speed up package development; it’s hard to imagine any other tools being a greater force multiplier than these. I can count the number of times I have run R CMD check on one hand, and the number of times I’ve run devtools::check() on the atoms of the universe.\nSimilarly, pkgdown is an incredible tool for the entire community. The fact that it is now easy – I mean, trivial – to have a nice-looking HTML documentation website for any package, so long as it has a README and man pages, is incredible. It makes the software we write more accessible to users, and incentivizes writing good documentation by turning man pages and vignettes into marketing material and a public-facing homepage."
  },
  {
    "objectID": "posts/2022-12-12-tools/index.html#footnotes",
    "href": "posts/2022-12-12-tools/index.html#footnotes",
    "title": "The tools I loved this year",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd that’s a big “might”. For the R packages I maintain, I’m able to track the number of downloads I get each day from the RStudio CRAN mirror. As I write this, this site is one of 103 mirrors, and while I suspect it’s the most popular one, I’m pretty confident that no one but CRAN actually knows – and to make matters worse, the people using the other 102 mirrors directly rather than the RStudio redirect are most certainly not a random sample. That means I’m only capturing some % of total downloads, but just about no one knows what % that might be, and it’s probably a different number for different packages. I can also track the number of clones each of my repos gets, but this is even less useful; spatialsample’s repo was cloned 28 times yesterday, by a total of 4 people. Who are you people? Why are you cloning this repo so much?↩︎\nTo again use the RStudio download numbers as an example, a fun game is to check how many downloads even the least popular packages get each day. I have never, using Hadley’s app, found a package with an average lower than ~5 downloads per day (see for instance my heddlr package as a benchmark). Of course, if you have a stable userbase your users presumably won’t need to download your packages unless they get a new computer, a new version of R, or you release an update – which often causes spikes in the download numbers – so “number of downloads” is a pretty bad metric, even before we get into all the downloads from non-users or all the ways this number can be gamed.↩︎\nAnd frankly, I don’t think most maintainers would actually like this; issues on a bug tracker are meant to be individual tasks to be tackled, and so positive feedback sent through this channel is probably the right message in the wrong place.↩︎\nWhich, don’t get me wrong, is still very appreciated. Even at my least meditative, I’d rather stop making the mistake :)↩︎\nA NUC running Ubuntu, which is really the perfect machine for “an inexpensive always-on computer that won’t deafen me with fans or beep really loudly when I’m asleep”.↩︎\nI use tailscale, which is not open source but would otherwise make this list.↩︎\n“Element”; in this case, we’re measuring how much phosphorous is in each gram of a wood sample.↩︎\nThere is no such thing.↩︎\nOne example here: units objects are not numeric vectors, in the normal sense. You can’t do, for instance, units::set_units(2, \"m\") + 1; without knowing what units 1 is in, it’s impossible to say what the return value would be. A whole class of bugs, eliminated by not letting you do something.↩︎\n(most)↩︎\nRead: “Whenever I see people posting about a new package on Twitter”↩︎\nMostly in LaTeX. Another confession: I don’t entirely understand the point of HTML tables. This is probably because I do not write statistical analyses for the internet. For the types of content I consume via HTML, I want you to either give me a graph of the important message of your data, or to give me a CSV so I can go figure out the message for myself. The sorts of things I consume via PDF, or write for PDF formats, usually have a higher evidence bar.↩︎\nMinus Mahoney and Stella 2020, which was written in Word; the horrors of formatting tables for that paper in Word is actually what got me into R Markdown in the first place.↩︎\nI didn’t use rgeos or rgdal directly, other than as part of functions in the raster package; I’m also new enough to the space that I’ve always used sf, and never really touched sp or maptools.↩︎\nThe first version of raster hit CRAN in 2010. That we should all be so long-lasting and so useful!↩︎\nPretty often, though not always, me.↩︎\nPretty often, though not always, Lucas.↩︎\nThe original draft, written very late at night, said “a little bit like recommending not being on fire” here. I changed that, because it did not make any sense, but I also found it really funny, hence this note.↩︎\nAs in, “I had more than one college course that was, in no small part, on FRAGSTATS”.↩︎\nWhich means yes, I do sometimes still use gather(), though weirdly never spread().↩︎"
  },
  {
    "objectID": "posts/2023-02-20-tools/index.html",
    "href": "posts/2023-02-20-tools/index.html",
    "title": "Commands I Use",
    "section": "",
    "text": "Greg Wilson posted this and then Eugene Wallingford posted this and it seemed like fun: what are the most common shell commands I use?\nCounting command usage is pretty straightforward:\n$ history | awk '{print $2}' | sort | uniq -c | sort -nr | head -n 50\nAnd my list winds up looking like this:\ngit # Though of course, this is a bunch of different commands\ncd # Did you know you don't need to use cd in zsh? \n# You can just type the directory. I learned this last week, evidently.\nsudo\nls\nfind\nquarto # I don't think this includes rendering from RStudio! \ngrep\nrm\ngdalwarp # -multi -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE \\ \n# -co PREDICTOR=3 -co BIGTIFF=YES\nra # My alias for radian -- https://github.com/randy3k/radian\nnano # Possibly from teaching Software Carpentry lessons\ncat\ndocker # I always teach using https://hub.docker.com/r/rocker/rstudio\nrsync\ncargo\nhtop\nxkill # No, *you're* cheating at Crusader Kings\npip\nssh\nR # Interesting to me that I have three separate R calls here\nRscript\ncurl\nmkdir\nlabrador # Alias to SSH into another computer\nconvert # Have done a _lot_ of image processing work for the PhD\ntmux # Highly, highly recommend https://github.com/tmux/tmux\nqvm # The _only_ way to use Quarto https://github.com/dpastoor/qvm\nping # I break my local network more than I care to admit\ngdal_calc.py \nkillall # \"Whoops, accidentally used all threads for that job again\"\necho\nco # Alias to SSH into another computer\ncp\nogr2ogr # Called a lot, but called _correctly_ very rarely\nmv\npython # More often called from inside an IDE\nogrinfo\nhistory\nwhich\ntouch\nwc\nsource\ngdalinfo\ndu # Doing spatial modeling on local compute resources is... rough\nsshpi # Alias to SSH into another computer\nsed\ngdal_translate\nffmpeg # Surprisingly useful for conference talks\nwget\nscp\nI’m surprised by how much more I use gdalwarp than the rest of the suite! But otherwise, this all makes pretty intuitive sense to me."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "",
    "text": "This is a quick walkthrough of how to use GitHub actions to automatically upload packages to personal {drat} repos.\nIf you just want the good stuff, here’s the link to the GitHub action I’m using as well as the GitHub workflow that uploads terrainr to my own drat repo."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#what-are-we-doing",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#what-are-we-doing",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "What are we doing?",
    "text": "What are we doing?\n{drat} is an R package that helps you set up CRAN-like repositories, hosted (primarily) on GitHub Pages. GitHub Actions is a continuous integration/continuous deployment service1, also hosted by GitHub, which lets you trigger compute workloads based on a CRON schedule, activity on a hosted repository, or manually.\nThis post walks through setting up a GitHub Actions CD workflow2 to automatically build and upload an R package to a drat repo based on a schedule or repository activity."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#why-are-we-doing-it",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#why-are-we-doing-it",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "Why are we doing it?",
    "text": "Why are we doing it?\nMy lab has a number of internal packages we use for our day-to-day work which handle things like downloading data from our central server and producing standardized model assessment outputs. All of these packages live on GitHub in our central organization, and for a long time our workflow for installing these packages has been to clone the repository and then devtools::install the package.\nRecently, though, we’ve had to share one of our packages with a number of outside collaborators who we aren’t going to be adding to the organization. We’ve also started working with a number of people who are less familiar with git and GitHub, both externally and within the lab, which means we need to provide a bit more instruction than simply saying “clone the repo and install that”3. This also means that solutions like “use remotes::install_github with a PAT” aren’t feasible; we want people to be able to get the packages they need without needing to know anything about GitHub itself. Therefore, we needed a simpler method to actually distribute our code, both to people we’d happily give repo access and people we wouldn’t want to.\nFor public repositories and organizations, I think the easiest solution to this problem is to set up your own R Universe, which rOpenSci has written fantastic instructions for getting started with; this offloads your responsibility for managing CI to the rOpenSci team. For this project, however, we wanted a separate and mostly hidden repository page, which made {drat} a perfect candidate.\nThe only problem was how to make sure we were always publishing the newest versions of our package. We’re a small team doing a lot of different things, so being able to automate away any repetitive task (like publishing patch releases to a {drat} repo) can really help reduce the cognitive load and number of mistakes associated with updating our shared codebase."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#how-did-we-do-it",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#how-did-we-do-it",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "How did we do it?",
    "text": "How did we do it?\nBecause the original version of this workflow is on a private repo, I’ll walk through the workflow that uploads terrainr to my own personal drat repo. The YAML file that runs the workflow looks like this:\non:\n  push:\n    branches:\n      - 'main'\n    paths:\n      - 'DESCRIPTION'\n  workflow_dispatch:\n\njobs:\n  drat-upload:\n    runs-on: ubuntu-20.04\n    name: Drat Upload\n    steps:\n      - uses: mikemahoney218/upload-to-drat-repo@v0.1\n        with:\n          drat_repo: 'mikemahoney218/drat'\n          token: \"${{ secrets.DRAT_TOKEN }}\"\n          commit_message: \"Automated update (add terrainr)\"\n          commit_email: \"mike.mahoney.218@gmail.com\"\nThis script has two main components to it: running the job and configuring the action itself.\n\nRunning the job\nAt the top of terrainr’s drat workflow script is the following chunk:\non:\n  push:\n    branches:\n      - 'main'\n    paths:\n      - 'DESCRIPTION'\n  workflow_dispatch:\nThis sets up two different ways to trigger the workflow. First off, any push commit to the main branch that touches the DESCRIPTION file will automatically build the package and push it to my drat repo. Because terrainr is published on CRAN and has a handful of users, I do my best to update version numbers whenever I make changes – even during development, I try and bump development versions whenever there’s a fix or update worth mentioning.\nSince the version number is stored in DESCRIPTION, this means I’ll only push updated package versions when I’ve actually updated something in the package; my small edits to the CI files, for instance, won’t create a new release.\nIf you want to be freer about updating your repo, you can drop any of these restrictions. For instance, our internal packages have workflow triggers that look like this:\non:\n  push:\n    branches: 'main'\nIn this case, we push an update any time we push any commit to the main branch. This can cause problems – for instance, if we don’t update version numbers then update.packages won’t recognize that local installations are out of date – but makes more sense for our small team and small group of users.\nThe second method this workflow uses to upload a new version is the workflow_dispatch: option, which lets me manually trigger the workflow from the GitHub UI. This is super helpful in case I accidentally mess up my drat repo, or make a small edit without changing the version number in the DESCRIPTION; without this line I’d have to rebuild the package on my laptop and update the drat repo from my local copy.\nThere’s a lot of other ways you can set to trigger workflows – see the full list here – but the other one I want to highlight is that you can also set the workflow to trigger on a schedule using a crontab. This snippet for instance will release daily builds every day at 0:00 UTC:\non:\n  schedule: \n    - cron: '0 0 * * *'\nYou can use this syntax to create incredibly complex schedules; I personally always use https://crontab.guru/ to make sure my crontabs are right when I’m trying to schedule jobs.\n\n\nConfiguring the action\nThe second half of the workflow script actually calls the action to build the package and upload it to your drat repo:\njobs:\n  drat-upload:\n    runs-on: ubuntu-20.04\n    name: Drat Upload\n    steps:\n      - uses: mikemahoney218/upload-to-drat-repo@v0.1\n        with:\n          drat_repo: 'mikemahoney218/drat'\n          token: \"${{ secrets.DRAT_TOKEN }}\"\n          commit_message: \"Automated update (add terrainr)\"\n          commit_email: \"mike.mahoney.218@gmail.com\"\nThis will, by default, spin up an Ubuntu 20.04 runner and run v0.1 of the upload action, which is the current version. This action will automatically check out the repo the workflow is called from, build and install the R package in that repo, then insert it into a drat repo.\nThere’s a few input values for this workflow which you must specify, which are under the “with” step:\n\ndrat_repo: The GitHub repository for the drat repo you’re trying to push your package to. If you don’t have a drat repo yet, you can create one locally using drat::initRepo, and then push the created directory up to GitHub.\ncommit_email: The author to write the drat repo commit as; used to set git config user.email. You must provide a value here.\ntoken: A personal access token (PAT). The action will use this PAT to clone your package and drat repo, as well as to push your drat repo, so make sure to authorize the PAT to do so. It’s a good idea to use a service account with the fewest permissions possible for this job.\n\nYou can also customize the job further with a few additional inputs:\n\ncommit_message: The message to use when committing to drat_repo.\ncommit_author: The author to write the commit as; used to set git config user.name.\npackage: The GitHub repository for the package you want to upload. Defaults to the repository the action is running in, but this can be used to run your workflow elsewhere (for instance, if you want to have all your upload jobs scheduled in the drat repo itself).\n\nAnd for most use cases, this should be enough to automatically deploy your package whenever you desire! So far, I’ve tested this workflow on a handful of my own packages (terrainr and then our internal packages), and things are working well so far – if you find any problems or have any suggestions, feel free to open an issue on the action repo here."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#footnotes",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#footnotes",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKinda. It’s probably more accurate to say GitHub Actions provides hosted compute which is typically used for CI/CD workflows, but can also be applied to other things; I use it to make art and post to Twitter, for instance.↩︎\nWorkflows are explicit, repository-specific jobs, while actions are generic scripts which can be included inside of workflows.↩︎\nI had completely forgotten how complex getting started with GitHub can be as a novice until recently, teaching a Carpentries workshop, I realized that teaching the initial account set-up requires a full 45 minute lesson.↩︎"
  },
  {
    "objectID": "posts/2022-08-11-waywiser-010-is-now-on-cran/index.html",
    "href": "posts/2022-08-11-waywiser-010-is-now-on-cran/index.html",
    "title": "Waywiser version 0.1.0 is now on CRAN!",
    "section": "",
    "text": "The waywiser hex sticker logo: a surveyor’s wheel over the word “waywiser”.\n\n\nI’ve got a new package on CRAN! waywiser is a yardstick extension package, providing functions for calculating spatial autocorrelation in model residuals in a way that cooperates with most (but not all, see below) of the tidymodels framework.\nYou can install it from CRAN:\ninstall.packages(\"waywiser\")\nOr the development version from the package’s GitHub page:\n# install.packages(\"remotes\")\nremotes::install_github(\"mikemahoney218/waywiser\")\nYou can use the package to estimate the spatial autocorrelation in residuals from any model – just provide spatial data, a vector of your “true” measurements, and a vector of your predicted values:\n\nlibrary(waywiser)\n\n# Data on morality crimes in France:\ndata(guerry, package = \"sfdep\")\n\nww_global_moran(guerry, crime_pers, predict(lm(crime_pers ~ literacy, guerry)))\n\nWarning: multiple methods tables found for 'area'\n\n\n\n\n  \n\n\n\nUnder the hood, waywiser uses two functions (ww_build_neighbors() and ww_build_weights()) to build sensible, if likely non-ideal neighbor lists and spatial weights for your data. However, waywiser also lets you provide your own weights object to override the automatic calculations, or provide a function to calculate spatial weights based on the input data frame:\n\nweights &lt;- ww_build_weights(guerry)\n\nww_global_moran(\n  guerry, \n  crime_pers, \n  predict(lm(crime_pers ~ literacy, guerry)),\n  wt = weights\n)\n\n\n\n  \n\n\n\nProviding our own weights is necessary in order to use the _vec() versions of waywiser functions, which can be helpful for use in dplyr functions:\n\n# For the %&gt;% pipe and mutate:\nlibrary(dplyr)\n# For visualization:\nlibrary(ggplot2)\n\nguerry %&gt;%\n  mutate(pred = predict(lm(crime_pers ~ literacy, .)),\n         .estimate = ww_local_moran_i_vec(crime_pers, pred, weights)) %&gt;% \n  sf::st_as_sf() %&gt;% \n  ggplot(aes(fill = .estimate)) +\n  geom_sf() + \n  scale_fill_gradient2(\n    \"Moran's I\", \n    low = \"#018571\", \n    mid = \"white\", \n    high = \"#A6611A\"\n  ) + \n  theme_minimal()\n\n\n\n\nThe package currently provides three main indices of autocorrelation – namely, Moran’s I and Geary’s C (both in global and local variants), as well as Getis-Ord’s G and G* (only the local variant).\nThis first version of the package integrates well with the rest of the tidymodels framework, except for the tune package (due to some difficulty in exposing either the original spatial data or the weights object to waywiser function while tuning a model). As a result, this version doesn’t let you include these functions as metrics to calculate inside of a call to fit_resamples().\nThe full list of features and documentation can be found on the package’s website. This has been a really fun package to work on; I’m excited to see it out in public, and will look forward to seeing if anyone else finds it useful!"
  },
  {
    "objectID": "posts/2022-12-16-femc/index.html",
    "href": "posts/2022-12-16-femc/index.html",
    "title": "Legibility and seats at the table",
    "section": "",
    "text": "There’s a bit of forestry lore that’s become really well-known in political science, popularized by James C Scott’s “Seeing Like a State”, but is as best as I can tell mostly unknown in English-speaking forest science.1 It goes a little something like this:\nPrussia in the 1800s was an industrializing nation in extreme demand for more of every resource they could get their hands on. As part of the industrialization process, the Prussians found themselves needing to approach their natural resources in a more standardized and predictable way; if a factory was expected to make a standard number of chairs each day, it required a standard amount of wood to work with. The normal rhythm of growing seasons and the boom-and-bust cycles of natural systems were antithetical to the increasingly regimented and systematized industry the government was aiming to encourage.\nAnd as such, the government sought to systematize their forests. The 1800s were the beginnings of forest science as a discipline, of allometrics and quantification and active management planning on scales which had not been attempted – or feasible – to earlier societies. The objective of the state was not just increasing productivity – although that was an objective – but to standardize it, to move towards a wood products industry that could support the demands of manufacturing irrespective of forest conditions. Measurements were standardized, growth tables developed, tools and techniques invented, all in the name of progress. And in the name of standardization and control, the state encouraged active management to enforce even-aged monocultural stands which could be easily accounted for, whose expected outputs and timing could be predicted and planned upon in order to ensure the engine of industry stayed alive.2\nBut of course, standardizing a landscape causes quite a disturbance, and a forest is made up of more than trees. Removing undesirable vegetation removes the habitat that game and other animals require, upsetting historical hunting patterns as well as pollination and seed distribution; clearing the understory and removing slash removes soil nutrients and prevents non-assisted regeneration. The start of systematization saw wood product production peak at never-before-seen heights; a century later, production was down 30% from the pre-standardized baseline. In some situations, management had made it so no trees would regenerate at all. The Prussians introduced one more innovation, a word to describe such places: Waldsterben. Forest death.\nScott tells this story as part of a bigger pattern, of states attempting to standardize complex systems in order to make them “legible” and manageable by remote administrators, at the cost of localized expertise and nuance. No matter how good your measurements and allometrics, there’s really no better way of knowing a forest than a walk in the woods."
  },
  {
    "objectID": "posts/2022-12-16-femc/index.html#waldsterben",
    "href": "posts/2022-12-16-femc/index.html#waldsterben",
    "title": "Legibility and seats at the table",
    "section": "",
    "text": "There’s a bit of forestry lore that’s become really well-known in political science, popularized by James C Scott’s “Seeing Like a State”, but is as best as I can tell mostly unknown in English-speaking forest science.1 It goes a little something like this:\nPrussia in the 1800s was an industrializing nation in extreme demand for more of every resource they could get their hands on. As part of the industrialization process, the Prussians found themselves needing to approach their natural resources in a more standardized and predictable way; if a factory was expected to make a standard number of chairs each day, it required a standard amount of wood to work with. The normal rhythm of growing seasons and the boom-and-bust cycles of natural systems were antithetical to the increasingly regimented and systematized industry the government was aiming to encourage.\nAnd as such, the government sought to systematize their forests. The 1800s were the beginnings of forest science as a discipline, of allometrics and quantification and active management planning on scales which had not been attempted – or feasible – to earlier societies. The objective of the state was not just increasing productivity – although that was an objective – but to standardize it, to move towards a wood products industry that could support the demands of manufacturing irrespective of forest conditions. Measurements were standardized, growth tables developed, tools and techniques invented, all in the name of progress. And in the name of standardization and control, the state encouraged active management to enforce even-aged monocultural stands which could be easily accounted for, whose expected outputs and timing could be predicted and planned upon in order to ensure the engine of industry stayed alive.2\nBut of course, standardizing a landscape causes quite a disturbance, and a forest is made up of more than trees. Removing undesirable vegetation removes the habitat that game and other animals require, upsetting historical hunting patterns as well as pollination and seed distribution; clearing the understory and removing slash removes soil nutrients and prevents non-assisted regeneration. The start of systematization saw wood product production peak at never-before-seen heights; a century later, production was down 30% from the pre-standardized baseline. In some situations, management had made it so no trees would regenerate at all. The Prussians introduced one more innovation, a word to describe such places: Waldsterben. Forest death.\nScott tells this story as part of a bigger pattern, of states attempting to standardize complex systems in order to make them “legible” and manageable by remote administrators, at the cost of localized expertise and nuance. No matter how good your measurements and allometrics, there’s really no better way of knowing a forest than a walk in the woods."
  },
  {
    "objectID": "posts/2022-12-16-femc/index.html#eye-in-the-sky",
    "href": "posts/2022-12-16-femc/index.html#eye-in-the-sky",
    "title": "Legibility and seats at the table",
    "section": "Eye in the sky",
    "text": "Eye in the sky\nI was lucky enough to be in Burlington, Vermont for this year’s Forest Ecosystem Monitoring Cooperative (FEMC) conference. It’s among my favorite conferences, not least because Burlington is a beautiful town, but also because the audience is largely made up of land managers from governments and industry. These are people who are in the business of solving problems, and are interested in your work to the extent that it helps them solve problems. It’s a nice curative to some of the tail-chasing tendencies of academia.\nMadeleine Desrochers gave an absolutely fantastic talk about her work, using models trained on satellite imagery to detect harvests in the Adirondack forest. In particular, Madeleine was focusing on how we can assess the accuracy of these models, especially working as we do in an area where most harvests aren’t clearcuts and as such can be a bit hard to see from space. The work is fascinating, and the presentation was great, but my particularly favorite bit was when Madeleine spoke about the motivations behind her work.\nTo paraphrase: this stuff is terrifying. We are not, as a society, used to every action we make on the surface of this Earth being watched by a silent observer in space, or picked up by a model run on a laptop in Syracuse. We are, by and large, neither expecting nor accepting that our actions will be so legible to anyone who cares to look.\nBut, one way or another, this stuff is the future. There are VC-backed startups promising change detection algorithms to any government agency who will give them the time of day, and these firms do not operate with particularly high standards for accuracy or validation. And it’s a bad outcome for everyone if spotty models are used to help the state make decisions, to identify people violating tax codes or taking more trees than they’re entitled to.\nThe best case scenario is that we publicly investigate these algorithms to figure out how, when, where, and why they work. And even more importantly, we need domain experts at the table when these algorithms are being considered, to help inform decisions about what it will mean for these things to become legible, who will be helped and who will be harmed, and how foresters and forests will react in years to come. The engineers are going to be implementing these tools no matter what; our best way through it is to make sure that domain experts, that local knowledge are well-represented as they do so, to make outcomes better for us all."
  },
  {
    "objectID": "posts/2022-12-16-femc/index.html#the-data-sciencification-of-everything",
    "href": "posts/2022-12-16-femc/index.html#the-data-sciencification-of-everything",
    "title": "Legibility and seats at the table",
    "section": "The data sciencification of everything",
    "text": "The data sciencification of everything\nAlso at FEMC, Jarlath O’Neil-Dunne spoke on a panel about his fear that we’re watching the “data scienceification of everything.”3 As noted by others on the panel, forest science has moved from a period of bespoke tooling – clinometers, densiometers, biltmore sticks, D-tapes and the rest of their 19th century companions – towards a place where we’re adapting the tooling of other disciplines for forest problems. This is perhaps not a new pattern, given the amount of farming equipment running on any forest operation, but the rapid shift in the field to a focus on complex models, remote sensing, and “big data” has created something of a gap between leading research and the problems that folks in the field actually need solved.4\nPart of the problem is that, as we add more and more programming and data management skills to ecology and forestry curricula, we’re possibly not paying enough attention to what gets cut. Now, I think this problem is typically overstated – there was more than a bit of overlap in my undergrad ecology, natural resources ecology, wetlands ecology and management, forest ecology, forest ecology and management, and natural resources silviculture courses – but it’s not wrong. Learning to code takes time, and that’s time not spent on a walk in the woods, building the sort of domain knowledge that’s essential to understanding how these complex systems actually work.\nBut we can’t just not teach students how to code. Not only is that educational malpractice – as Richard Hamming put it, teachers should prepare the student for the student’s future, not for the teacher’s past – but also, these tools and techniques aren’t going away, and the world isn’t slowing down. We can’t simply cede this ground to engineers and data scientists; being able to apply knowledge of the underlying system to the problem at hand remains essential for producing not only accurate models but also actionable insights into the world at large. If we want forest science to benefit from recent gains in processing power, in the ability for humans to think about, understand, and manage large-scale complex systems, then we need forest science to have a seat at the table when deciding how these tools are used for our field. We need folks in forest science to have a say in how our world is made legible, before those decisions are made for us."
  },
  {
    "objectID": "posts/2022-12-16-femc/index.html#footnotes",
    "href": "posts/2022-12-16-femc/index.html#footnotes",
    "title": "Legibility and seats at the table",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI personally heard this the first time via Bret Devereaux’s ACOUP, a blog which has both made me smarter and also turned me on to Paradox games, at the cost of hundreds of hours of my life.↩︎\nIn a very real way, a lot of our forest management is still in this paradigm, though plenty of folks are making valiant efforts to push for more modern management methodologies. Scott makes the point that Pinchot was trained at a school following “a German-style curriculum”; the foundations of forestry as a science are fundamentally German.↩︎\nI think I’m mischaracterizing his comments here, for what it’s worth; Jarlath was talking about the importance of other ways of knowing, beyond simple raw processing of quantitative information. The themes here were part of the panel discussion overall, though.↩︎\nMy personal bugbear here: most global models of ecological attributes – say, biomass – are pointless. Global models trade accuracy for scope, which makes them practically useless for informing decisonmaking at a regional level. Given that most organizations which produce global models aren’t making decisions at a global level, and don’t typically have the ear of those who do, their stakeholders would be better served by a smaller geographic focus and in turn a higher accuracy. That said, if you’re in the business of selling models, it is much more appealing to your P&L if you only need to make one model which you’ll sell to anyone who will buy it; the incentives here put the modeler in direct conflict with stakeholder interests. There are also so, so many other problems with global models, but there’s been a trend recently of papers claiming such issues are surpassable; I think the incentives problem is somewhat more fundamental.↩︎"
  },
  {
    "objectID": "posts/2022-08-04-how-to-use-quarto-for-parameterized-reporting/index.html",
    "href": "posts/2022-08-04-how-to-use-quarto-for-parameterized-reporting/index.html",
    "title": "How to use Quarto for Parameterized Reporting",
    "section": "",
    "text": "There’s a lot of conversations around about Quarto as a new system for technical communication. And that’s well deserved! Quarto, the successor to the R Markdown system, extends and improves upon its predecessor and produces downright beautiful documents (including this very website).\nBut as a recovering data analyst, the thing that I’m most excited about with Quarto is its ability to generate reports based on a document template and a handful of parameter values. A whole lot of my professional career has amounted to “report out the same handful of KPIs every day (or month, or quarter), aggregated to each relevant business unit and a handful of combined groups.” R Markdown’s parameterized reporting let me automate that boring stuff and spend my time on more fun (and higher impact) projects, and Quarto supports more or less the same style of templating. As such, this blog post walks through how you can use parameterization to automate your own reporting with Quarto.\n\nA Simple Script\nLet’s say that you have a colleague who’s really into elevators (like, inexplicably interested in elevators). Because of this interest, and because you’re already working on a blog post showing off how cool Quarto is, you decide to use data about elevators in a small example script you’re working on to make them happy.\nWell, luckily enough, my colleague Emil Hvitfeldt has a package on GitHub with data about elevators. Let’s install it now:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"EmilHvitfeldt/elevators\")\n\nThis package includes a tibble, elevators, which contains quite a few observations of quite a few variables describing the elevators throughout New York City:\n\nelevators::elevators\n\n\n\n  \n\n\n\nBecause all you’re doing is writing a small example script, you just grab a few variables at random and plot them in a pretty simple Quarto document. That .qmd looks like this:\n---\ntitle: \"Cool graphs about elevators\"\nauthor: Mike Mahoney\nsubtitle: \"Last generated on:\"\ndate: today\nformat:\n  html:\n    echo: false\n---\n\n```{r}\n#| message: false\nlibrary(elevators)\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n```\n\n## Speed over time\n\n```{r}\n#| message: false\n#| warning: false\nelevators |&gt;\n  ggplot(aes(approval_date, speed_fpm)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth() +\n  scale_y_log10()\n```\n\n## Speed versus capacity\n\n```{r}\n#| message: false\n#| warning: false\nelevators |&gt;\n  ggplot(aes(capacity_lbs, speed_fpm)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth() +\n  scale_y_log10()\n```\n\n## Where in the world did all my elevators go\n\n```{r}\nelevators |&gt;\n  ggplot(aes(longitude, latitude)) + \n  geom_point(alpha = 0.05) +\n  coord_sf()\n```\n(You can see what this looks like when rendered over at this link.)\nWith your small example all set, you rush off to show it to friends and family before officially publishing it to the web.\n\n\nParameterized Reports\nYour colleague likes your report! However, as it goes when you show people a graph, they have questions. They want to know: what would these graphs look like for only Manhattan?\nThis isn’t your first time giving a colleague some data, and so you know better than to just scrape the data and email them a number. Instead, you edit your Quarto file so that it could filter to any borough, and make it easy to change the filter based on Quarto parameters:\n---\ntitle: \"Cool graphs about elevators\"\nauthor: Mike Mahoney\nsubtitle: \"Last generated on: 2022-08-04\"\ndate: today\nformat: \n  html: \n    echo: false\nparams: \n  borough: \"NA\"\n---\n\n```{r}\n#| message: false\nlibrary(elevators)\n\nif (!is.na(params$borough) && params$borough != \"NA\") {\n  elevators &lt;- elevators[elevators$borough == params$borough, ]\n}\nif (nrow(elevators) == 0) {\n  stop(\"No elevators were selected. Did you misspell `borough`?\")\n}\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n```\n\n## Speed over time\n\n```{r}\n#| message: false\n#| warning: false\nelevators |&gt; \n  ggplot(aes(approval_date, speed_fpm)) + \n  geom_point(alpha = 0.05) + \n  geom_smooth() + \n  scale_y_log10()\n```\n\n## Speed versus capacity\n\n```{r}\n#| message: false\n#| warning: false\nelevators |&gt; \n  ggplot(aes(capacity_lbs, speed_fpm)) + \n  geom_point(alpha = 0.05) + \n  geom_smooth() + \n  scale_y_log10()\n```\n\n## Where in the world did all my elevators go\n\n```{r}\nelevators |&gt; \n  ggplot(aes(longitude, latitude)) + \n  geom_point(alpha = 0.05) + \n  coord_sf()\n```\nNow, by default, this output looks the same as the last time. However, when we run quarto render now, we can either pass parameters on the command line, for instance using:\nquarto render elevators.qmd -P borough:Manhattan\nOr, we can write a YAML file with our parameters (say, params.yml):\nborough: Manhattan\nAnd call quarto render via:\nquarto render elevators.qmd --execute-params params.yml\n(The results of this approach are at this link.)\nThis will make it easier to re-run your report every time your colleague comes to you with “one last” request.\nOr, heck, if they have R installed you can make them run it themselves. You spend an hour bashing out a Shiny app and save it in the same folder as elevators.qmd:\nrequired_packages &lt;- c(\"shiny\", \"ggplot2\", \"quarto\", \"rmarkdown\", \"remotes\")\nfor (x in required_packages) {\n  if (!requireNamespace(x, quietly = TRUE)) install.packages(x)\n}\nif (!requireNamespace(\"elevators\", quietly = TRUE)) {\n  remotes::install_github(\"EmilHvitfeldt/elevators\")\n}\n\nlibrary(shiny)\nelevators &lt;- elevators::elevators\n\nui &lt;- fluidPage(\n\n    titlePanel(\"Self-Service Elevator Analytics\"),\n\n    selectizeInput(\n        \"borough\",\n        \"Borough:\",\n        c(\"NA\", unique(as.character(elevators$borough)))),\n\n    actionButton(\n        \"render\",\n        \"Render!\"\n    ),\n\n    textOutput(\"status\")\n)\n\nserver &lt;- function(input, output) {\n\n    output$status &lt;- renderText({\n        if (input$render) {\n            isolate(\n                quarto::quarto_render(\n                    \"elevators.qmd\",\n                    execute_params = list(borough = input$borough)\n                )\n            )\n            paste(\"Rendering complete at\", Sys.time())\n        }\n    })\n\n}\n\nshinyApp(ui = ui, server = server)\nNow your colleague just needs to open the file in RStudio, click the cute little “Run App” button, and then they can answer all elevator questions to their heart’s content. If you set up your Quarto document to generate PDFs, they can even email their reports to other elevator enthusiasts.\n\n\nMore Complicated Logic\nYour app has been a hit with your colleague! In fact, possibly too much of a hit. After a recent trip, they’ve collected a bunch more data reflecting elevators across Maryland, and are wondering if you’d be able to extend your report to support this new dataset as well.\nYou have now become appropriately resigned to the fact that you are now “the elevator guy”, and will have weird facts about elevator speeds stored in your brain until the end of time. So you start digging into the Maryland data:\n\nelevators::md_elevators\n\n\n\n  \n\n\n\nNow Maryland, being worse than New York, doesn’t give you all the same information that you used for your original report. That means that only one of our graphs will actually render for both states.\nLuckily, Quarto lets us selectively render parts of the report using standard chunk options. By making chunk evaluation conditional on the state being described, we can make our single report work for both states.\nWith some tweaking, we can edit our .qmd so it works for both states:\n---\ntitle: \"Cool graphs about elevators\"\nauthor: Mike Mahoney\nsubtitle: \"Last generated on: 2022-08-04\"\ndate: today\nformat: \n  html: \n    echo: false\nparams: \n  region: \"NA\"\n  state: \"NY\"\n---\n\n```{r}\n#| message: false\nlibrary(elevators)\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n```\n\n```{r}\n#| message: false\n#| warning: false\nif (params$state == \"MD\") {\n  elevators &lt;- md_elevators\n  \n  if (!is.na(params$region) && params$region != \"NA\") {\n    elevators &lt;- elevators[elevators$county == params$region, ]\n  }\n  \n  elevators &lt;- elevators[c(\"equipment_capacity\", \"equipment_speed\")]\n  names(elevators) &lt;- c(\"capacity_lbs\", \"speed_fpm\")\n  elevators$capacity_lbs &lt;- as.numeric(elevators$capacity_lbs)\n  elevators$speed_fpm &lt;- as.numeric(elevators$speed_fpm)\n  \n} else {\n  \n  if (!is.na(params$region) && params$region != \"NA\") {\n    elevators &lt;- elevators[elevators$borough == params$region, ]\n  }\n  \n}\n\nif (nrow(elevators) == 0) {\n  stop(\"No elevators were selected. Did you misspell `region`?\")\n}\n```\n\n```{r}\n#| eval: !expr params$state == \"NY\"\n#| results: asis\n\ncat(\"## Speed over time\")\n```\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: !expr params$state == \"NY\"\n#| results: asis\nelevators |&gt; \n  ggplot(aes(approval_date, speed_fpm)) + \n  geom_point(alpha = 0.05) + \n  geom_smooth() + \n  scale_y_log10()\n```\n\n## Speed versus capacity\n\n```{r}\n#| message: false\n#| warning: false\nelevators |&gt; \n  ggplot(aes(capacity_lbs, speed_fpm)) + \n  geom_point(alpha = 0.05) + \n  geom_smooth() + \n  scale_y_log10()\n```\n\n```{r}\n#| eval: !expr params$state == \"NY\"\n#| results: asis\n\ncat(\"## Where in the world did all my elevators go\")\n```\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: !expr params$state == \"NY\"\n\nelevators |&gt; \n  ggplot(aes(latitude, longitude)) + \n  geom_point(alpha = 0.05) + \n  coord_sf()\n```\nOur New York report looks the same as ever, while the Maryland report has a little less content. As it deserves.\n\n\nParameterized SQL\nBut why stop there? Everyone loves your report. You’re crowned elevator king. You’ve made your peace with knowing you’ll never do anything but elevators again.\nIn order to make your life a little bit easier, you’ve loaded your elevator data into a database, so you don’t need to keep track of the individual tables:\n\nlibrary(DBI)\nlibrary(RSQLite)\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \"elevators.db\")\nDBI::dbWriteTable(\n  con, \n  \"ny_elevators\", \n  elevators::elevators, \n  overwrite = TRUE\n)\nDBI::dbWriteTable(\n  con, \n  \"md_elevators\", \n  elevators::md_elevators, \n  overwrite = TRUE\n)\nDBI::dbDisconnect(con)\n\nThis will make it easier to incorporate additional data into your report, as new elevators are constructed and your colleague visits new states. It’s easy enough to change your report over so that it reads from the database:\n---\ntitle: \"Cool graphs about elevators\"\nauthor: Mike Mahoney\nsubtitle: \"Last generated on: 2022-08-04\"\ndate: today\nformat: \n  html: \n    echo: false\nparams: \n  region: \"NA\"\n  state: \"MD\"\n---\n\n```{r}\n#| message: false\nlibrary(elevators)\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n```\n\n```{r}\n#| message: false\n#| warning: false\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \"elevators.db\")\nquery &lt;- \"SELECT \n    ?capacity_column AS capacity_lbs, \n    ?speed_column    AS speed_fpm,\n    ?approval_date   AS approval_date,\n    ?latitude        AS latitude, \n    ?longitude       AS longitude\n  FROM ?table\"\n\nif (params$state == \"NY\") {\n  query_args &lt;- list(\n    capacity_column = DBI::dbQuoteIdentifier(con, \"capacity_lbs\"),\n    speed_column = DBI::dbQuoteIdentifier(con, \"speed_fpm\"),\n    approval_date = DBI::dbQuoteIdentifier(con, \"approval_date\"),\n    latitude = DBI::dbQuoteIdentifier(con, \"latitude\"),\n    longitude = DBI::dbQuoteIdentifier(con, \"longitude\"),\n    region_column = DBI::dbQuoteIdentifier(con, \"borough\"),\n    region = params$region\n  )\n} else {\n  query_args &lt;- list(\n    capacity_column = DBI::dbQuoteIdentifier(con, \"equipment_capacity\"),\n    speed_column = DBI::dbQuoteIdentifier(con, \"equipment_speed\"),\n    approval_date = 0,\n    latitude = 0,\n    longitude = 0,\n    region_column = DBI::dbQuoteIdentifier(con, \"county\"),\n    region = params$region\n  )\n}\n\nif (!is.na(params$region) && params$region != \"NA\") {\n  query &lt;- paste(query, \"\n  WHERE \n    ?region_column == ?region\")\n} else {\n  query_args &lt;- head(query_args, -2)\n}\n  \nquery &lt;- DBI::sqlInterpolate(\n  con, \n  query, \n  table = DBI::dbQuoteIdentifier(con, \"md_elevators\"),\n  .dots = query_args\n)\nelevators &lt;- DBI::dbGetQuery(con, query)\nelevators$capacity_lbs &lt;- as.numeric(elevators$capacity_lbs)\nelevators$speed_fpm &lt;- as.numeric(elevators$speed_fpm)\nDBI::dbDisconnect(con)\n```\n\n```{r}\n#| eval: !expr params$state == \"NY\"\n#| results: asis\n\ncat(\"## Speed over time\")\n```\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: !expr params$state == \"NY\"\n#| results: asis\nelevators |&gt; \n  ggplot(aes(approval_date, speed_fpm)) + \n  geom_point(alpha = 0.05) + \n  geom_smooth() + \n  scale_y_log10()\n```\n\n## Speed versus capacity\n\n```{r}\n#| message: false\n#| warning: false\nelevators |&gt; \n  ggplot(aes(capacity_lbs, speed_fpm)) + \n  geom_point(alpha = 0.05) + \n  geom_smooth() + \n  scale_y_log10()\n```\n\n```{r}\n#| eval: !expr params$state == \"NY\"\n#| results: asis\n\ncat(\"## Where in the world did all my elevators go\")\n```\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: !expr params$state == \"NY\"\n#| results: asis\n\nelevators |&gt; \n  ggplot(aes(latitude, longitude)) + \n  geom_point(alpha = 0.05) + \n  coord_sf()\n```\nNote that I’m using DBI::sqlInterpolate() and DBI::dbQuoteIdentifier(), to make sure we’re protecting our database from SQL injection attacks. And for this toy example, I’m hard-coding the database connection in. If your organization has a smart way to manage credentials, you’ll probably want to plug into that system (and should definitely not be storing passwords as plaintext in the file).\nAnd to step out of the bit for a minute, there’s no reason that your Quarto document actually needs to generate a meaningful report. Sure, automatically updating a website (or emailing a PDF) is nice, but Quarto documents are able to run SQL queries, write out files, and do any other task you can do with any of the languages supported by knitr or jupyter. This makes it really easy to write powerful scripts that your colleagues can use to access and understand whatever data you’re working with."
  },
  {
    "objectID": "posts/2020/01/index.html",
    "href": "posts/2020/01/index.html",
    "title": "Announcing {heddlr}, now on CRAN!",
    "section": "",
    "text": "My first package just got published to CRAN today! heddlr is a set of tools that make it easier to write modular R Markdown documents by decomposing them into a set of patterns which can be repeated and combined based on your input data, letting you dynamically add and remove sections based on your data. I started this package to solve an issue I found myself running into when building flexdashboards, and have since found out that there’s all sorts of cool tricks you can do by applying this type of functional programming mindset to R Markdown documents.\nYou can find out more on heddlr’s documentation website, proudly made in R via pkgdown. This first version on CRAN is 0.5.0, with 0.1 -&gt; 0.4 previously released on GitHub."
  },
  {
    "objectID": "posts/2020/03/index.html",
    "href": "posts/2020/03/index.html",
    "title": "Announcing {spacey}, now on CRAN!",
    "section": "",
    "text": "I’ve launched a new package to CRAN! spacey helps you pull elevation and image overlay data from the USGS and ESRI, then helps you turn them into beautiful maps via the fantastic rayshader package.\nThe package has a documentation website built with pkgdown – check it out for more information!"
  },
  {
    "objectID": "posts/2020/05/index.html",
    "href": "posts/2020/05/index.html",
    "title": "Installing the TIG stack on Raspberry Pi",
    "section": "",
    "text": "Do the following in a shell you’ve already auth’d into sudo on:\nsudo apt update\nsudo apt upgrade\n\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb\nsudo systemctl unmask influxdb\nsudo systemctl enable influxdb\nsudo systemctl start influxdb\n\n# you can find the current telegraf release here: https://portal.influxdata.com/downloads/\nwget https://dl.influxdata.com/telegraf/releases/telegraf-1.14.2_linux_armhf.tar.gz\ntar xf telegraf-1.14.2_linux_armhf.tar.gz\nsudo systemctl enable --now telegraf\nrm telegraf-1.14.2_linux_armhf.tar.gz\n\nsudo apt-get install -y adduser libfontconfig1\n# you can find the current grafana release here: https://grafana.com/grafana/download\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nThis should cause all three services to start on system boot. You’ll need to configure Telegraf to actually write to your local Influx instance at http://127.0.0.1:8086 (there’s a sample config under the Telegraf part of the post), then set up Grafana to read from Influx (at the same port) via the UI at localhost:3000.\n\n\n\n\n\n\nI’m getting a little cabin-fevery as the 2020 quarantine moves into its third month. To try and defray some of the extra energy, I’ve been hacking on a Pi I set up with a Pi-hole and openvpn server about a month ago.\nOne of the cool things about the Pi-hole is that it gives you a little at-a-glance view of how your machine is doing, including CPU load, memory utilization, and temperature. This window into system stats made me realize that my little box is packing heat:\n\nI’m running a Pi 4, which is known for generating more heat than it can handle, so temperatures of ~60 C (the upper range of “safe”) isn’t too shocking – but with summer coming and me planning to add some load to this machine in the near future, I wanted to set up monitoring to make sure my box wasn’t going to melt on me. This also has the side benefit that I’ll have a metrics system already in place for anything else I stand up on this machine.\nEnter the TIG stack. TIG – Telegraf, InfluxDB, and Grafana – is a suite of open-source solutions for collecting, storing, and visualizing time-series data, like the sort you’ll get from repeatedly measuring system temperature.\nThis tutorial will walk you through setting up each of these services separately. These steps were tested on a Raspberry Pi 4 running Raspbian Buster, so other configurations might require some tweaking.\nAll of the code here should be run in a terminal on your Raspberry Pi unless I specify it needs to go somewhere else. To make sure you’re not going to run into dependency hell, it’s a good idea to run sudo apt update && sudo apt upgrade before installing any of the stack.\n\n\n\n\n\n\nFirst up, we need to set up our InfluxDB instance. This database is where our Telegraf instance will send metrics and where Grafana will read from, so it makes sense to stand it up first!\nInstalling the service is easy enough – we just need to add Influx’s authentication key, add their repository to our trusted sources, and then install it via apt:\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb influxdb-client\n\n\nNow we want to actually start the database, and tell our system to start it after reboots – since we’re expecting to always be collecting metrics via Telegraf, we need to make sure that we always have a place to write to, as well. This is a quick two-liner using systemctl – we first need to unmask Influx, which will let us add it as a service, then tell our Pi to start the service both right now and every time the system restarts via the enable --now command:\nsudo systemctl unmask influxdb\nsudo systemctl enable --now influxdb\n \n \nAfter this, you should be able to run systemctl status influxdb to see the service status – if everything went according to plan, you should see Active: active (running) around line 3 of the output.\nAt this point, it’s probably healthy to add authentication to your Influx instance if your pi is exposed to external networks. You can set up a basic admin account via:\ninflux\nCREATE USER admin WITH PASSWORD '&lt;password&gt;' WITH ALL PRIVILEGES\n \n \nYou can then force HTTP authentication by adding the following under the HTTP header in /etc/influxdb/influxdb.conf:\n[HTTP]\nauth-enabled = true\npprof-enabled = true\npprof-auth-enabled = true\nping-auth-enabled = true\n \n \nThe changes take effect the next time your service starts, which you can trigger via sudo systemctl restart influxdb.\n\n\n\n\n\n\nWith Influx up and running, it’s time for us to start writing records, which means standing up Telegraf!\nTelegraf is updated pretty frequently, so it’s a good idea to check the release page to see what version you should be installing. At the time of writing, the current version is 1.14.2, so I ran the following to install Telegraf on my machine:\nwget https://dl.influxdata.com/telegraf/releases/telegraf_1.14.2-1_armhf.deb\nsudo dpkg -i telegraf_1.14.2-1_armhf.deb\nrm telegraf_1.14.2-1_armhf.deb\n \n \nWe now have Telegraf installed on our machine, but the service won’t do us much good before we set up our configuration, located at /etc/telegraf/telegraf.conf. Telegraf operates by coordinating a bunch of “plugins”, which work to collect and write data to and from different sources. You can see the full list of plugins at Telegraf’s GitHub repo, and activate each by copying the configuration from the plugin’s readme into your /etc/telegraf/telegraf.conf file.\nI spent far too much time pouring over the various plugins and wound up with the following configuration file – you can use this to overwrite your default telegraph.conf file and start collecting metrics right away, or you can spend the time now to set up your instance to suit your own particular needs. Just make sure you edit your [[outputs.influxdb]] to include the following:\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n \n \nMy full configuration looks like this:\n[agent]\n   # Batch size of values that Telegraf sends to output plugins.\n   metric_batch_size = 1000\n   # Default data collection interval for inputs.\n   interval = \"30s\"\n   # Added degree of randomness in the collection interval.\n   collection_jitter = \"5s\"\n   # Send output every 5 seconds\n   flush_interval = \"5s\"\n   # Buffer size for failed writes.\n   metric_buffer_limit = 10000\n   # Run in quiet mode, i.e don't display anything on the console.\n   quiet = true\n[[inputs.ping]] # # Ping given url(s) and return statistics\n## urls to ping\nurls = [\"www.github.com\",\"www.amazon.com\",\"1.1.1.1\",\"www.mm218.dev\"]\n## number of pings to send per collection (ping -c )\ncount = 3\n## interval, in s, at which to ping. 0 == default (ping -i )\nping_interval = 15.0\n## per-ping timeout, in s. 0 == no timeout (ping -W )\ntimeout = 10.0\n## interface to send ping from (ping -I )\ninterface = \"wlan0\"\n[[inputs.system]]\n[[inputs.influxdb]]\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n  ## http request & header timeout\n  timeout = \"5s\"\n[[inputs.disk]]\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n[[inputs.diskio]]\n[[inputs.internal]]\n  ## If true, collect telegraf memory stats.\n  collect_memstats = true\n[[inputs.mem]]\n[[inputs.processes]]\n# custom temperature script\n# https://github.com/mikemahoney218/pi-admin/blob/master/telegraf-scripts/systemp.sh\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n   ## The target database for metrics (telegraf will create it if not exists).\n   database = \"pi_logs\" # required\n   ## Name of existing retention policy to write to.  Empty string writes to\n   ## the default retention policy.\n   retention_policy = \"\"\n   ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\"\n   write_consistency = \"any\"\n   ## Write timeout (for the InfluxDB client), formatted as a string.\n   ## If not provided, will default to 5s. 0s means no timeout (not recommended).\n   timeout = \"10s\"\n \n \nIn putting all this together, I found out that the Telegraf plugin to measure system temperature – the thing that got me down this rabbit hole in the first place – doesn’t actually work on Raspberry Pi systems. As a workaround, I threw together a simple one-liner in Bash:\necho \"systemp temp=`cat /sys/class/thermal/thermal_zone0/temp`\"\n \n \nI saved that script off to /tmp/telegraf-scripts/systemp.sh, then added it to my telegraf.conf in the brick:\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n \n \nIf you’re not worried about measuring temperature, you don’t need (or want) to include that section in your telegraf.conf.\nIf you set up HTTP authentication for your Influx instance, you’re going to want to add username and password fields under the [[outputs.influxdb]]\nWith our configuration in place, all that’s left now is to start and enable the Telegraf service:\nsudo systemctl enable --now telegraf\n \n \nAs before, you should be able to see that the service is running without issue by running systemctl status telegraf.\nNow that your service is running, any changes that you make to your telegraf.config file will only take effect after the service restarts. You can always restart the service using sudo systemctl restart telegraf, but I personally kept forgetting to do so (and then was surprised when my metrics weren’t showing up in Influx). To deal with that, I wrote an extremely-micro service that restarts Telegraf for me.\n\n\n\n\n\n\nWe’re finally onto our last service, the G in the TIG stack, Grafana. A quick word of warning: don’t try to sudo apt install grafana. The main repository has an outdated version of Grafana, which will leave you stuck at a blank screen when you try to log on for the first time.\nInstead, we’ll install Grafana via dpkg, like we did with Telegraf. Check for the most current version at Grafana’s downloads page. At the time of writing, I was installing version 6.7.3, so my commands to install looked like this:\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nUnlike Influx and Telegraf, Grafana can be managed almost entirely from a UI. Boot up localhost:3000 on your Pi and log in using admin for both your username and password – you’ll be prompted to change it once you’re logged in for the first time.\nYou’ll then want to add your local Influx instance as a datasource for Grafana. Assuming you’ve followed along until now, the URL for your Influx instance is http://localhost:8086. You’ll also want to add whatever database Telegraf is writing to – in the sample configuration I posted, the database name is pi_logs, but you can find yours by looking for the database field under [[outputs.influxdb]]. If you added authentication to your Influx instance, you’ll also want to turn on basic auth and provide your database credentials.\n\n\n\n\n\n\nAnd with that, you should have everything you need to start monitoring your Pi – and, with a little elbow grease, anything your Pi can touch! While it certainly feels a little like overkill, I’ve now got state-of-the art tracking and system metrics for my Pi, letting me confirm beyond a shadow of a doubt that… my Pi is running too hot. With all the time I spent on this, maybe I should have just bought a fan.\nBut hey – would a fan look this good?"
  },
  {
    "objectID": "posts/2020/05/index.html#setting-up-influxdb-telegraf-and-grafana-on-raspberry-pi",
    "href": "posts/2020/05/index.html#setting-up-influxdb-telegraf-and-grafana-on-raspberry-pi",
    "title": "Installing the TIG stack on Raspberry Pi",
    "section": "",
    "text": "Do the following in a shell you’ve already auth’d into sudo on:\nsudo apt update\nsudo apt upgrade\n\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb\nsudo systemctl unmask influxdb\nsudo systemctl enable influxdb\nsudo systemctl start influxdb\n\n# you can find the current telegraf release here: https://portal.influxdata.com/downloads/\nwget https://dl.influxdata.com/telegraf/releases/telegraf-1.14.2_linux_armhf.tar.gz\ntar xf telegraf-1.14.2_linux_armhf.tar.gz\nsudo systemctl enable --now telegraf\nrm telegraf-1.14.2_linux_armhf.tar.gz\n\nsudo apt-get install -y adduser libfontconfig1\n# you can find the current grafana release here: https://grafana.com/grafana/download\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nThis should cause all three services to start on system boot. You’ll need to configure Telegraf to actually write to your local Influx instance at http://127.0.0.1:8086 (there’s a sample config under the Telegraf part of the post), then set up Grafana to read from Influx (at the same port) via the UI at localhost:3000.\n\n\n\n\n\n\nI’m getting a little cabin-fevery as the 2020 quarantine moves into its third month. To try and defray some of the extra energy, I’ve been hacking on a Pi I set up with a Pi-hole and openvpn server about a month ago.\nOne of the cool things about the Pi-hole is that it gives you a little at-a-glance view of how your machine is doing, including CPU load, memory utilization, and temperature. This window into system stats made me realize that my little box is packing heat:\n\nI’m running a Pi 4, which is known for generating more heat than it can handle, so temperatures of ~60 C (the upper range of “safe”) isn’t too shocking – but with summer coming and me planning to add some load to this machine in the near future, I wanted to set up monitoring to make sure my box wasn’t going to melt on me. This also has the side benefit that I’ll have a metrics system already in place for anything else I stand up on this machine.\nEnter the TIG stack. TIG – Telegraf, InfluxDB, and Grafana – is a suite of open-source solutions for collecting, storing, and visualizing time-series data, like the sort you’ll get from repeatedly measuring system temperature.\nThis tutorial will walk you through setting up each of these services separately. These steps were tested on a Raspberry Pi 4 running Raspbian Buster, so other configurations might require some tweaking.\nAll of the code here should be run in a terminal on your Raspberry Pi unless I specify it needs to go somewhere else. To make sure you’re not going to run into dependency hell, it’s a good idea to run sudo apt update && sudo apt upgrade before installing any of the stack.\n\n\n\n\n\n\nFirst up, we need to set up our InfluxDB instance. This database is where our Telegraf instance will send metrics and where Grafana will read from, so it makes sense to stand it up first!\nInstalling the service is easy enough – we just need to add Influx’s authentication key, add their repository to our trusted sources, and then install it via apt:\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb influxdb-client\n\n\nNow we want to actually start the database, and tell our system to start it after reboots – since we’re expecting to always be collecting metrics via Telegraf, we need to make sure that we always have a place to write to, as well. This is a quick two-liner using systemctl – we first need to unmask Influx, which will let us add it as a service, then tell our Pi to start the service both right now and every time the system restarts via the enable --now command:\nsudo systemctl unmask influxdb\nsudo systemctl enable --now influxdb\n \n \nAfter this, you should be able to run systemctl status influxdb to see the service status – if everything went according to plan, you should see Active: active (running) around line 3 of the output.\nAt this point, it’s probably healthy to add authentication to your Influx instance if your pi is exposed to external networks. You can set up a basic admin account via:\ninflux\nCREATE USER admin WITH PASSWORD '&lt;password&gt;' WITH ALL PRIVILEGES\n \n \nYou can then force HTTP authentication by adding the following under the HTTP header in /etc/influxdb/influxdb.conf:\n[HTTP]\nauth-enabled = true\npprof-enabled = true\npprof-auth-enabled = true\nping-auth-enabled = true\n \n \nThe changes take effect the next time your service starts, which you can trigger via sudo systemctl restart influxdb.\n\n\n\n\n\n\nWith Influx up and running, it’s time for us to start writing records, which means standing up Telegraf!\nTelegraf is updated pretty frequently, so it’s a good idea to check the release page to see what version you should be installing. At the time of writing, the current version is 1.14.2, so I ran the following to install Telegraf on my machine:\nwget https://dl.influxdata.com/telegraf/releases/telegraf_1.14.2-1_armhf.deb\nsudo dpkg -i telegraf_1.14.2-1_armhf.deb\nrm telegraf_1.14.2-1_armhf.deb\n \n \nWe now have Telegraf installed on our machine, but the service won’t do us much good before we set up our configuration, located at /etc/telegraf/telegraf.conf. Telegraf operates by coordinating a bunch of “plugins”, which work to collect and write data to and from different sources. You can see the full list of plugins at Telegraf’s GitHub repo, and activate each by copying the configuration from the plugin’s readme into your /etc/telegraf/telegraf.conf file.\nI spent far too much time pouring over the various plugins and wound up with the following configuration file – you can use this to overwrite your default telegraph.conf file and start collecting metrics right away, or you can spend the time now to set up your instance to suit your own particular needs. Just make sure you edit your [[outputs.influxdb]] to include the following:\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n \n \nMy full configuration looks like this:\n[agent]\n   # Batch size of values that Telegraf sends to output plugins.\n   metric_batch_size = 1000\n   # Default data collection interval for inputs.\n   interval = \"30s\"\n   # Added degree of randomness in the collection interval.\n   collection_jitter = \"5s\"\n   # Send output every 5 seconds\n   flush_interval = \"5s\"\n   # Buffer size for failed writes.\n   metric_buffer_limit = 10000\n   # Run in quiet mode, i.e don't display anything on the console.\n   quiet = true\n[[inputs.ping]] # # Ping given url(s) and return statistics\n## urls to ping\nurls = [\"www.github.com\",\"www.amazon.com\",\"1.1.1.1\",\"www.mm218.dev\"]\n## number of pings to send per collection (ping -c )\ncount = 3\n## interval, in s, at which to ping. 0 == default (ping -i )\nping_interval = 15.0\n## per-ping timeout, in s. 0 == no timeout (ping -W )\ntimeout = 10.0\n## interface to send ping from (ping -I )\ninterface = \"wlan0\"\n[[inputs.system]]\n[[inputs.influxdb]]\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n  ## http request & header timeout\n  timeout = \"5s\"\n[[inputs.disk]]\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n[[inputs.diskio]]\n[[inputs.internal]]\n  ## If true, collect telegraf memory stats.\n  collect_memstats = true\n[[inputs.mem]]\n[[inputs.processes]]\n# custom temperature script\n# https://github.com/mikemahoney218/pi-admin/blob/master/telegraf-scripts/systemp.sh\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n   ## The target database for metrics (telegraf will create it if not exists).\n   database = \"pi_logs\" # required\n   ## Name of existing retention policy to write to.  Empty string writes to\n   ## the default retention policy.\n   retention_policy = \"\"\n   ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\"\n   write_consistency = \"any\"\n   ## Write timeout (for the InfluxDB client), formatted as a string.\n   ## If not provided, will default to 5s. 0s means no timeout (not recommended).\n   timeout = \"10s\"\n \n \nIn putting all this together, I found out that the Telegraf plugin to measure system temperature – the thing that got me down this rabbit hole in the first place – doesn’t actually work on Raspberry Pi systems. As a workaround, I threw together a simple one-liner in Bash:\necho \"systemp temp=`cat /sys/class/thermal/thermal_zone0/temp`\"\n \n \nI saved that script off to /tmp/telegraf-scripts/systemp.sh, then added it to my telegraf.conf in the brick:\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n \n \nIf you’re not worried about measuring temperature, you don’t need (or want) to include that section in your telegraf.conf.\nIf you set up HTTP authentication for your Influx instance, you’re going to want to add username and password fields under the [[outputs.influxdb]]\nWith our configuration in place, all that’s left now is to start and enable the Telegraf service:\nsudo systemctl enable --now telegraf\n \n \nAs before, you should be able to see that the service is running without issue by running systemctl status telegraf.\nNow that your service is running, any changes that you make to your telegraf.config file will only take effect after the service restarts. You can always restart the service using sudo systemctl restart telegraf, but I personally kept forgetting to do so (and then was surprised when my metrics weren’t showing up in Influx). To deal with that, I wrote an extremely-micro service that restarts Telegraf for me.\n\n\n\n\n\n\nWe’re finally onto our last service, the G in the TIG stack, Grafana. A quick word of warning: don’t try to sudo apt install grafana. The main repository has an outdated version of Grafana, which will leave you stuck at a blank screen when you try to log on for the first time.\nInstead, we’ll install Grafana via dpkg, like we did with Telegraf. Check for the most current version at Grafana’s downloads page. At the time of writing, I was installing version 6.7.3, so my commands to install looked like this:\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nUnlike Influx and Telegraf, Grafana can be managed almost entirely from a UI. Boot up localhost:3000 on your Pi and log in using admin for both your username and password – you’ll be prompted to change it once you’re logged in for the first time.\nYou’ll then want to add your local Influx instance as a datasource for Grafana. Assuming you’ve followed along until now, the URL for your Influx instance is http://localhost:8086. You’ll also want to add whatever database Telegraf is writing to – in the sample configuration I posted, the database name is pi_logs, but you can find yours by looking for the database field under [[outputs.influxdb]]. If you added authentication to your Influx instance, you’ll also want to turn on basic auth and provide your database credentials.\n\n\n\n\n\n\nAnd with that, you should have everything you need to start monitoring your Pi – and, with a little elbow grease, anything your Pi can touch! While it certainly feels a little like overkill, I’ve now got state-of-the art tracking and system metrics for my Pi, letting me confirm beyond a shadow of a doubt that… my Pi is running too hot. With all the time I spent on this, maybe I should have just bought a fan.\nBut hey – would a fan look this good?"
  },
  {
    "objectID": "posts/2020/04/mechanics-of-viz/index.html",
    "href": "posts/2020/04/mechanics-of-viz/index.html",
    "title": "Mechanics of Data Visualizations",
    "section": "",
    "text": "(Note: this is part two of a three part series on data visualization, originally published on Towards Data Science in 2019.\nLet’s move from theoretical considerations of graphing to the actual building blocks you have at your disposal. As we do so, we’re also going to move on to mantra #2:\n\nEverything should be made as simple as possible – but no simpler.\nGraphs are inherently a 2D image of our data:\n\nThey have an x and a y scale, and - as in our scatter plot here - the position a point falls along each scale tells you how large its values are. But this setup only allows us to look at two variables in our data - and we’re frequently interested in seeing relationships between more than two variables.\nSo the question becomes: how can we visualize those extra variables? We can try adding another position scale:\n\nknitr::include_graphics(\"unnamed-chunk-2-1.png\")\n\n\n\n\nBut 3D images are hard to wrap your head around, complicated to produce, and not as effective in delivering your message. They do have their uses - particularly when you’re able to build real, physical 3D models, and not just make 3D shapes on 2D planes - but frequently aren’t worth the trouble.\nSo what tools do we have in our toolbox? The ones that are generally agreed upon (no, really - this is an area of active debate) fall into four categories:\n\nPosition (like we already have with X and Y)\nColor\nShape\nSize\n\nThese are the tools we can use to encode more information into our graphics. We’re going to call these aesthetics, but any number of other words could work - some people refer to them as scales, some as values. I call them aesthetics because that’s what my software of choice calls them - but the word itself comes from the fact that these are the things that change how your graph looks.\nFor what it’s worth, we’re using an EPA data set for this unit, representing fuel economy data from 1999 and 2008 for 38 popular models of car. “Hwy” is highway mileage, “displ” is engine displacement (so volume), and “cty” is city mileage. But frankly, our data set doesn’t matter right now - most of our discussion here is applicable to any data set you’ll pick up.\nWe’re going to go through each of these aesthetics, to talk about how you can encode more information in each of your graphics. Along the way, remember our mantras:\n\nA good graphic tells a story\nEverything should be made as simple as possible - but no simpler\nUse the right tool for the job\nInk is cheap. Electrons are even cheaper\n\nWe’ll talk about how these are applicable throughout this section.\n\n\nPosition\nLet’s start off discussing these aesthetics by finishing up talking about position. The distance of values along the x, y, or – in the case of our 3D graphic – z axes represents how large a particular variable is. People inherently understand that values further out on each axis are more extreme - for instance, imagine you came across the following graphic (made with simulated data):\n\nWhich values do you think are higher?\nMost people innately assume that the bottom-left hand corner represents a 0 on both axes, and that the further you get from that corner the higher the values are. This – relatively obvious – revelation hints at a much more important concept in data visualizations: perceptual topology should match data topology. Put another way, that means that values which feel larger in a graph should represent values that are larger in your data. As such, when working with position, higher values should be the ones further away from that lower left-hand corner – you should let your viewer’s subconscious assumptions do the heavy lifting for you.\nApplying this advice to categorical data can get a little tricky. Imagine that we’re looking at the average highway mileages for manufacturers of the cars in our data set:\n\nIn this case, the position along the x axis just represents a different car maker, in alphabetical order. But remember, position in a graph is an aesthetic that we can use to encode more information in our graphics. And we aren’t doing that here – for instance, we could show the same information without using x position at all:\n\nTry to compare Pontiac and Hyundai on the first graph, versus on this second one. If anything, removing our extraneous x aesthetic has made it easier to compare manufacturers. This is a big driver behind our second mantra – that everything should be made as simple as possible, but no simpler. Having extra aesthetics confuses a graph, making it harder to understand the story it’s trying to tell.\nHowever, when making a graphic, we should always be aiming to make important comparisons easy. As such, we should take advantage of our x aesthetic by arranging our manufacturers not alphabetically, but rather by their average highway mileage: \nBy reordering our graphic, we’re now able to better compare more similar manufacturers. It’s now dramatically faster to understand our visualization – closer comparisons are easier to make, so placing more similar values closer together makes them dramatically easier to grasp. Look at Pontiac vs Hyundai now, for instance. Generally speaking, don’t put things in alphabetical order - use the order you place things to encode additional information.\nAs a quick sidenote, I personally believe that, when working with categorical values along the X axis, you should reorder your values so the highest value comes first. For some reason, I just find having the tallest bar/highest point (or whatever is being used to show value) next to the Y axis line is much cleaner looking than the alternative:\n\nFor what it’s worth, I’m somewhat less dogmatic about this when the values are on the Y axis. I personally believe the highest value should always be at the top, as humans expect higher values to be further from that bottom left corner: \nHowever, I’m not as instantly repulsed by the opposite ordering as I am with the X axis, likely because the bottom bar/point being the furthest looks like a more natural shape, and is still along the X axis line: \nFor this, at least, your mileage may vary. Also, it’s worth pointing out how much cleaner the labels on this graph are when they’re on the Y axis - flipping your coordinate system, like we’ve done here, is a good way to display data when you’ve got an unwieldy number of categories.\n\n\nColor\nWhile we’ve done a good job covering the role position plays in communicating information, we’re still stuck on the same question we started off with: How can we show a third variable on the graph?\nOne of the most popular ways is to use colors to represent your third variable. It might be worth talking through how color can be used with a simulated data set. Take for example the following graph: \nAnd now let’s add color for our third variable: \nRemember: perceptual topology should match data topology. Which values are larger?\nMost people would say the darker ones. But is it always that simple? Let’s change our color scale to compare: \nSure, some of these colors are darker than others – but I wouldn’t say any of them tell me a value is particularly high or low.\nThat’s because humans don’t percieve hue – the actual shade of a color – as an ordered value. The color a point is doesn’t communicate that the point has a higher or lower value than any other point on the graph. Instead, hue works as an unordered value, which only tells us which points belong to which groupings. In order to tell how high or low a point’s value is, we instead have to use luminescence – or how bright or dark the individual point is.\nThere’s one other axis you can move colors along in order to encode value – how vibrant a color is, known as chroma:\n\nJust keep in mind that luminescence and chroma – how light a color is and how vibrant it is – are ordered values, while hue (or shade of color) is unordered This becomes relevant when dealing with categorical data. For instance, moving back to the scatter plot we started with:\n\nIf we wanted to encode a categorical variable in this – for instance, the class of vehicle – we could use hue to distinguish the different types of cars from one another:\n\nIn this case, using hue to distinguish our variables clearly makes more sense than using either chroma or luminesence:\n\nThis is a case of knowing what tool to use for the job - chroma and luminescence will clearly imply certain variables are closer together than is appropriate for categorical data, while hue won’t give your audience any helpful information about an ordered variable. Note, though, that I’d still discourage using the rainbow to distinguish categories in your graphics – the colors of the rainbow aren’t exactly unordered values (for instance, red and orange are much more similar colors than yellow and blue), and you’ll wind up implying connections between your categories that you might not want to suggest. Also, the rainbow is just really ugly:\n\nSpeaking of using the right tool for the job, one of the worst things people like to do in data visualizations is overuse color. Take for instance the following example:\n\nIn this graph, the variable “class” is being represented by both position along the x axis, and by color. By duplicating this effort, we’re making our graph harder to understand – encoding the information once is enough, and doing it any more times than that is a distraction. Remember the second mantra: Everything should be made as simple as possible – but no simpler. The best data visualization is one that includes all the elements needed to deliver the message, and no more.\nYou can feel free to use color in your graphics, so long as it adds more information to the plot - for instance, if it’s encoding a third variable:\n\nBut replicating as we did above is just adding more junk to your chart.\nThere’s one last way you can use color effectively in your plot, and that’s to highlight points with certain characteristics:\n\nDoing so allows the viewer to quickly pick out the most important sections of our graph, increasing its effectiveness. Note that I used shape instead of color to separate the class of vehicles, by the way – combining point highlighting and using color to distinguish categorical variables can work, but can also get somewhat chaotic:\n\nThere’s one other reason color is a tricky aesthetic to get right in your graphics: about 5% of the population (10% of men, 1% of women) can’t see colors at all. That means you should be careful when using it in your visualizations – use colorblind-safe color palettes (google “ColorBrewer” or “viridis” for more on these), and pair it with another aesthetic whenever possible.\n\n\nShape\nThe easiest aesthetic to pair color with is the next most frequently used – shape. This one is much more intuitive than color – to demonstrate, let’s go back to our scatter plot:\n\nWe can now change the shape of each point based on what class of vehicle it represents: \nImagine we were doing the same exercise as we did with color earlier – which values are larger?\nI’ve spoiled the answer already by telling you what the shapes represent – none of them are inherently larger than the others. Shape, like hue, is an unordered value.\nThe same basic concepts apply when we change the shape of lines, not just points. For instance, if we plot separate trendlines for front-wheel, rear-wheel, and four-wheel drive cars, we can use linetype to represent each type of vehicle:\n\nBut even here, no one linetype implies a higher or lower value than the others.\nThere are two caveats to be made to this rule, however. For instance, if we go back to our original scatter plot and change which shapes we’re using:\n\nThis graph seems to imply more connection between the first three classes of car (which are all different types of diamonds) and the next three classes (which are all types of triangle), while singling out SUVs. In this way, we’re able to use shape to imply connection between our groupings - more similar shapes, which differ only in angle or texture, imply a closer relationship to one another than to other types of shape. This can be a blessing as well as a curse - if you pick, for example, a square and a diamond to represent two unrelated groupings, your audience might accidentally read more into the relationship than you had meant to imply.\nIt’s also worth noting that different shapes can pretty quickly clutter up a graph. As a general rule of thumb, using more than 3-4 shapes on a graph is a bad idea, and more than 6 means you need to do some thinking about what you actually want people to take away.\n\n\nSize\nOur last aesthetic is that of size. Going back to our original scatter plot, we could imagine using size like this:\n\nSize is an inherently ordered value - large size points imply larger values. Specifically, humans perceive larger areas as corresponding to larger values - the points which are three times larger in the above graph are about three times larger in value, as well.\nThis becomes tricky when size is used incorrectly, either by mistake or to distort the data. Sometimes an analyst maps radius to the variable, rather than area of the point, resulting in graphs as the below:\n\nIn this example, the points representing a cty value of 10 don’t look anything close to 1/3 as large as the points representing 30. This makes the increase seem much steeper upon looking at this chart – so be careful when working with size as an aesthetic that your software is using the area of points, not radius!\nIt’s also worth noting that unlike color – which can be used to distinguish groupings, as well as represent an ordered value – it’s generally a bad idea to use size for a categorical variable. For instance, if we mapped point size to class of vehicle:\n\nWe seem to be implying relationships here that don’t actually exist, like a minivan and midsize vehicle being basically the same. As a result, it’s best to only use size for continuous (or numeric) data.\n\n\nA Tangent\nNow that we’ve gone over these four aesthetics, I want to go on a quick tangent. When it comes to how quickly and easily humans perceive each of these aesthetics, research has settled on the following order:\n\nPosition\nSize\nColor (especially chroma and luminescence)\nShape\n\nAnd as we’ve discussed repeatedly, the best data visualization is one that includes exactly as many elements as it takes to deliver a message, and no more. Everything should be made as simple as possible, but no simpler.\nHowever, we live in a world of humans, where the scientifically most effective method is not always the most popular one. And since color is inherently more exciting than size as an aesthetic, the practitioner often finds themselves using colors to denote values where size would have sufficed. And since we know that color should usually be used alongside shape in order to be more inclusive in our visualizations, size often winds up being the last aesthetic used in a chart. This is fine - sometimes we have to optimize for other things than “how quickly can someone understand my chart”, such as “how attractive does my chart look” or “what does my boss want from me”. But it’s worth noting, in case you see contradictory advice in the future - the disagreement comes from if your source is teaching the most scientifically sound theory, or the most applicable practice.\n\n\nSummary\nWe started off this section with our second mantra: that everything should be made as simple as possible, but no simpler. The first half of that cautions us against overusing aesthetics and against adding too much to a graphic, lest we erode its efficency in conveying information:\n\nThe second half cautions us against not using all the aesthetics it takes to tell our story, in case we don’t produce the most expressive graphic possible: \nInstead, we should use exactly as many aesthetics as it takes to tell our story, carefully choosing each to encode the most information possible into our graphics: \nAs for the specific takeaways from this section, I can think of the following:\n\nMatch perceptual and data topology – if a color or position feels like a higher value, use it to represent data that is a higher value\nMake important comparisons easy – place them near each other, call attention to them\nUse aesthetics to encode more information into your graphics\n\nUse exactly as many aesthetics as you need – no more, no less.\n\nDon’t place things in alphabetical order\nDon’t use the rainbow for a color scheme\nUse ordered aesthetics (like position, chroma, luminescence, and size) to show ordered values (like numeric data)\nUse unordered aesthetics (like hue or shape) to show unordered values\n\nLet’s transition away from aesthetics, and towards our third mantra:\n\n\nUse the right tool for the job.\nThink back to our first chart:\n\nAs you already know, this is a scatter plot - also known as a point graph. Now say we added a line of best fit to it:\n\nThis didn’t stop being a scatter plot once we drew a line on it – but the term scatter plot no longer really encompasses everything that’s going on here. It’s also obviously not a line chart, as even though there’s a line on it, it also has points.\nRather than quibble about what type of chart this is, it’s more helpful to describe what tools we’ve used to depict our data. We refer to these as geoms, short for geometries – because when you get really deep into things, these are geometric representations of how your data set is distributed along the x and y axes of your graph. I don’t want to get too far down that road – I just want to explain the vocabulary so that we aren’t talking about what type of chart that is, but rather what geoms it uses. Framing things that way makes it easier to understand how things can be combined and reformatted, rather than assuming each type of chart can only do one thing.\n\n\nTwo continuous variables\nThis chart uses two geoms that are really good for graphs that have a continuous y and a continuous x - points and lines. This is what people refer to most of the time when they say a line graph - a single smooth trendline that shows a pattern in the data. However, a line graph can also mean a chart where each point is connected in turn:\n\nIt’s important to be clear about which type of chart you’re expected to produce! I always refer to the prior as a trendline, for clarity.\nThese types of charts have enormous value for quick exploratory graphics, showing how various combinations of variables interact with one another. For instance, many analysts start familiarizing themselves with new data sets using correlation matrices (also known as scatter plot matrices), which create a grid of scatter plots representing each variable:\n\nIn this format, understanding interactions between your data is quick and easy, with certain variable interactions obviously jumping out as promising avenues for further exploration.\nTo back up just a little, there’s one major failing of scatter plots that I want to highlight before moving on. If you happen to have more than one point with the same x and y values, a scatter plot will just draw each point over the previous, making it seem like you have less data than you actually do. Adding a little bit of random noise - for instance, using RAND() in Excel - to your values can help show the actual densities of your data, especially when you’re dealing with numbers that haven’t been measured as precisely as they could a have been. \nOne last chart that does well with two continuous variables is the area chart, which resembles a line chart but fills in the area beneath the line: \nArea plots make sense when 0 is a relevant number to your data set – that is, a 0 value wouldn’t be particularly unexpected. They’re also frequently used when you have multiple groupings and care about their total sum:\n\n(This new data set is the “diamonds” data set, representing 54,000 diamonds sizes, qualities, cut, and sale prices. We’ll be going back and forth using it and the EPA data set from now on.)\nNow one drawback of stacked area charts is that it can be very hard to estimate how any individual grouping shifts along the x axis, due to the cumulative effects of all the groups underneath them. For instance, there are actually fewer “fair” diamonds at 0.25 carats than at 1.0 – but because “ideal” and “premium” spike so much, your audience might draw the wrong conclusions. In situations where the total matters more than the groupings, this is alright – but otherwise, it’s worth looking at other types of charts as a result.\n\n\nOne continuous variable\nIf instead you’re looking to see how a single continuous variable is distributed throughout your data set, one of the best tools at your disposal is the histogram. A histogram shows you how many observations in your data set fall into a certain range of a continuous variable, and plot that count as a bar plot:\n\nOne important flag to raise with histograms is that you need to pay attention to how your data is being binned. If you haven’t picked the right width for your bins, you might risk missing peaks and valleys in your data set, and might misunderstand how your data is distributed – for instance, look what shifts if we graph 500 bins, instead of the 30 we used above: \nAn alternative to the histogram is the frequency plot, which uses a line chart in the place of bars to represent the frequency of a value in your data set: \nAgain, however, you have to pay attention to how wide your data bins are with these charts – you might accidentally smooth over major patterns in your data if you aren’t careful! \nOne large advantage of the frequency chart over the histogram is how it deals with multiple groupings – if your groupings trade dominance at different levels of your variable, the frequency graph will make it much more obvious how they shift than a histogram will.\n(Note that I’ve done something weird to the data in order to show how the distributions change below.) \n\n\nOne categorical variable, one continuous\nIf you want to compare a categorical and continuous variable, you’re usually stuck with some form of bar chart:\n\nThe bar chart is possibly the least exciting type of graph in existence, mostly because of how prevalent it is – but that’s because it’s really good at what it does. Bar charts are one of the most easily interpreted and effective types of visualizations, no matter how exciting they are.\nHowever, some people are really intent on ruining that. Take, for instance, the stacked bar chart, often used to add a third variable to the mix:\n\nCompare Fair/G to Premium/G. It’s next to impossible to accurately compare the boxes – they don’t share a top or a bottom line, so you can’t really make a comparison. In these situations, it’s a better idea to use a dodged bar chart instead:\n\nDodged bar charts are usually a better choice for comparing the actual numbers of different groupings. However, this chart does a good job showing one of the limitations dodged bar charts come up against – once you get past 4 or 5 groupings, making comparisons is tricky. In these cases, you’re probably trying to apply the wrong chart for the job, and should consider either breaking your chart up into smaller ones – remember, ink is cheap, and electrons or cheaper – or replacing your bars with a few lines.\nThe one place where stacked bar charts are appropriate, however, is when you’re comparing the relative proportions of two different groups in each bar. For instance, take the following graph:\n\nIn this case, making comparisons across groups is trivial, made simple by the fact that the groupings all share a common line - at 100% for group 1, and at 0% for group 2. This point of reference solves the issue we had with more than two groupings – though note we’d still prefer a dodged bar chart if the bars didn’t always sum to the same amount.\n\nA Quick Tangent\nThis is usually where most people will go on a super long rant about pie charts and how bad they are. They’re wrong, but in an understandable way.\nPeople love to hate on pie charts, because they’re almost universally a bad chart. However, if it’s important for your viewer to be able to quickly figure out what proportion two or more groupings make up of the whole, a pie chart is actually the fastest and most effective way to get the point across. For instance, compare the following pie and bar charts, made with the same data set: \n\nIt’s a lot easier to tell that, say, A is smaller than C through F in the pie chart than the bar plot, since humans are better at summing angles than areas. In these instances, feel free to use a pie chart – and to tell anyone giving you flack that I said it was OK.\n\n\n\nTwo categorical variables\nOur last combination is when you’re looking to have a categorical variable on both the x and y axis. These are trickier plots to think about, as we no longer encode value in position based on how far away a point is from the lower left hand corner, but rather have to get creative in effectively using position to encode a value. Remember that a geom is a geometric representation of how your data set is distributed along the x and y axes of your graph. When both of your axes are categorical, you have to get creative to show that distribution.\nOne method is to use density, as we would in a scatter plot, to show how many datapoints you have falling into each combination of categories graphed. You can do this by making a “point cloud” chart, where more dense clouds represent more common combinations: \nEven without a single number on this chart, its message is clear - we can tell how our diamonds are distributed with a single glance. A similar way to do this is to use a heatmap, where differently colored cells represent a range of values:\n\nI personally think heatmaps are less effective – partially because by using the color aesthetic to encode this value, you can’t use it for anything else – but they’re often easier to make with the resources at hand."
  },
  {
    "objectID": "posts/2020/04/making-excellent-viz/index.html",
    "href": "posts/2020/04/making-excellent-viz/index.html",
    "title": "Making Excellent Visualizations",
    "section": "",
    "text": "As we move into our final section, it’s time to dwell on our final mantra:\n\nInk is cheap. Electrons are even cheaper.\nThis is a fancy, dogmatic way to say: Make more than one chart. It’s rare that your first try is going to produce your best looking output. Play around with your data set, try out different visuals, and keep the concepts we’ve talked about in mind. Your graphs will be all the better for it. In this section, we’ll talk about solutions to some of the most common problems people have with making charts:\n\n\nDealing with big data sets\nThink back to the diamonds data set we used in the last section. It contains data on 54,000 individual diamonds, including the carat and sale price for each. If we wanted to compare those two continuous variables, we might think a scatter plot would be a good way to do so:\n\nUnfortunately, it seems like 54,000 points is a few too many for this plot to do us much good! This is a clear case of what’s called overplotting – we simply have too much data on a single graph.\nThere are three real solutions to this problem. First off, we could decide simply that we want to refactor our chart, and instead show how a metric – such as average sale price – changes at different carats, rather than how our data is distributed:\n\nThere are all sorts of ways we can do this sort of refactoring – if we wanted, we could get a very similar graph by binning our data and making a bar plot:\n\nEither way, though, we’re not truly showing the same thing as was in the original graph – we don’t have any indication of the actual distribution of our data set along these axes.\nThe second solution solves this problem much more effectively – make all your points semi-transparent:\n\nBy doing this, we’re now able to see areas where our data is much more densely distributed, something that was lost in the summary statistics – for instance, it appears that low-carat diamonds are much more tighly grouped than higher carat ones. We can also see some dark stripes at “round-number” values for carat – that indicates to me that our data has some integrity issues, if appraisers are more likely to give a stone a rounded number.\nThe challenge with this approach comes when we want to map a third variable – let’s use cut – in our graphic. We can try to change the aesthetics of our graph as usual:\n\nBut unfortunately the sheer number of points drowns out most of the variance in color and shape on the graphic. In this case, our best option may be to turn to option number three and facet our plots – that is, to split our one large plot into several small multiples:\n\nRemember: Ink is cheap. Electrons are even cheaper. Make more than one graph.\nBy splitting out our data into several smaller graphics, we’re much better able to see how the distribution shifts between our categories. In fact, we could use this technique to split our data even further, into a matrix of scatter plots showing how different groups are distributed:\n\nOne last, extremely helpful use of faceting is to split apart charts with multiple entangled lines: \nThese charts, commonly referred to as “spaghetti charts”, are usually much easier to use when split into small multiples:\n\nNow, one major drawback of facet charts is that they can make comparisons much harder – if, in our line chart, it’s more important to know that most clarities are similar in price at 2 carats than it is to know how the price for each clarity changes with carat, then the first chart is likely the more effective option. In those cases, however, it’s worth reassessing how many lines you actually need on your graph – if you only care about a few clarities, then only include those lines, and if you only care about a narrow band of prices or carats, window your data so that’s all you show. The goal is to make making comparisons easy, with the understanding that some comparisons are more important than others.\n\n\nDealing with chartjunk\nCast your mind back to the graphic I used as an example of an explanatory chart:\n\nYou might have noticed that this chart is differently styled from all the others in this course – it doesn’t have the grey background or grid lines or anything else.\nThink back to our second mantra: everything should be made as simple as possible, but no simpler. This chart reflects that goal. We’ve lost some of the distracting elements – the colored background and grid lines – and changed the other elements to make the overall graphic more effective. The objective is to have no extraneous element on the graph, so that it might be as expressive and effective as possible. This usually means using minimal colors, minimal text, and no grid lines. (After all, those lines are usually only useful in order to pick out a specific value – and if you’re expecting people to need specific values, you should give them a table!)\nThose extraneous elements are known as chartjunk. You see this a lot with graphs made in Excel – they’ll have dark backgrounds, dark lines, special shading effects or gradients that don’t encode information, or – worst of all – those “3D” bar/line/pie charts, because these things can be added with a single click. However, they tend to make your graphics less effective as they force the user to spend more time separating data from ornamentation. Everything should be made as simple as possible, but no simpler; every element of your graphic should increase expressiveness or effectiveness. In short: don’t try to pretty up your graph with non-useful elements.\nAnother common instance of chartjunk is animation in graphics. While animated graphics are exciting and trendy, they tend to reduce the effectiveness of your graphics because as humans, when something is moving we can’t focus on anything else. Check out these examples from the Harvard Vision Lab – they show just how hard it is to notice changes when animation is added. This isn’t to say you can never use animation – but its uses are best kept to times when your graphic looking cool is more important than it conveying information.\n\n\nCommon Mistakes\nAs we wind down this section, I want to touch on a few common mistakes that didn’t have a great home in any other section – mostly because we were too busy talking about good design principles.\n\n\nDual y axes\nChief amongst these mistakes are plots with two y axes, beloved by charlatans and financial advisors since days unwritten. Plots with two y axes are a great way to force a correlation that doesn’t really exist into existence on your chart, through manipulation of your units and axes. In almost every case, you should just make two graphs – ink is cheap. Electrons are even cheaper.\nFor an extremely entertaining read on this subject, check out this link. I’ve borrowed Kieran’s code for the below viz – look at how we can imply different things, just by changing how we scale our axes!\n\n\n\nOvercomplex visualizations\nAnother common issue in visualizations comes from the analyst getting a little too technical with their graphs. For instance, think back to our original diamonds scatter plot: \nLooking at this chart, we can see that carat and price have a positive correlation – as one increases, the other does as well. However, it’s not a linear relationship; instead, it appears that price increases faster as carat increases.\nThe more statistically-minded analyst might already be thinking that we could make this relationship linear by log-transforming the axes – and they’d be right! We can see a clear linear relationship when we make the transformation:\n\nUnfortunately, transforming your visualizations in this way can make your graphic hard to understand – in fact, only about 60% of professional scientists can even understand them. As such, transforming your axes like this tends to reduce the effectiveness of your graphic – this type of visualization should be reserved for exploratory graphics and modeling, instead.\n\n\nConclusion\nAnd that just about wraps up this introduction to the basic concepts of data visualizations. Hopefully you’ve picked up some concepts or vocabulary that can help you think about your own visualizations in your daily life. I wanted to close out here with a list of resources I’ve found helpful in making graphics – I’ll keep adding to this over time:\n\nWhen picking colors, I often find myself reaching for one of the following tools:\n\nColorBrewer provided most of the palettes for these graphics\nColorSupply makes picking custom colors easier\nViridis provides beautiful, colorblind-friendly palettes for use (though this resource is a little harder to understand)\n\nI used the following resources in putting this post together:\n\nHadley Wickham’s Stat 405 Course, particularly the lecture on effective visualizations (I’ve lifted “perceptual topology should match data toplogy”, “make important comparisons easy”, and “visualization is only one part of data analysis” directly from his slides)\nJeffrey Heer’s CSE 442 lecture on visualizations, particularly the definitions for expressiveness and effectiveness"
  },
  {
    "objectID": "posts/2020/04/corona-viz/index.html",
    "href": "posts/2020/04/corona-viz/index.html",
    "title": "A minimalist visualization of Coronavirus rates",
    "section": "",
    "text": "I had been getting frustrated with not being able to quickly find coronavirus data for my area, and not being able to see recent trends without framing and interpretation. So I grabbed down the John Hopkins CSSE data and made a quick Shiny app to visualize case and death rates. I’m trying to not contribute to the constant noise surrounding the ongoing pandemic, but having a way to see these numbers without an overwhelming amount of surrounding editorialization has made me feel like I understand the world a bit better.\n\nknitr::include_graphics(\"covid.png\")\n\n\n\n\nThe app lives at this link. Thanks to R Studio, who are providing free hosting for coronavirus apps through the pandemic."
  },
  {
    "objectID": "posts/2020/04/theory-of-data-viz/index.html",
    "href": "posts/2020/04/theory-of-data-viz/index.html",
    "title": "Theory of Data Visualizations",
    "section": "",
    "text": "(Note: this is part one of a three part series on data visualization, originally published on Towards Data Science in 2019.\nData visualization – our working definition will be “the graphical display of data” – is one of those things like driving, cooking, or being funny: everyone thinks they’re really great at it, because they’ve been doing it for a while, and yet many – if not most – people don’t even know where they could start learning how much better they could be doing things. For something so essential to so many people’s daily work, data visualization is so rarely directly taught, and is usually assumed to be something people will pick up with time.\nHowever, that isn’t the best approach. Data visualization is a skill like any other, and even experienced practitioners could benefit from honing their skills in the subject. Hence, this series.\nThis series doesn’t set out to teach you how to make a specific graphic in a specific software. I don’t know what softwares might be applicable to your needs in the future, or what visualizations you’ll need to formulate when, and quite frankly Google exists – so this isn’t a cookbook with step-by-step instructions. The goal here is not to provide you with recipes for the future, but rather to teach you what flour is – to introduce you to the basic concepts and building blocks of effective data visualizations.\n\nThe mantras\nAs much as possible, I’ve collapsed those concepts into four mantras we’ll return to throughout this course. The mantras are:\n\nA good graphic tells a story.\nEverything should be made as simple as possible, but no simpler.\nUse the right tool for the job.\nInk is cheap. Electrons are even cheaper.\n\nEach mantra serves as the theme for a section, and will also be interwoven throughout. The theme of this section is, easily enough:\n\n\nA good graphic tells a story\nWhen making a graphic, it is important to understand what the graphic is for. After all, you usually won’t make a chart that is an exact depiction of your data – modern data sets tend to be too big (in terms of number of observations) and wide (in terms of number of variables) to depict every datapoint on a single graph. Instead, the analyst consciously chooses what elements to include in a visualization in order to identify patterns and trends in the data in the most effective manner possible. In order to make those decisions, it helps a little to think both about why and how graphics are made.\n\n\nWhy do we tell a story?\nAs far as the why question goes, the answer usually comes down to one of two larger categories:\n\nTo help identify patterns in a data set, or\nTo explain those patterns to a wider audience\n\nThese are the rationales behind creating what are known as, respectively, exploratory and explanatory graphics. Exploratory graphics are often very simple pictures of your data, built to identify patterns in your data that you might not know exist yet. Take for example a simple graphic, showing tree circumference as a function of age:\n\nThis visualization isn’t anything too complex – two variables, thirty-five observations, not much text – but it already shows us a trend that exists in the data. We could use this information, if we were so inspired, to start investigating the whys of why tree growth changes with age, now that we’re broadly aware of how it changes.\nExplanatory graphs, meanwhile, are all about the whys. Where an exploratory graphic focuses on identifying patterns in the first place, an explanatory graphic aims to explain why they happen and – in the best examples – what exactly the reader is to do about them. Explanatory graphics can exist on their own or in the context of a larger report, but their goals are the same: to provide evidence about why a pattern exists and provide a call to action. For instance, we can reimagine the same tree graph with a few edits in order to explain what patterns we’re seeing:\n\nknitr::include_graphics(\"theory2.png\")\n\n\n\n\nI want to specifically call out the title here: “Orange tree growth tapers by year 4.” A good graphic tells a story, remember. As such, whatever title you give your graph should reflect the point of that story – titles such as “Tree diameter (cm) versus age (days)” and so on add nothing that the user can’t get from the graphic itself. Instead, use your title to advance your message whenever it makes sense – otherwise, if it doesn’t add any new information, you’re better off erasing it altogether.\nThe important takeaway here is not that explanatory graphics are necessarily more polished than exploratory ones, or that exploratory graphics are only for the analyst – periodic reporting, for instance, will often use highly polished exploratory graphics to identify existing trends, hoping to spur more intensive analysis that will identify the whys. Instead, the message is that knowing the end purpose of your graph – whether it should help identify patterns in the first place or explain how they got there – can help you decide what elements need to be included to tell the story your graphic is designed to address.\n\n\nHow do we tell a story?\nThe other important consideration when thinking about graph design is the actual how you’ll tell your story, including what design elements you’ll use and what data you’ll display. My preferred paradigm when deciding between the possible “hows” is to weigh the expressiveness and effectiveness of the resulting graphic – as defined by Jeffrey Heer at the University of Washington, that means:\n\nExpressiveness: A set of facts is expressible in a visual language if the sentences (i.e. the visualizations) in the language express all the facts in the set of data, and only the facts in the data.\n\nEffectiveness: A visualization is more effective than another visualization if the information conveyed by one visualization is more readily perceived than the information in the other visualization.\n\nOr, to simplify:\n\nTell the truth and nothing but the truth (don’t lie, and don’t lie by omission)\nUse encodings that people decode better (where better = faster and/or more accurate)\n\nKeep this concept in the back of your mind as we move into the mechanics of data visualization in our next post – it should be your main consideration while deciding which elements you use! We’ll keep returning to these ideas of explanatory and exploratory, as well as expressiveness and effectiveness, throughout the rest of this article."
  },
  {
    "objectID": "posts/2020/06/index.html",
    "href": "posts/2020/06/index.html",
    "title": "Make a Retweet Bot in R",
    "section": "",
    "text": "A while back, I made a tweetbot that retweets a set of ecology-related hashtags, in order to signal boost ecology content in a similar manner to statsbot or Plotter Bot. The code to do this is pretty simple – made almost trivial by the rtweet package – but I found that environmental hashtags have a pretty low signal-to-noise ratio, driven down by various political and industry groups, as well as trolls.\nI didn’t want to get into the business of content filtering, so instead I started looking for other markers that a tweet was – or wasn’t – worth promoting, and have gotten to what I believe is a respectable place with my filtration. So I’ve open-sourced the code (without the specific values I use to filter) for anyone else who might be interested in setting up their own automated retweet app."
  },
  {
    "objectID": "posts/2020/10/index.html",
    "href": "posts/2020/10/index.html",
    "title": "Some Updates",
    "section": "",
    "text": "It’s been a busy few months! Rather than try to make separate update posts for everything I’ve been up to, here’s a list of the top bullets from my past four months."
  },
  {
    "objectID": "posts/2020/10/index.html#new-digs-and-new-gigs",
    "href": "posts/2020/10/index.html#new-digs-and-new-gigs",
    "title": "Some Updates",
    "section": "New Digs and New Gigs",
    "text": "New Digs and New Gigs\nI’ve left Boston and left Wayfair to take a PhD position at SUNY-ESF in Syracuse, working with Colin Beier and Aidan Ackerman to make 3D landscape visualizations as a way to improve the interpretability of ecological models and help democratize scientific outputs."
  },
  {
    "objectID": "posts/2020/10/index.html#paper-stem-size-selectivity-is-stronger-than-species-preferences-for-beaver-a-central-place-forager",
    "href": "posts/2020/10/index.html#paper-stem-size-selectivity-is-stronger-than-species-preferences-for-beaver-a-central-place-forager",
    "title": "Some Updates",
    "section": "Paper: Stem size selectivity is stronger than species preferences for beaver, a central place forager",
    "text": "Paper: Stem size selectivity is stronger than species preferences for beaver, a central place forager\nMy first first-authored paper is out in Forest Ecology and Management! I wrote a little about the process of this paper, from start to finish, on my extremely short-lived newsletter."
  },
  {
    "objectID": "posts/2020/10/index.html#tweetbots",
    "href": "posts/2020/10/index.html#tweetbots",
    "title": "Some Updates",
    "section": "Tweetbots",
    "text": "Tweetbots\nI’ve now got a small army of robots living in the corner of my apartment, tweeting out to their heart’s content. At the moment, I’ve got three retweet bots running off the original ecology_tweets codebase, namely @ecology_tweets, @rstats_tweets (a more heavily-filtered alternative to the more popular rstatstweets bot), and @30daymap_tweets, built for the #30DayMapChallenge.\nMore interesting are the two GPT-2 “AI” tweetbots now running, including @fortunes_teller and @fund_me_please_. The former is trained against a collection of fortunes packages from various *nix distros and the R fortunes package, while the latter was run against 150 GRFP personal statements and is now tweeting out some frankly bizarre applications of its own making."
  },
  {
    "objectID": "posts/2020/10/index.html#terrainr",
    "href": "posts/2020/10/index.html#terrainr",
    "title": "Some Updates",
    "section": "{terrainr}",
    "text": "{terrainr}\nI’ve got a new R package out, the first real “product” from my PhD. {terrainr} wraps the USGS National Map family of APIs to help users download geospatial data for their areas of interest, and provides functionality to turn those files into tiles that can be imported into Unity for 3D landscape visualization:\n\n\n\n\n\nA 3D landscape visualization of the area southeast of Mt. Whitney, California, USA.\n\n\n\n\nI love writing packages with visual outputs. I love writing packages with visual outputs that look like this."
  },
  {
    "objectID": "posts/2020/10/index.html#misc",
    "href": "posts/2020/10/index.html#misc",
    "title": "Some Updates",
    "section": "Misc",
    "text": "Misc\nI bought a pen plotter."
  },
  {
    "objectID": "posts/2022-10-04-rsample/index.html",
    "href": "posts/2022-10-04-rsample/index.html",
    "title": "How rsample keeps memory usage low",
    "section": "",
    "text": "A few months back, I wrote two comments on a GitHub issue explaining a bit of how rsample works under the hood. Specifically, a user asked how rsample keeps the total amount of memory that its resamples use relatively low. I’ve sent this GitHub issue to a few people since then, so it felt like it might be useful enough to turn the issue into a blog.1"
  },
  {
    "objectID": "posts/2022-10-04-rsample/index.html#whats-an-rsample",
    "href": "posts/2022-10-04-rsample/index.html#whats-an-rsample",
    "title": "How rsample keeps memory usage low",
    "section": "What’s an rsample?",
    "text": "What’s an rsample?\nIn case you’ve never used it, rsample is an R package for data resampling – if you need bootstrap resampling, V-fold cross-validation, permutation sampling, and more, rsample is meant for you.2 The majority of these rsample functions return rset objects, which are just jazzed-up tibbles:\n\nset.seed(123)\nlibrary(rsample)\nlibrary(mlbench)\ndata(LetterRecognition)\n\nboots &lt;- bootstraps(LetterRecognition, times = 2)\nboots\n\n# Bootstrap sampling \n# A tibble: 2 × 2\n  splits               id        \n  &lt;list&gt;               &lt;chr&gt;     \n1 &lt;split [20000/7403]&gt; Bootstrap1\n2 &lt;split [20000/7375]&gt; Bootstrap2\n\n\nEach of our individual resamples is stored as an rsplit object, each of which takes up a row in the splits column. Printing these objects tells us how many rows are in our analysis and assessment sets,3 but hides most of the actual structure of the rsplit object. If we use str() instead, we can see that there are three named elements in each rsplit: data, our original data frame; in_id, which has the indices for which observations are going to be held “in” our analysis set, and out_id, which sometimes4 has the indices for which observations are going to be held “out” to make up our assessment set, but here is NA:\n\nboots$splits[[1]]\n\n&lt;Analysis/Assess/Total&gt;\n&lt;20000/7403/20000&gt;\n\nstr(\n  boots$splits[[1]]\n)\n\nList of 4\n $ data  :'data.frame': 20000 obs. of  17 variables:\n  ..$ lettr: Factor w/ 26 levels \"A\",\"B\",\"C\",\"D\",..: 20 9 4 14 7 19 2 1 10 13 ...\n  ..$ x.box: num [1:20000] 2 5 4 7 2 4 4 1 2 11 ...\n  ..$ y.box: num [1:20000] 8 12 11 11 1 11 2 1 2 15 ...\n  ..$ width: num [1:20000] 3 3 6 6 3 5 5 3 4 13 ...\n  ..$ high : num [1:20000] 5 7 8 6 1 8 4 2 4 9 ...\n  ..$ onpix: num [1:20000] 1 2 6 3 1 3 4 1 2 7 ...\n  ..$ x.bar: num [1:20000] 8 10 10 5 8 8 8 8 10 13 ...\n  ..$ y.bar: num [1:20000] 13 5 6 9 6 8 7 2 6 2 ...\n  ..$ x2bar: num [1:20000] 0 5 2 4 6 6 6 2 2 6 ...\n  ..$ y2bar: num [1:20000] 6 4 6 6 6 9 6 2 6 2 ...\n  ..$ xybar: num [1:20000] 6 13 10 4 6 5 7 8 12 12 ...\n  ..$ x2ybr: num [1:20000] 10 3 3 4 5 6 6 2 4 1 ...\n  ..$ xy2br: num [1:20000] 8 9 7 10 9 6 6 8 8 9 ...\n  ..$ x.ege: num [1:20000] 0 2 3 6 1 0 2 1 1 8 ...\n  ..$ xegvy: num [1:20000] 8 8 7 10 7 8 8 6 6 1 ...\n  ..$ y.ege: num [1:20000] 0 4 3 2 5 9 7 2 1 1 ...\n  ..$ yegvx: num [1:20000] 8 10 9 8 10 7 10 7 7 8 ...\n $ in_id : int [1:20000] 18847 18895 2986 1842 3371 11638 4761 6746 16128 2757 ...\n $ out_id: logi NA\n $ id    : tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ id: chr \"Bootstrap1\"\n - attr(*, \"class\")= chr [1:2] \"boot_split\" \"rsplit\""
  },
  {
    "objectID": "posts/2022-10-04-rsample/index.html#the-mystery-of-the-missing-mbs",
    "href": "posts/2022-10-04-rsample/index.html#the-mystery-of-the-missing-mbs",
    "title": "How rsample keeps memory usage low",
    "section": "The mystery of the missing MBs",
    "text": "The mystery of the missing MBs\nSo, just looking at this structure, it seems like each rsplit contains a complete copy of our original data. But somehow, to borrow the example from the rsample README, creating a 50-times bootstrap sample doesn’t require 50 times as much memory, but instead about 3x:\n\nlobstr::obj_size(LetterRecognition)\n\n2.64 MB\n\nset.seed(35222)\nboots &lt;- bootstraps(LetterRecognition, times = 50)\nlobstr::obj_size(boots)\n\n6.69 MB\n\n\nEven that top-line result is a little misleading, though, because rsample isn’t copying the data to actually create boots. If we look at the object sizes for both the original data and the resamples together, we can see that boots is only contributing ~4 MB:\n\nlobstr::obj_size(LetterRecognition, boots)\n\n6.69 MB\n\nlobstr::obj_sizes(LetterRecognition, boots)\n\n* 2.64 MB\n* 4.04 MB\n\n\nSo: what? How?"
  },
  {
    "objectID": "posts/2022-10-04-rsample/index.html#copying-modifying",
    "href": "posts/2022-10-04-rsample/index.html#copying-modifying",
    "title": "How rsample keeps memory usage low",
    "section": "Copying; modifying",
    "text": "Copying; modifying\nWell, R uses what’s known as copy-on-modify semantics. That means that, when you assign the same data to multiple variables, each of those variables will actually point at the same address in RAM:\n\nLetterRecognition2 &lt;- LetterRecognition\n\nlobstr::obj_addr(LetterRecognition)\n\n[1] \"0x5573114c93e0\"\n\nlobstr::obj_addr(LetterRecognition2)\n\n[1] \"0x5573114c93e0\"\n\nidentical(\n  lobstr::obj_addr(LetterRecognition),\n  lobstr::obj_addr(LetterRecognition2)\n)\n\n[1] TRUE\n\n\nThis also means that LetterRecognition2 takes up literally 0 space in your RAM:\n\nlobstr::obj_size(LetterRecognition, LetterRecognition2)\n\n2.64 MB\n\n\nAnd that will stay true up until we modify either of these objects. No copy is made, no additional RAM gets used, until one of the objects is modified.\nThat also means that, right now, LetterRecognition2 is another name for the data stored in each of our rsplits:\n\nidentical(\n  lobstr::obj_addr(boots$splits[[1]]$data),\n  lobstr::obj_addr(LetterRecognition2)\n)\n\n[1] TRUE\n\n\nAnd if we get rid of LetterRecognition, which both LetterRecognition2 and our bootstraps are based off of, those objects will still point at the same address,5 and our data slot in boots still won’t take up additional space:\n\nrm(LetterRecognition)\ngc()\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  849739 45.4    1358681 72.6  1358681 72.6\nVcells 2362850 18.1    8388608 64.0  8384745 64.0\n\nidentical(\n  lobstr::obj_addr(boots$splits[[1]]$data),\n  lobstr::obj_addr(LetterRecognition2)\n)\n\n[1] TRUE\n\nlobstr::obj_sizes(LetterRecognition2, boots$splits[[1]]$data)\n\n* 2.64 MB\n*     0 B\n\n\nSo how does rsample keep its objects so small? By not making extra copies of your data where it doesn’t have to. This is how the entire boots table winds up only adding ~1.5x the space of the original data:\n\nlobstr::obj_sizes(LetterRecognition2, boots)\n\n* 2.64 MB\n* 4.04 MB\n\n\nAnd that’s pretty close to as small as this object could get – that’s just the amount of space required to store the indices (in this case, 20,000 indices per repeat, 50 repeats):\n\nlobstr::obj_size(sample.int(20000 * 50))\n\n4.00 MB\n\n\n(The 42kb difference is the attributes we’ve attached to each split – things like its class and ID and so on – but that’s not going to be enough memory to be relevant for most applications.)\nThis is also, as it happens, why out_id is set NA in our bootstrap resamples.6 Because you can figure out which observations we want to “hold out” for the assessment set based on which ones we’re keeping “in” for analysis, rsample doesn’t store those indices for most of its resampling methods.7\nAnd one last thought: if you modified LetterRecognition2, then the data in our splits would no longer point at the same address space as the original table. That’s entirely on purpose and desirable, because once you start messing with your original data, your resampling indices are no longer guaranteed to correspond to the original table you used to create them.\n\nLetterRecognition2 &lt;- NA\n\nidentical(\n  lobstr::obj_addr(boots$splits[[1]]$data),\n  lobstr::obj_addr(LetterRecognition2)\n)\n\n[1] FALSE\n\n\nBut, as best as possible, rsample will keep the rset small.\n\nlobstr::obj_size(boots)\n\n6.69 MB"
  },
  {
    "objectID": "posts/2022-10-04-rsample/index.html#footnotes",
    "href": "posts/2022-10-04-rsample/index.html#footnotes",
    "title": "How rsample keeps memory usage low",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlus, I’ve been writing my candidacy exam for two weeks now, and need an excuse to look at anything else for an hour.↩︎\nFor what it’s worth, while I’m an author on rsample, I didn’t write any of the rsample features mentioned in this blog post. I believe the rsample-specific details were all written by Max Kuhn. All the copy-on-modify semantics stuff, however, is just part of R and written over the past few decades by R Core.↩︎\n“Analysis” maps to “training” while “assessment” maps to “testing”. “Analysis” and “assessment” are purposefully used to avoid confusion over which training and test set are being used.↩︎\nWe’ll come back to this.↩︎\nAs of R 4.0, as I understand it.↩︎\nTold ya we’d come back to it.↩︎\nNow the package I maintain, spatialsample, does include out_id on its objects relatively often. Most of the time, this is because the objects were created with a non NULL buffer, and so our hold out set isn’t simply “all of the data that’s not in”; sometimes it’s because I initially always included out_id, and haven’t fixed my code to be more efficient yet. PRs welcome!↩︎"
  },
  {
    "objectID": "posts/2022-08-10-ground-filtering/index.html",
    "href": "posts/2022-08-10-ground-filtering/index.html",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "",
    "text": "I have a new open-access article, “Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes”, out today in GIScience & Remote Sensing.\nA very common pre-processing step, when deriving predictors for machine learning models from LiDAR point clouds, is to filter out “ground returns” based on a simple height threshold (where all returns below a certain level are removed). This practice comes from the early days of LiDAR, where researchers were attempting to measure the heights of individual trees using LiDAR returns, and makes a lot of sense in that context; if a return didn’t hit a tree, it probably can’t tell you much about how tall that tree is. But the process stuck around as we moved from modeling trees to forests, and knowing that a return could reach the ground – that it was able to find a gap in the canopy to reach the ground through – might reflect important characteristics of stand openness and therefore structure.\nWe show that filtering out “ground returns” from LiDAR based on height thresholds biases the derived predictors, and that bias can make predictive models of forest structure (in this case, AGB) worse. Particularly for models attempting to represent mixed landscape types, where gaps in the canopy are more common, filtering ground returns might remove valuable information and result in your model making worse predictions.\nOr as the cliche machine might put it: As we move to focus on seeing forests more than trees, it’s time for us to see these data as signal, not just noise.\nThis was a very fun project, with an awesome team (Lucas Johnson, Eddie Bevilacqua, and Colin Beier). I’m very excited to see this one in press!"
  },
  {
    "objectID": "posts/2021-06-12-virtual-environments-talk-at-user-2021/index.html",
    "href": "posts/2021-06-12-virtual-environments-talk-at-user-2021/index.html",
    "title": "Virtual environments talk at useR! 2021",
    "section": "",
    "text": "I’m thrilled to be giving a talk (with Colin Beier and Aidan Ackerman) at useR 2021! We’re calling the talk “Virtual Environments: Using R as a Frontend for 3D Rendering of Digital Landscapes” – generally speaking, I’m talking about the idea of using R to create visualizations inside game engines, and specifically about how we do so using terrainr.\nThe video should go up on YouTube after the conference, and the slides (and a rough script for the talk) are online at https://github.com/mikemahoney218/user2021 . My favorite line from the script is “R is where our users are” – I didn’t quite realize I had any Boston accent left until I tripped over that sentence a few times."
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html",
    "href": "posts/2021/02/terrainr/index.html",
    "title": "terrainr 0.3.0 is out today",
    "section": "",
    "text": "terrainr is in review with rOpenSci and the first review just came back! I’ve been working through the comments over the past week or so, and today that work has culminated in the release of terrainr version 0.3.0.\nThis is a big release with a handful of breaking changes, so I felt like I should give a brief overview of the biggest user-facing changes."
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html#breaking-changes",
    "href": "posts/2021/02/terrainr/index.html#breaking-changes",
    "title": "terrainr 0.3.0 is out today",
    "section": "Breaking Changes",
    "text": "Breaking Changes\n\nObject Classes Are Dead; Long Live Object Classes\nThe single largest change is that terrainr specific classes are no longer exported, and users shouldn’t need to worry about getting data into or out of those formats anymore. Instead, use any sf or Raster object in their place. For instance, workflows that used to look like this:\n\n# Doesn't run:\nlibrary(terrainr)\nsimulated_data &lt;-  data.frame(id = seq(1, 100, 1),\n                              lat = runif(100, 44.04905, 44.17609), \n                              lng = runif(100, -74.01188, -73.83493))\n\nbbox &lt;- get_bbox(lat = simulated_data$lat, lng = simulated_data$lng) \n\noutput_tiles &lt;- get_tiles(bbox = bbox,\n                          services = c(\"elevation\", \"ortho\"),\n                          resolution = 90)\n\nNow look like this:\n\nlibrary(terrainr)\nsimulated_data &lt;-  data.frame(id = seq(1, 100, 1),\n                              lat = runif(100, 44.04905, 44.17609), \n                              lng = runif(100, -74.01188, -73.83493))\n\nsimulated_data &lt;- sf::st_as_sf(simulated_data, coords = c(\"lng\", \"lat\"))\nsimulated_data &lt;- sf::st_set_crs(simulated_data, 4326)\n\noutput_tiles &lt;- get_tiles(data = simulated_data,\n                          services = c(\"elevation\", \"ortho\"),\n                          resolution = 90)\n\nAs part of this change, get_bbox, get_coordinate_bbox, and all class creation and export functions are gone now. Use sf (or Raster*) objects in their place instead.\n\n\nNew Names, Who This?\nget_tiles now uses the services argument to name its output list:\n\nnames(output_tiles)\n\n[1] \"elevation\" \"ortho\"    \n\n\nThis means that if you request the service elevation you can retrieve your tiles using the name elevation. If you request the same endpoint with multiple names, get_tiles will use whatever name was first in the vector.\n\n\nFewer Utilities, More Useful\nUtility functions calc_haversine_distance, convert_distance, point_from_distance, rad_to_deg, and deg_to_rad have been removed (or removed from exports). For unit conversions, check out the units package. This shouldn’t impact the main uses of the package, but is still worth flagging."
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html#show-me-what-you-got",
    "href": "posts/2021/02/terrainr/index.html#show-me-what-you-got",
    "title": "terrainr 0.3.0 is out today",
    "section": "Show Me What You Got",
    "text": "Show Me What You Got\nterrainr 0.3.0 adds a ggplot2 geom, geom_spatial_rgb, for plotting 3 band RGB rasters:\n\nlibrary(ggplot2)\n\nggplot() + \n  geom_spatial_rgb(data = output_tiles[[\"ortho\"]],\n                   # Required aesthetics r/g/b specify color bands:\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nNote that above we just passed the file path to our raster; we can also pass a RasterStack:\n\northo &lt;- raster::stack(output_tiles[[\"ortho\"]])\n\nggplot() + \n  geom_spatial_rgb(data = ortho,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nOr a data.frame:\n\northo_df &lt;- raster::as.data.frame(ortho, xy = TRUE)\nnames(ortho_df) &lt;- c(\"x\", \"y\", \"red\", \"green\", \"blue\")\n\nggplot() + \n  geom_spatial_rgb(data = ortho,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nNote that each step here gives you a little more control over the output image – for instance, if your raster bands aren’t in RGB order (or you have more than RGBA bands), you’ll need to provide a data.frame to get a true color image.\nYou can then use these basemaps like most other ggplot geoms:\n\nggplot() + \n  geom_spatial_rgb(data = ortho_df,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  geom_sf(data = simulated_data) + \n  coord_sf(crs = 4326)"
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html#new-docs-who-this",
    "href": "posts/2021/02/terrainr/index.html#new-docs-who-this",
    "title": "terrainr 0.3.0 is out today",
    "section": "New Docs, Who This?",
    "text": "New Docs, Who This?\nThose are just a few of the changes in 0.3.0; you can find a longer list in the NEWS file.\nOne thing not mentioned in the NEWS file, though, is that this version of terrainr included a complete rewrite of the documentation. The docs were mostly written while the package was being conceptually developed, and as a result gave a bit too much emphasis to some ideas while completely ignoring others. So I’ve rewritten all of the documentation that lives on the terrainr website – let me know what you think about the new versions (or if you catch anything I’ve missed!)."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#introduction",
    "href": "posts/2021/01/model-averaging/index.html#introduction",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Introduction",
    "text": "Introduction\nBuilding models is hard. Choosing what models to build can be even harder. With seemingly infinite different modeling approaches to select between (and somehow even more individual implementations), it can be difficult to guess what methods will be the best fit for your data – particularly if you’re working with data that will change over time with new observations or predictors being added to the mix.\nUsually, we disclose this sort of uncertainty with things like confidence intervals and standard errors. Yet when it comes to selecting a single model, we often don’t discuss how confident we are in that model being the right one – instead, we present and report only our final choice as if there was no chance other candidate models would be as good or even better fits.\nEnsemble models prove a way to deal with that uncertainty (Wintle et al. 2003). By averaging predictions from a handful of candidate models, ensembles acknowledge that there might be multiple models that could be used to describe our data – and by weighting the average we can communicate how confident we are in each individual model’s view of the world. Of course, while this is all nice and flowery, it needs to work too – and model averaging delivers, typically reducing prediction errors beyond even above even the best individual component model (Dormann et al. 2018).\nThere are a ton of approaches to model averaging1. The rest of this post will walk through a few of the simplest – equal-weight averaging, fit-based averages, and model-based combinations – that you can easily implement yourself without needing to worry about slowing down your iteration time or making your modeling code too complex.\n\nGetting Started\nWe’ll be using the following libraries for data manipulation and visualization:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\nAdditionally, we’ll be using both ranger and lightgbm to develop component models:\n\nlibrary(ranger)\nlibrary(lightgbm)\n\n\nAttaching package: 'lightgbm'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nAnd finally, we need the actual data we’re modeling. For this example, we’ll build models predicting the arrival delay of the flights included in the nycflights13 package using both flight details and weather data. This next chunk of code will preprocess our data into a model-ready format:\n\nflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather %&gt;% \n  select(-wind_gust) # About 80% missing values, so we'll drop this column\n\n# combine the two data frames into one complete set\nflight_data &lt;- flights %&gt;% \n  left_join(weather,\n            by = c(\"year\", \"month\", \"day\", \"origin\", \"hour\", \"time_hour\")) %&gt;% \n  drop_na()\n\nflight_data &lt;- flight_data %&gt;% \n  # Drop 37 pretty dramatic outliers\n  filter(arr_delay &lt;= 500) %&gt;% \n  # Get rid of useless predictors -- \n  # these each cause problems with at least one of our regressions\n  select(-year, -time_hour, -minute) %&gt;% \n  # Skip the work of encoding non-numeric values, to save my poor laptop\n  select_if(is.numeric)\n\nAnd for one final pre-processing step, we’ll split our data into training, validation, and testing sets (sticking 20% into both validation and testing and dumping the rest into training). We’ll be using model performance against the validation set to determine weights for our averages.\n\nset.seed(123)\n# Generate a random sequence to subset our data into train/validate/test splits\nrow_index &lt;- sample(nrow(flight_data), nrow(flight_data))\n\n# Testing gets the 20% of data with the highest random index values\nflight_testing &lt;- flight_data[row_index &gt;= nrow(flight_data) * 0.8, ]\n\n# Validation gets the next highest 20%\nflight_validation &lt;- flight_data[row_index &gt;= nrow(flight_data) * 0.6 &\n                                   row_index &lt; nrow(flight_data) * 0.8, ]\n\n# Training gets the rest\nflight_training &lt;- flight_data[row_index &lt; nrow(flight_data) * 0.6, ]\n\n# LightGBM requires matrices, rather than data frames and formulas:\nxtrain &lt;- as.matrix(select(flight_training, -arr_delay))\nytrain &lt;- as.matrix(flight_training[[\"arr_delay\"]])\n\nxvalid &lt;- as.matrix(select(flight_validation, -arr_delay))\nxtest &lt;- as.matrix(select(flight_testing, -arr_delay))\n\nSo with that out of the way, it’s time to start training our models!"
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#component-models",
    "href": "posts/2021/01/model-averaging/index.html#component-models",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Component Models",
    "text": "Component Models\n\nLinear Model\nLet’s start off with a simple linear regression model, using all of our predictors in the flight dataset to try and estimate arrival delays:\n\nlinear_model &lt;- lm(arr_delay ~ ., flight_training)\nsummary(linear_model)\n\n\nCall:\nlm(formula = arr_delay ~ ., data = flight_training)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.895  -9.133  -1.538   7.076 159.388 \n\nCoefficients:\n                  Estimate  Std. Error  t value           Pr(&gt;|t|)    \n(Intercept)    -4.61324673  5.99244467   -0.770           0.441394    \nmonth           0.03825040  0.01117238    3.424           0.000618 ***\nday             0.02220492  0.00410860    5.404 0.0000000650770797 ***\ndep_time        0.00009509  0.00027953    0.340           0.733722    \nsched_dep_time -0.00349249  0.00189448   -1.844           0.065257 .  \ndep_delay       1.01251264  0.00106768  948.332            &lt; 2e-16 ***\narr_time        0.00088164  0.00011818    7.460 0.0000000000000868 ***\nsched_arr_time -0.00471343  0.00014783  -31.884            &lt; 2e-16 ***\nflight         -0.00004692  0.00002541   -1.846           0.064863 .  \nair_time        0.75629859  0.00307431  246.006            &lt; 2e-16 ***\ndistance       -0.09791613  0.00039245 -249.500            &lt; 2e-16 ***\nhour            0.59997173  0.18707035    3.207           0.001341 ** \ntemp            0.11726625  0.02231781    5.254 0.0000001487014873 ***\ndewp            0.03632142  0.02404661    1.510           0.130928    \nhumid           0.01860018  0.01228626    1.514           0.130053    \nwind_dir       -0.00607627  0.00040085  -15.158            &lt; 2e-16 ***\nwind_speed      0.19198999  0.00753768   25.471            &lt; 2e-16 ***\nprecip         26.88470146  3.01386317    8.920            &lt; 2e-16 ***\npressure       -0.01634187  0.00561852   -2.909           0.003631 ** \nvisib          -0.46031686  0.03238825  -14.212            &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.82 on 170687 degrees of freedom\nMultiple R-squared:  0.8707,    Adjusted R-squared:  0.8707 \nF-statistic: 6.048e+04 on 19 and 170687 DF,  p-value: &lt; 2.2e-16\n\n\nCool! We have our first model – and it seems to be a pretty ok fit, with an R^2 of 0.87. We could probably make this model better by being a bit more selective with our terms or throwing in some interaction terms – but as a first stab at a model that we’ll incorporate into our average, this is pretty alright.\nOf course, we want to make sure this model can generalize outside of the data it was trained with – let’s use it to make predictions against our validation set, too:\n\nflight_validation$lm_pred &lt;- predict(\n  linear_model,\n  newdata = flight_validation\n)\n\nsqrt(mean((flight_validation$lm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lm_pred, flight_validation))$r.squared\n\n\n\n[1] 14.73962\n\n\n[1] 0.8684178\n\n\nR^2 remains at about 0.87 and RMSE comes in at about 14.74 minutes – which, considering delays in the validation set range from -75 to 485 minutes, feels not too bad for a naively implemented linear model.\n\n\nRandom Forest\nSo we have our first model sorted, but we need more than that to take an average! While we could average out a number of linear models with different parameters, it feels more interesting to combine a few different algorithms as component models. So let’s use ranger to implement a random forest to represent our data – fair warning, this one takes a little while to train!\n\nranger_model &lt;- ranger::ranger(arr_delay ~ ., data = flight_training)\nsqrt(ranger_model$prediction.error)\nranger_model$r.squared\n\n\n\n[1] 11.08573\n\n\n[1] 0.9276561\n\n\nSo this model has an RMSE of 11.09 and an R^2 of 0.93 – an improvement over our linear model! While we could eke out some improvements with careful tuning, it looks like this version is a good enough fit to use as an example in our ensemble. As before, we want to check out how well this model generalizes by using it to generate predictions for our validation set:\n\nranger_predictions &lt;- predict(\n    ranger_model,\n    data = flight_validation\n  )\n\nflight_validation$ranger_pred &lt;- ranger_predictions$predictions\n\nsqrt(mean((flight_validation$ranger_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ ranger_pred, flight_validation))$r.squared\n\n\n\n[1] 10.96209\n\n\n[1] 0.9302306\n\n\nOur model actually performs (extremely) slightly better on the validation set!\n\n\nGBM\nSo that’s two models sorted! For completeness sake, let’s implement a third and final component model, this time using the LightGBM package to fit a gradient boosting machine. Similar to the last two, we won’t do a ton to parameterize this model – the only change I’ll make to the model fit defaults is to use 100 rounds, to let the boosting algorithm get into the same performance range as our other two models.\n\nlightgbm_model &lt;- lightgbm::lightgbm(xtrain, \n                                     ytrain, \n                                     nrounds = 100, \n                                     obj = \"regression\", \n                                     metric = \"rmse\",\n                                     # Suppress output\n                                     force_col_wise = TRUE,\n                                     verbose = 0L)\n\nThe lightgbm_model doesn’t have the same easy method for evaluating in-bag performance as our linear model and random forests did. We’ll skip right to the validation set instead:\n\nflight_validation$lightgbm_pred &lt;- predict(\n  lightgbm_model,\n  xvalid\n)\n\nsqrt(mean((flight_validation$lightgbm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lightgbm_pred, flight_validation))$r.squared\n\n\n\n[1] 10.4088\n\n\n[1] 0.9347398\n\n\nSo it looks like LightGBM model performs about as well (if not marginally better than) our random forest! For reference, here are the RMSE values from each of our candidate models:\n\nprediction_values &lt;- flight_validation %&gt;% \n  # Only select our y and y-hat columns\n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots &lt;- prediction_values %&gt;% \n  pivot_longer(cols = -arr_delay) %&gt;% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, levels = c(\"lightgbm\", \"ranger\", \"lm\")))\n\nprediction_plots %&gt;% \n  group_by(Model = name) %&gt;% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %&gt;% \n  arrange(RMSE) %&gt;% \n  knitr::kable()\n\n\n\n\nModel\nRMSE\n\n\n\n\nlightgbm\n10.40880\n\n\nranger\n10.96209\n\n\nlm\n14.73962\n\n\n\n\n\nOf course, individual metrics don’t tell the whole story – it can be helpful to look at diagnostic plots of our predictions to try and understand patterns in how our predictions match the data. For instance, “linear models are about four minutes worse on average” is all well and good in the abstract, but graphics like the one below can help us see that – for instance – linear models tend to do a bit worse around 0 minute delays (where most of the data is clustered) while our random forest performs worse on higher extremes:\n\nprediction_plots %&gt;% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")"
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#model-averaging",
    "href": "posts/2021/01/model-averaging/index.html#model-averaging",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Model Averaging",
    "text": "Model Averaging\nWith our candidate models in tow, we’re now fully ready to move on to model averaging methods! We’ll walk through basic implementations of three methods (equal weighting, fit-based weights, and model-based estimates) and then evaluate our ensembles at the end.\n\nEqual Weights\nPerhaps the most obvious way to average models is to take the simple arithmetic mean of your model predictions. This method presupposes that each of your models are equally good representations of your underlying data; since that isn’t the case here, we might expect this method to not substantially reduce error overall.\nA benefit of this method, though, is that implementation takes no time at all:\n\nprediction_values &lt;- prediction_values %&gt;% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\n\n\nFit-Based Weights\nA slightly more involved method is to weight models based on some metric of their performance. Almost any metric with a standard definition across component models can be used (so for instance, AIC or BIC with nested models or MSE and MAPE); as we’ve been using RMSE so far, we’ll use it to weight our errors.\nWeighting models based on fit statistics is also relatively easy in the grand scheme of data science. First, calculate the fit statistic for each of your models:\n\nmodel_rmse &lt;- vapply(\n  prediction_values,\n  function(x) sqrt(mean((x - prediction_values$arr_delay)^2)),\n  numeric(1)\n  )[1:3] # Only our 3 component models!\nmodel_rmse\n\n      lm_pred   ranger_pred lightgbm_pred \n     14.73962      10.96209      10.40880 \n\n\nThen, depending on your statistic, you may need to take the reciprocal of each value – as lower RMSEs are better, we need to do so here:\n\nrmse_weights &lt;- (1 / (model_rmse))\n\nLastly, calculate your weights as proportion of the whole set of – you can view these values as the proportion of the ensemble prediction contributed by each component:\n\nrmse_weights &lt;- rmse_weights / sum(rmse_weights)\nrmse_weights\n\n      lm_pred   ranger_pred lightgbm_pred \n    0.2659099     0.3575422     0.3765479 \n\n\nMaking predictions with the ensemble is then relatively easy – just multiply each of your predicted values by their proportion and sum the results:\n\nprediction_values &lt;- prediction_values %&gt;% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\n\n\nModel-Based Weights\nThe last averaging method we’ll walk through is a little more involved, but still pretty comprehensible: take your model outputs, turn around, and use them as model inputs.\n\nOur toy example here is a pretty good fit for this method – we already saw in our graphics that a strong linear relationship exists between our predictions and the true value, and this relationship is a little different for each model:\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFrom this plot, we can guess that a linear model combining our component predictions as features will be a good fit2 for averaging these models. To do so, we simply need to build a linear model:\n\npredictions_model &lt;- lm(arr_delay ~ lm_pred * ranger_pred * lightgbm_pred, \n                        data = prediction_values)\n\nAnd then use it to generate predictions just like our original component linear model:\n\nprediction_values$model_based_pred &lt;- predict(\n  predictions_model,\n  newdata = prediction_values\n)\n\nNote that if we saw non-linear relationships between our predictions and true values, we’d want to rely on non-linear methods to average out predictions; it just so happens that our models are already pretty strong fits for the underlying data and can be well-represented with simple linear regression."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#howd-we-do",
    "href": "posts/2021/01/model-averaging/index.html#howd-we-do",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "How’d We Do?",
    "text": "How’d We Do?\nNow that we have our ensemble models prepared, it’s time to evaluate all of our models out against our testing set!\nThe first step is to generate predictions for the test set using our component models:\n\nflight_testing$lm_pred &lt;- predict(\n  linear_model,\n  newdata = flight_testing\n)\n\nranger_predictions &lt;- predict(\n    ranger_model,\n    data = flight_testing\n  )\n\nflight_testing$ranger_pred &lt;- ranger_predictions$predictions\n\nflight_testing$lightgbm_pred &lt;- predict(\n  lightgbm_model,\n  xtest\n)\n\nWe can use those predictions to generate our ensemble predictions. Note that we’re still using the weights and models calibrated on the validation data – we (theoretically) shouldn’t know the “true” values for the test set, so we can’t re-weight our averages now!\n\nflight_testing &lt;- flight_testing %&gt;% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\nflight_testing &lt;- flight_testing %&gt;% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\nflight_testing$model_based_pred &lt;- predict(\n  predictions_model,\n  newdata = flight_testing\n)\n\nSo how’d we do? Let’s check out the RMSE for each of our models:\n\nprediction_values &lt;- flight_testing %&gt;% \n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots &lt;- prediction_values %&gt;% \n  pivot_longer(cols = -arr_delay) %&gt;% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, \n                       levels = c(\"lightgbm\", \"ranger\", \"lm\",\n                                  \"model_based\", \"fit_based\", \"equal_weight\")))\n\nprediction_plots %&gt;% \n  group_by(Model = name) %&gt;% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %&gt;% \n  arrange(RMSE) %&gt;% \n  knitr::kable()\n\n\n\n\nModel\nRMSE\n\n\n\n\nmodel_based\n9.492409\n\n\nlightgbm\n10.290113\n\n\nranger\n10.968544\n\n\nfit_based\n11.057728\n\n\nequal_weight\n11.311836\n\n\nlm\n14.621943\n\n\n\n\n\n\nprediction_plots %&gt;% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\nCool – our model-based ensemble actually performed better than any of the components! While the equal weight and fit-based averages were pretty middle-of-the-road, in other settings these methods can also help to reduce bias in predictions and produce estimates with less variance than any of the component models."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#conclusion",
    "href": "posts/2021/01/model-averaging/index.html#conclusion",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Conclusion",
    "text": "Conclusion\nModel averaging can be a powerful tool for reducing model bias and addressing the implicit uncertainty in attempting to pick the “best” model for a situation. While plenty of complex and computationally expensive approaches to averaging exist – and can greatly improve model performance – simpler ensemble methods can provide the same benefits without necessarily incurring the same costs."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#footnotes",
    "href": "posts/2021/01/model-averaging/index.html#footnotes",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee table 1 of Dormann et. al. for a partial list.↩︎\nNo pun intended.↩︎"
  },
  {
    "objectID": "posts/2023-01-17-spatialsample/index.html",
    "href": "posts/2023-01-17-spatialsample/index.html",
    "title": "spatialsample 0.3.0 is now on CRAN",
    "section": "",
    "text": "Photo by Nik Shuliahin 💛💙 on Unsplash\nA new release of spatialsample – an R package extending the rsample framework to handle spatial resampling and cross-validation – just landed on CRAN. This post will describe the new breaking changes to the spatial_clustering_cv() function, as well as improvements made to how spatialsample handles repeated cross-validation, as well as improvements made to how spatialsample handles repeated cross-validation; for a full list of changes, check out this release’s NEWS entry\nYou can install the package from CRAN via:"
  },
  {
    "objectID": "posts/2023-01-17-spatialsample/index.html#breaking-changes-to-spatial_clustering_cv",
    "href": "posts/2023-01-17-spatialsample/index.html#breaking-changes-to-spatial_clustering_cv",
    "title": "spatialsample 0.3.0 is now on CRAN",
    "section": "Breaking changes to spatial_clustering_cv()",
    "text": "Breaking changes to spatial_clustering_cv()\nThe only (intentional!) breaking changes in this version are to the function spatial_clustering_cv(). This function is the oldest part of the package, and as such didn’t quite work like any of the other parts of the package: distance calculations sometimes used dist() instead of sf::st_distance(), distances used centroids instead of polygon edges, and trying to adapt this function to work with both sf and non-spatial data had made the internals a bit hairy.\nAs of rsample 1.1.1, all of those non-spatial elements have been outsourced to rsample::clustering_cv(). If you’ve been using spatial_clustering_cv() for non-spatial data, this function is pretty close to a drop-in replacement, and you should migrate code over.\nFor spatial point data, spatial_clustering_cv() should work identically to past versions. For other geometry types however, note that distance calculations have now changed to use edge-to-edge distance between geometries, rather than centroids as in past versions. This means this function now uses distances in a way that’s more consistent with the rest of the package, and more consistent with what I personally view as best practice; two polygons sharing a lot of perimeter but with centroids separated by a decent margin are still likely to be highly similar.\n\nlibrary(spatialsample)\nspatial_clustering_cv(boston_canopy, 2)\n\n#  2-fold spatial cross-validation \n# A tibble: 2 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [277/405]&gt; Fold1\n2 &lt;split [405/277]&gt; Fold2"
  },
  {
    "objectID": "posts/2023-01-17-spatialsample/index.html#repetition-repetition-repetition",
    "href": "posts/2023-01-17-spatialsample/index.html#repetition-repetition-repetition",
    "title": "spatialsample 0.3.0 is now on CRAN",
    "section": "Repetition, repetition, repetition1",
    "text": "Repetition, repetition, repetition1\nI think the most exciting part of this release is that all spatialsample functions can now handle cross-validation with repeats. Simply pass your desired number of repeats to the repeats argument of any function.2 The autoplot() function will now also automatically detect when you’ve used repeated cross-validation, and facet plots accordingly:\n\nboston_canopy |&gt; \n  spatial_block_cv(v = 5, repeats = 4) |&gt; \n  autoplot()"
  },
  {
    "objectID": "posts/2023-01-17-spatialsample/index.html#and-more",
    "href": "posts/2023-01-17-spatialsample/index.html#and-more",
    "title": "spatialsample 0.3.0 is now on CRAN",
    "section": "…and more!",
    "text": "…and more!\nThis is just scratching the surface of the new features and improvements in this release of spatialsample. You can see a full list in this release’s NEWS entry."
  },
  {
    "objectID": "posts/2023-01-17-spatialsample/index.html#repetition",
    "href": "posts/2023-01-17-spatialsample/index.html#repetition",
    "title": "spatialsample 0.3.0 is now on CRAN",
    "section": "Repetition!",
    "text": "Repetition!"
  },
  {
    "objectID": "posts/2023-01-17-spatialsample/index.html#footnotes",
    "href": "posts/2023-01-17-spatialsample/index.html#footnotes",
    "title": "spatialsample 0.3.0 is now on CRAN",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis joke I keep making is stolen from an old Kurt Braunohler video. Or potentially, this other Kurt Braunohler video. Or potentially, that first Kurt Braunohler video.↩︎\nThough note that spatialsample should error if repeats would return the same exact folds, such as when using spatial_leave_location_out_cv() with v = NULL. This behavior might change in the future if anyone complains, but for the modeling projects I’ve worked on, you’d only repeat identical CV folds by accident.↩︎"
  },
  {
    "objectID": "posts/2019/03/index.html",
    "href": "posts/2019/03/index.html",
    "title": "Thesis Now Available in ESF Digital Commons",
    "section": "",
    "text": "My thesis is now available on the ESF Digital Commons! I’m extremely grateful to Drs. John Drake and Bill Shields for their help in the revision and submission process, and of course to Dr. John Stella for the extensive support he provided throughout the project, from conceptualization to publication.\nIn the thesis, we look at the impacts beavers have on the forest community around them as they remove trees for food and building dams. While people had looked at these impacts in other parts of beaver’s range, the Adirondacks are a strange enough ecosystem - being largely protected from anthropogenic disturbances, most of the forest landscape exhibits only one or two age classes - that we weren’t sure how applicable conclusions from these regions would be. What we found was that while the broad conclusions of these studies held true - beavers still operate as central place foragers and create large disturbances in the landscape - the lack of early-successional species throughout the Adirondacks seriously shifted which stems were harvested preferrentially. We also found a lot of variance in the patterns of how individual species were utilized - for instance, beaver harvested almost any size speckled alder they could find, so long as it was close to their dam, but would harvest red maple at any distance, so long as the stem was small.\n\nWe’re currently working on a journal article version of the thesis, using an expanded dataset and focusing more closely on the patterns in forage selectivity we found, and how they differ from other regions. That should hopefully be in the review process within the next few weeks."
  },
  {
    "objectID": "posts/2022-11-07-agb/index.html",
    "href": "posts/2022-11-07-agb/index.html",
    "title": "Fine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages",
    "section": "",
    "text": "I’m part of the team behind a new open-access article, “Fine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages”, newly out in International Journal of Applied Earth Observation and Geoinformation.1 The paper was lead by my incredible coworker Lucas Johnson, and is his first first-author paper – and what a powerful way to enter the field!\nThis paper is, plain and simple, the first public documentation we’ve released about how we’re helping New York State track how much carbon is stored in its forests statewide. Using a patchwork of LiDAR collections and FIA field measurements, we use a stacked ensemble (a linear regression combining a random forest, LightGBM-based gradient boosting machine, and support vector machine) to estimate forest aboveground biomass (all the woody non-root bits of a tree, which2 are made out of carbon pulled out of the atmosphere) at a 30 meter resolution across New York State. This data is already being used to identify high-priority areas across the state where the most carbon is currently stored and where a- and re-forestation might have the greatest impacts in the future.\nThis is very exciting in and of itself, but I’m also extremely proud of the effort we put into validating our predictions, to make sure we’re speaking only with exactly as much confidence as we deserve. We use the area of applicability from Meyer and Pebesma (2021) to mask out areas where LiDAR data is too “different” from what our models were trained with, in order to make sure we aren’t accidentally extrapolating into areas our models aren’t prepared to handle. We implement the Riemann et al. (2010) multi-scale assessment procedure to make sure our predictions are useful when aggregated to other scales. We look at our error spatially and as a function of FIA-measured AGB to try and fully characterize where our models succeed and where they need a bit more work. The goal is for our maps to be useful, and not just “pretty pictures”.\nIn addition to Lucas (who, I cannot stress enough, did a fantastic job leading this paper), I was lucky enough to get to work with Eddie Bevilacqua, Steve Stehman, Grant Domke, and Colin Beier on this one. It’s a great team and a great project, and I’m very excited for the things we’ve got coming down the pipeline next."
  },
  {
    "objectID": "posts/2022-11-07-agb/index.html#footnotes",
    "href": "posts/2022-11-07-agb/index.html#footnotes",
    "title": "Fine-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve needed to google this journal name every time I go to post about the article. I’ve read papers from this journal before and generally like their output, but for some reason the acronym IJAEOG just refuses to stick in my mind.↩︎\nTo simplify quite a bit.↩︎"
  },
  {
    "objectID": "posts/2022-12-01-sf-in-packages/index.html",
    "href": "posts/2022-12-01-sf-in-packages/index.html",
    "title": "How to include sf data in R packages",
    "section": "",
    "text": "Store your sf objects as internal data. Add a .R file to data/ (probably named your_data.R) containing the following:\ndelayedAssign(\"your_data\", local({\n  requireNamespace(\"sf\", quietly = TRUE)\n  your_package:::your_data\n}))\nDocument your data as usual.\nSee the update at the bottom of this post for a bit more information."
  },
  {
    "objectID": "posts/2022-12-01-sf-in-packages/index.html#tldr",
    "href": "posts/2022-12-01-sf-in-packages/index.html#tldr",
    "title": "How to include sf data in R packages",
    "section": "",
    "text": "Store your sf objects as internal data. Add a .R file to data/ (probably named your_data.R) containing the following:\ndelayedAssign(\"your_data\", local({\n  requireNamespace(\"sf\", quietly = TRUE)\n  your_package:::your_data\n}))\nDocument your data as usual.\nSee the update at the bottom of this post for a bit more information."
  },
  {
    "objectID": "posts/2022-12-01-sf-in-packages/index.html#the-problem",
    "href": "posts/2022-12-01-sf-in-packages/index.html#the-problem",
    "title": "How to include sf data in R packages",
    "section": "The Problem",
    "text": "The Problem\nIf you work with spatial data in R, you’re familiar with the sf package. If you write spatially-oriented packages for R, there’s a good chance you think a lot about how to handle and work with sf objects. I think that’s a good thing; sf makes data analysis with spatial data miles easier, and I rely on it throughout my packages.\n\n\n\nCC-BY 4.0 Artwork by Allison Horst.\n\n\nIf you write spatially-oriented packages, there’s a good chance your examples or tests could benefit from your package including some sf objects as demonstration data. But there’s a few weird edge cases that crop up when including sf objects in package data.\nFor instance, let’s look at the guerry data from the geodaData package. On its own, this object seems to load and print perfectly fine:\n\ngeodaData::guerry |&gt; head(1)\n\n  CODE_DE COUNT AVE_ID_ dept Region Dprtmnt Crm_prs Crm_prp Litercy Donatns\n1      01     1      49    1      E     Ain   28870   15890      37    5098\n  Infants Suicids MainCty Wealth Commerc Clergy Crm_prn Infntcd Dntn_cl Lottery\n1   33120   35039       2     73      58     11      71      60      69      41\n  Desertn Instrct Prsttts Distanc Area Pop1831\n1      55      46      13 218.372 5762  346.03\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          geometry\n1 801150, 800669, 800688, 800780, 800589, 800333, 799095, 799136, 799690, 799329, 797715, 797655, 797410, 797149, 796755, 796738, 796612, 795234, 794905, 794547, 791555, 791465, 791935, 791835, 791670, 790205, 790035, 789304, 788617, 788005, 787670, 786705, 786350, 785720, 785562, 786670, 786930, 787055, 787208, 787367, 786900, 786795, 786830, 786811, 786384, 786550, 786560, 786465, 785956, 787211, 787415, 787600, 787536, 787137, 786610, 786457, 787138, 787365, 787545, 787880, 788198, 788225, 789130, 789370, 789790, 789995, 790445, 790237, 789585, 788993, 788830, 788650, 788638, 788673, 789501, 789510, 789596, 789680, 789845, 790050, 790594, 790750, 790740, 790739, 791139, 791749, 791860, 792199, 792285, 792880, 793615, 793737, 793650, 793618, 793480, 794012, 794347, 794535, 794686, 795135, 796115, 796193, 796250, 796311, 796335, 796324, 796316, 796385, 796495, 797830, 797970, 797959, 797910, 797905, 798850, 798930, 798980, 799122, 799255, 799340, 800295, 800326, 800757, 801505, 802043, 802820, 804846, 805165, 805338, 805485, 808352, 808655, 809605, 809930, 811710, 813076, 814292, 814670, 815189, 816978, 817045, 818290, 819798, 819990, 819753, 820496, 820305, 820810, 821023, 821020, 821753, 822453, 822690, 823263, 823585, 824250, 825284, 825600, 828440, 829372, 828470, 828705, 828395, 827745, 828485, 830445, 831525, 832747, 833030, 833286, 833885, 833687, 832940, 832830, 833866, 833825, 833752, 833675, 835995, 835930, 836194, 836185, 836190, 836405, 836690, 837059, 837545, 838628, 838735, 840056, 840530, 840855, 841371, 841670, 841591, 841093, 840525, 840483, 841576, 841898, 843750, 843973, 844372, 844675, 846805, 848292, 848440, 848651, 850029, 850390, 851184, 851650, 851857, 851710, 852195, 853883, 854050, 854684, 854950, 855010, 854706, 856495, 857425, 857675, 857795, 860115, 860522, 860356, 860868, 861160, 861499, 861800, 863931, 864270, 867954, 868320, 868625, 868940, 870819, 871569, 872099, 872230, 873891, 874080, 875215, 875688, 875836, 876350, 877249, 877575, 877751, 879950, 880577, 882757, 884049, 885600, 886165, 886435, 889069, 889778, 890867, 893000, 894101, 894575, 894622, 893375, 892712, 892540, 891600, 891281, 891245, 891340, 890292, 890080, 890987, 891235, 891560, 891794, 891337, 890413, 889185, 888822, 887853, 887180, 886092, 885735, 885432, 884760, 883760, 883455, 882749, 882058, 881989, 880591, 880422, 879890, 879855, 882197, 882019, 881408, 880810, 880202, 880177, 879600, 879351, 878142, 877449, 876853, 875845, 875089, 874800, 874322, 874925, 874725, 872900, 872100, 871837, 870648, 869972, 869296, 869435, 868710, 868663, 868525, 868760, 868985, 868525, 868617, 868760, 868864, 869034, 868545, 868661, 868975, 868911, 869020, 869215, 870970, 870979, 870925, 871130, 870721, 870455, 870800, 870344, 870311, 870897, 870710, 870615, 869975, 869465, 869314, 869040, 868989, 868710, 868050, 867940, 867995, 867975, 867825, 867736, 867995, 867979, 867788, 868170, 867935, 867643, 867110, 867246, 867588, 866723, 866013, 865294, 864275, 864000, 863659, 861885, 861845, 862185, 862398, 862694, 862533, 860925, 860842, 861160, 861300, 861210, 860973, 860337, 858511, 858205, 858072, 857453, 857320, 857265, 856650, 856355, 856182, 855610, 854744, 854474, 852864, 852620, 852647, 851685, 851354, 850753, 850470, 850320, 850069, 850023, 850741, 851509, 852157, 852073, 850451, 850375, 850111, 849805, 848375, 848150, 847545, 847281, 845980, 845694, 844840, 842895, 842098, 841825, 841604, 841170, 840895, 839825, 839769, 840798, 840745, 840580, 840305, 839320, 839165, 839016, 838200, 836342, 836100, 835840, 834252, 833175, 833017, 832785, 832719, 832433, 831189, 830330, 829565, 829550, 829422, 828638, 827810, 827523, 826955, 826653, 825785, 825487, 824587, 822700, 822339, 821748, 821660, 821459, 820815, 820470, 819281, 818215, 817851, 816405, 815735, 815844, 815501, 814830, 814502, 814175, 813785, 811510, 811202, 810503, 809935, 808612, 808305, 807965, 807085, 806759, 806450, 805480, 804700, 804426, 803230, 802864, 801150, 2092615, 2093190, 2095430, 2095795, 2096112, 2097190, 2098505, 2098838, 2100360, 2100395, 2100950, 2101330, 2102850, 2102788, 2102425, 2102785, 2103487, 2103892, 2104515, 2104359, 2102570, 2102760, 2104075, 2105023, 2105295, 2105555, 2105875, 2107116, 2107148, 2106760, 2106708, 2106990, 2107029, 2107335, 2108399, 2109486, 2109775, 2110010, 2110344, 2111056, 2113550, 2114260, 2114370, 2114751, 2116197, 2116940, 2117563, 2117860, 2119206, 2120474, 2120775, 2121475, 2121809, 2122743, 2123110, 2124100, 2124890, 2125160, 2125377, 2125830, 2126745, 2127075, 2128290, 2128585, 2129209, 2129525, 2131460, 2131793, 2132230, 2132742, 2133100, 2133655, 2134028, 2135142, 2136382, 2137110, 2138174, 2138520, 2139015, 2139590, 2140547, 2142011, 2142380, 2142716, 2143599, 2144456, 2144790, 2147049, 2147420, 2148815, 2150425, 2150782, 2152285, 2152660, 2153400, 2155103, 2155696, 2155980, 2156241, 2157025, 2159030, 2160314, 2160635, 2161408, 2161795, 2162102, 2163021, 2163470, 2163785, 2165380, 2165700, 2167362, 2167695, 2167835, 2169845, 2170117, 2170395, 2171035, 2171335, 2171435, 2171593, 2170924, 2170498, 2170754, 2171611, 2171760, 2171217, 2171380, 2170485, 2170210, 2168729, 2168520, 2168719, 2168735, 2170150, 2169640, 2170568, 2170640, 2171442, 2171075, 2172530, 2171719, 2171534, 2171205, 2170948, 2169812, 2169145, 2168648, 2167568, 2167190, 2167185, 2166355, 2166075, 2166394, 2166285, 2165476, 2165330, 2165160, 2165065, 2163119, 2162490, 2161655, 2161531, 2161519, 2161005, 2159908, 2159915, 2158958, 2158780, 2158494, 2158105, 2157807, 2157040, 2156696, 2155806, 2155445, 2155097, 2154750, 2153047, 2152675, 2151171, 2150780, 2150680, 2150149, 2150070, 2153840, 2154400, 2151187, 2150820, 2151192, 2151708, 2151840, 2151081, 2150965, 2150606, 2148479, 2148155, 2146631, 2145603, 2145394, 2145800, 2146033, 2145523, 2145410, 2146002, 2147688, 2148035, 2148733, 2148841, 2148775, 2149407, 2150710, 2150922, 2151180, 2152638, 2153465, 2153815, 2154550, 2153948, 2153605, 2152546, 2152522, 2151954, 2151680, 2151135, 2150917, 2149401, 2147845, 2145955, 2145690, 2145842, 2146060, 2146529, 2146675, 2146311, 2146245, 2146162, 2146145, 2146227, 2147047, 2147009, 2146655, 2148801, 2149145, 2148900, 2149908, 2151749, 2152270, 2151866, 2151825, 2152160, 2156500, 2157904, 2159490, 2160898, 2163565, 2164270, 2164140, 2163415, 2162493, 2162099, 2160395, 2160083, 2159465, 2158743, 2156340, 2155944, 2155185, 2153375, 2152677, 2152295, 2150917, 2149984, 2149695, 2147869, 2147615, 2147405, 2146272, 2145258, 2144570, 2145445, 2145356, 2144792, 2145110, 2143618, 2143730, 2143963, 2143997, 2143495, 2143280, 2142419, 2142149, 2141782, 2141448, 2140052, 2139555, 2139435, 2138192, 2137078, 2136753, 2134940, 2133632, 2132631, 2132205, 2132001, 2131755, 2131650, 2131962, 2130970, 2130628, 2130520, 2129464, 2128114, 2126982, 2126720, 2126810, 2127080, 2128513, 2128829, 2128495, 2127050, 2125650, 2125275, 2123780, 2123569, 2123070, 2121885, 2121591, 2120685, 2120349, 2119679, 2118760, 2118458, 2117935, 2117610, 2116295, 2115554, 2114005, 2113621, 2112855, 2111817, 2110485, 2110250, 2110165, 2109573, 2108843, 2107500, 2106390, 2106018, 2103410, 2101545, 2101232, 2099875, 2099512, 2098080, 2097320, 2096964, 2094425, 2094056, 2092595, 2092277, 2091670, 2091277, 2090131, 2088645, 2088380, 2088330, 2088110, 2087753, 2086334, 2085607, 2084278, 2084187, 2084575, 2084465, 2084328, 2084450, 2084109, 2083150, 2082820, 2082092, 2081333, 2079632, 2078870, 2078155, 2077808, 2077080, 2076787, 2076386, 2076155, 2075110, 2074811, 2074591, 2073960, 2073584, 2073221, 2073460, 2073797, 2074780, 2075941, 2077758, 2078730, 2079020, 2079386, 2079920, 2080021, 2079717, 2079917, 2080840, 2081131, 2082613, 2082507, 2081230, 2081554, 2082291, 2083206, 2083940, 2084174, 2084350, 2084500, 2084810, 2086215, 2086449, 2087620, 2087856, 2088564, 2091270, 2091898, 2092105, 2092424, 2093065, 2093318, 2094355, 2096198, 2096536, 2096910, 2097185, 2097470, 2098117, 2098880, 2099191, 2099825, 2100899, 2101165, 2101453, 2102554, 2102260, 2102031, 2101525, 2101162, 2100482, 2099710, 2098550, 2095140, 2094745, 2094389, 2093577, 2091920, 2091724, 2091325, 2091135, 2090510, 2090257, 2089517, 2089735, 2089692, 2090044, 2090765, 2091109, 2092110, 2092273, 2093148, 2093260, 2093406, 2093980, 2093715, 2092629, 2092497, 2094120, 2094025, 2093925, 2093907, 2093620, 2093793, 2093796, 2093390, 2093210, 2093030, 2092976, 2093490, 2093370, 2093210, 2092960, 2093105, 2093280, 2093210, 2093269, 2092615\n\n\nBut if we cast this object to a tibble, we get an error:\n\ntry(tibble::as_tibble(geodaData::guerry))\n\nError : All columns in a tibble must be vectors.\n✖ Column `geometry` is a `sfc_MULTIPOLYGON/sfc` object.\n\n\nThis is a known bug that packages need to work around themselves, and can be a bit of a pain to figure out solutions for. It tends to crop up when data.frame objects are implicitly casted to tibbles, for instance by dplyr functions:\n\ntry(dplyr::group_by(geodaData::guerry, CODE_DE))\n\nError : All columns in a tibble must be vectors.\n✖ Column `geometry` is a `sfc_MULTIPOLYGON/sfc` object.\n\n\nBut it can happen without casting to tibbles as well, for instance when trying to use dplyr::arrange():1\n\ntry(dplyr::arrange(geodaData::guerry))\n\nError in vec_size() : \n  `x` must be a vector, not a &lt;sfc_MULTIPOLYGON/sfc&gt; object.\n\n\nAny code that calls vctrs::vec_size() will wind up erroring in this situation, which includes a good amount of tidyverse code.\nA final challenge with including sf objects in R packages is that some projected CRS include non-ASCII characters in their WKT, causing an aggravating warning in R CMD check: Warning: found non-ASCII string. Your package won’t be accepted to CRAN with that warning, and as such, this needs to be fixed.2\nAll three of these problems can be solved by the same approach, which I think is the best way to include sf objects in packages."
  },
  {
    "objectID": "posts/2022-12-01-sf-in-packages/index.html#the-solution",
    "href": "posts/2022-12-01-sf-in-packages/index.html#the-solution",
    "title": "How to include sf data in R packages",
    "section": "The Solution",
    "text": "The Solution\nRather than including sf objects directly as package data in the normal manner, you should store the sf object as internal data and then load it via delayedAssign. This is an approach inspired by palmerpenguins (here’s the relevant PR), now in use in the spatialsample and waywiser packages I maintain.\nTo store your sf objects as internal data, save them into a file named R/sysdata.rda. If you use usethis::use_data(), set the argument internal = TRUE to make this happen automatically. This solves the third problem for us; something about not exporting the sf object directly means that R CMD check no longer checks the CRS for non-ASCII characters, and the warning is no longer triggered.\nYou now need to somehow export this data, in order to let users actually use your newly-internal sf object. To do so, we’re going to write a file in the data/ folder, named something like your_data.R, taking advantage of the delayedAssign() function like so:3\ndelayedAssign(\"your_data\", local({\n  requireNamespace(\"sf\", quietly = TRUE)\n  your_package:::your_data\n}))\nThis function makes it so that, whenever a user calls your data object for the first time in a session, you quietly load the sf package before assigning your internal data to the exported object. This doesn’t attach the sf package, meaning that your users won’t accidentally have the entire sf package dumped into their search path. Loading the sf package, however, is enough to fix both problems 1 and 2; your data will now play happily with tibbles, dplyr functions, and the broader R ecosystem.\nYou then need to document your data in a file under R/ the same way you always would.\nAnd that’s it! Your package will now be more user-friendly and CRAN-acceptable. I’ve been using this approach for months with no side effects4 and am planning to keep using it going forward."
  },
  {
    "objectID": "posts/2022-12-01-sf-in-packages/index.html#update",
    "href": "posts/2022-12-01-sf-in-packages/index.html#update",
    "title": "How to include sf data in R packages",
    "section": "Update",
    "text": "Update\nEdzer points out on Mastodon that, in order to play nicely with potential changes in sf, you can include your data as an external file instead. The steps here are broadly similar: save your sf object to a file in inst/extdata, and write a file in R/ that looks something like this:\ndelayedAssign(\"your_data\", local({\n  try(\n    sf::read_sf(\n      system.file(\"extdata/your_data.gpkg\", package = \"your_package\")\n    ),\n    silent = TRUE\n  )\n}))\nWe need to wrap the call in try() in order to make this approach work with tooling like devtools::document(), which will otherwise complain about trying to read a file that isn’t there.\nIn my tests, this was enough to fix issues 1 and 2; calling sf::read_sf() will load the sf package and should fix any interoperability issues. However, this approach doesn’t necessarily fix the non-ASCII character WARNING given off by R CMD check.\nOf course, you can also include external files in inst/ and then have your users read it themselves using system.file(), like the sf vignettes do themselves!"
  },
  {
    "objectID": "posts/2022-12-01-sf-in-packages/index.html#footnotes",
    "href": "posts/2022-12-01-sf-in-packages/index.html#footnotes",
    "title": "How to include sf data in R packages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis section was edited on 2022-12-01; thanks to Barry Rowlingson for pointing out a mistake in the original draft.↩︎\nTheoretically this should be fixed by setting Encoding: UTF8 in your package DESCRIPTION, but that rarely seems to help.↩︎\nJust in case it’s not clear: make sure to change your_data and your_package below to, well, the name of your data and the name of your package.↩︎\nKnock on wood for me, I’ve tempted the gods.↩︎"
  },
  {
    "objectID": "posts/2022-12-21-shrubland/index.html",
    "href": "posts/2022-12-21-shrubland/index.html",
    "title": "Classification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast",
    "section": "",
    "text": "I have a new paper, “Classification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast”, out today in the International Journal of Remote Sensing! This project was a fantastic collaboration with with Lucas Johnson, Abigail Guinan, and Colin Beier, and I’m thrilled to see it in print.\nShrublands – which we roughly define as “vegetation that’s too tall to be a grassland, but too short to be a forest” – are a pretty rare land cover class in the northeastern United States. In part, that’s because our forests out here are generally pretty stable, and don’t get completely wiped out in the same way as forests in the western US, which means there’s just not a ton of land reforesting after some major disturbance. But also, most forests in the Northeast got their start by taking over abandoned farmland, which was generally left unplowed following major bank runs in the late 1800s and early 1900s.1 Because the first trees to establish on a field all grow up at more or less the same rate,2 that means that older forests are taller forests. With most of our forests pushing 100,3 we don’t have a ton of low-statured vegetation to go around in the region.\nAnd this is a problem! A ton of wildlife species really prefer shrublands, and depend on denser understories and lower branches than 100 year-old forests really provide; a turkey isn’t going to be able to hide from predators in a stately grove of 80 foot tall pines. New York is intentionally managing its public lands to try and increase the prevalence of shrublands for this reason. Shrublands also are promising targets for conservation efforts designed to increase carbon sequestration in the landscape, as they represent potentially unproductive (from a carbon sequestation standpoint) lands returning to forest. While there’s a lot of ink spilled about the sequestration potentials of young forests versus older forests, everyone is generally agreed that we want more protected forestlands to improve carbon sequestration, and being able to identify and preserve current shrublands would likely mean increasing the amount of forestlands in the future.\nAnd as such, there’s a lot of interest in being able to map shrubland prevalence in the Northeast. Unfortunately, that’s easier said than done. Because shrublands are so rare in this region, they’re generally not well-represented in most land cover mapping projects; shrublands make up about 2% of the landscape, and so a model can be wrong about shrublands 100% of the time and still be 98% accurate if it nails everything else. Shrublands also look different in the Northeast than they do elsewhere in the country; whereas much of the western United States has semi-permanent shrubland biomes with different species and structure than forestlands, in the Northeast shrublands are generally composed of the same species as forests, just shorter. Add to this the fact that from space northeastern shrublands look more or less just like forests – it’s hard to gauge the height of vegetation based on the satellite images usually used to train land cover models – and it makes sense that shrublands are usually one of, if not the worst-represented class in land cover models.\nSo, we set out to try and improve our ability to identify shrublands across New York State. We took the same basic approach as our AGB mapping project, used LiDAR data and national land cover maps to identify areas that were probably4 shrublands, and trained models against satellite imagery to try and ID shrublands in areas and years where we don’t have LiDAR data to work with. I think it’s a pretty cool project, and I’m excited to see where Abigail takes this line of work in years to come – as I understand it, she’s currently using the map to help her pick field sites, to try and investigate the dynamics of invasive species in shrublands across the state.\nI’m also incredibly proud of the tech stack backing this project, which does not make it into the paper but really enabled this project from start to finish. In summer 2021, my lab invested heavily in standing up data-sharing infrastructure to make it easy to access pretty much any data that anyone in the lab had ever cared about, be it relatively raw remote sensing data, heavily-engineered predictor suites, or the outputs from past models.5 The original gains from this data platform were pretty incremental; we were already pretty deep in the weeds on our AGB work, so while it was nice that Lucas and I could easily send each other new data6, it just didn’t come up that often.\nThis shrubland project was then the first time we had a new, greenfield project that could take advantage of the data platform. And thanks to our internal infrastructure7 obviating out a lot of the rote busywork involved with collecting, processing, and generally marshaling spatial data from disparate sources, we were able to stand up the first proof-of-concept models much faster than with our past AGB modeling work.\nThis project wound up being quite the hairy beast by the time it actually went to print – the total archive contains ~640GB of data, uncompressed, plus a mix of R, Python, and shell scripts to actually process it all. I learned a lot from pulling this all together, and I’m very excited to see it out in print!"
  },
  {
    "objectID": "posts/2022-12-21-shrubland/index.html#footnotes",
    "href": "posts/2022-12-21-shrubland/index.html#footnotes",
    "title": "Classification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncluding obviously the Great Depression, but also a great number of other panics such as the Panic of 1857. An interesting thing about living in the East is that practically all land in the region was, at one point or another, in private ownership; almost all state or federal lands were either purchased back from private owners or, more often, reclaimed when the owner defaulted on their taxes. A fun activity, for a certain value of fun, is looking up the date your local park was established, and then looking up the closest bank panic to guess if the two are related.↩︎\nThis is one of those things that feels like it should be made up, but if you think about it makes sense: trees more-or-less attempt to maximize sunlight intake, and can only get more sun by being taller than their neighbors, and, crucially, by not being shorter and therefore shaded out by competitors. As such, less competitive individuals from ruderal species will grow out more slowly than their neighbors, but either grow up with the rest of the neighborhood or get killed off altogether.↩︎\nWhich is not old for a forest by any extent, but is certainly older than most shrublands.↩︎\nBut not definitely, and we talk about this as a limitation! Maybe LCMAP thinks 5-foot tall wood post fences look like shrublands; if so, we’d be labeling our training data incorrectly. But our validation against 2019 imagery more or less agreed with our validation against the LiDAR-labeled map, which suggests this wasn’t that big of a problem in the end.↩︎\nThis infrastructure is one of my favorite projects I’ve been a part of in my PhD; I need to write about it publicly at some point.↩︎\nOr, if I’m being honest, that Lucas could send me; if memory serves, while I wrote a lot of the code for how people could get data from the platform, it took over a year for me to learn how to send data to the platform for others.↩︎\nThe data platform and other internal model tools, though the data platform is much better than our internal model tooling. Part of that is because we don’t feel the need to keep a lot of our model tooling internal; we can open-source tools for dealing with common modeling problems much more easily than tools for dealing with our particular approach to cloud data storage. Speaking of which, go star waywiser on GitHub.↩︎"
  },
  {
    "objectID": "posts/2023-04-10-quarto_citation_use_count/index.html",
    "href": "posts/2023-04-10-quarto_citation_use_count/index.html",
    "title": "Counting the number of times each citation is used in a Quarto document",
    "section": "",
    "text": "I’m currently editing my candidacy exam – where I wrote what was effectively a review article on a subject adjacent to, but not within, my dissertation area – down into something that might be publishable. Part of that work has been trimming down the number of citations in the paper: my original exam had 300 individual citations, which is a bit much, and the places I’m looking at sending this paper generally restrict articles to 100 total cites.\nAs such, I’ve needed to trim down the number of citations used in this paper.1 To do that, I’ve been trying to figure out which sources were only cited a handful of times and could be easily removed. There’s plenty online about how to do that calculation with LaTeX, but I’ve been writing both my exam and this new paper in Quarto and was having a hard time getting those code snippets to work.\nIt turned out that I needed to use three fields in my document’s YAML header. Namely, I needed to set the cite-method of my document to biblatex, set the biblatexoptions field to citecounter=true, and then include a LaTeX snippet to actually print those citation counts to the text key below include-before-body:2\nNow when you render your document, each citation entry will be followed by a small snippet of text reading “Cited X times”, where X is the number of times each reference was cited in your document."
  },
  {
    "objectID": "posts/2023-04-10-quarto_citation_use_count/index.html#footnotes",
    "href": "posts/2023-04-10-quarto_citation_use_count/index.html#footnotes",
    "title": "Counting the number of times each citation is used in a Quarto document",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd to be clear, these are not 300 citations which are truly core to the paper’s argument. There’s a number of places in the paper where I more or less say “This line of work has been highly influential in later work”, and then cite every single article I read which cited the original study; this made a bit more sense in the context of “candidacy exam” where I was trying to demonstrate that I had comprehensively surveyed the literature, but isn’t something I’d ever do in published work.↩︎\nIs that key new? I could have sworn I used to provide raw TeX to the include-before-body field, but apparently now that needs to be a file name.↩︎"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html",
    "href": "junkdrawer/summer2021/index.html",
    "title": "Summer 2021 Targets",
    "section": "",
    "text": "Otherwise, unless we have more observations or are willing to compute more predictors, I think this is pretty well set\n\n\n\n\n\nHistoric AGB map accuracy pipeline\nI think it makes sense for me to start getting more involved with this end, & give historical models the attention I’ve given the LiDAR-year set. But I haven’t been particularly involved before, I don’t know what this entails beyond:\n\nWork with non-WWE sets\nInclude more predictors\nEnsembling\n\n\n\n\n\n\nBreak up model building notebook into functions; probably its own package\n\n\n\n\n\nCarbon Accounting Tool"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#cafri",
    "href": "junkdrawer/summer2021/index.html#cafri",
    "title": "Summer 2021 Targets",
    "section": "",
    "text": "Otherwise, unless we have more observations or are willing to compute more predictors, I think this is pretty well set\n\n\n\n\n\nHistoric AGB map accuracy pipeline\nI think it makes sense for me to start getting more involved with this end, & give historical models the attention I’ve given the LiDAR-year set. But I haven’t been particularly involved before, I don’t know what this entails beyond:\n\nWork with non-WWE sets\nInclude more predictors\nEnsembling\n\n\n\n\n\n\nBreak up model building notebook into functions; probably its own package\n\n\n\n\n\nCarbon Accounting Tool"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#dissertation-research",
    "href": "junkdrawer/summer2021/index.html#dissertation-research",
    "title": "Summer 2021 Targets",
    "section": "Dissertation Research",
    "text": "Dissertation Research\n\nWrite proposal\nterrain-into-Unity pipeline\nPlace arbitrary object pipeline\nStretch goal: chapter 1 (On Abstraction)\nStretch goal: chapter 2 (survey of environmental visualization)"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#ml-course",
    "href": "junkdrawer/summer2021/index.html#ml-course",
    "title": "Summer 2021 Targets",
    "section": "ML Course",
    "text": "ML Course\n\nWrite course handouts\nWrite assignments"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#personal",
    "href": "junkdrawer/summer2021/index.html#personal",
    "title": "Summer 2021 Targets",
    "section": "Personal",
    "text": "Personal\n\nNo plans yet but going to try to disappear into the woods for a week if I can swing it"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#completed",
    "href": "junkdrawer/summer2021/index.html#completed",
    "title": "Summer 2021 Targets",
    "section": "Completed",
    "text": "Completed\n\nLiDAR-Year AGB\n\nAdd in the missing 30ish plots"
  },
  {
    "objectID": "junkdrawer/a-simple-script/index.html#speed-versus-capacity",
    "href": "junkdrawer/a-simple-script/index.html#speed-versus-capacity",
    "title": "Cool graphs about elevators",
    "section": "Speed versus capacity",
    "text": "Speed versus capacity"
  },
  {
    "objectID": "junkdrawer/a-simple-script/index.html#where-in-the-world-did-all-my-elevators-go",
    "href": "junkdrawer/a-simple-script/index.html#where-in-the-world-did-all-my-elevators-go",
    "title": "Cool graphs about elevators",
    "section": "Where in the world did all my elevators go",
    "text": "Where in the world did all my elevators go"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/manhattan/index.html#speed-versus-capacity",
    "href": "junkdrawer/parameterized-reports/manhattan/index.html#speed-versus-capacity",
    "title": "Cool graphs about elevators",
    "section": "Speed versus capacity",
    "text": "Speed versus capacity"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/manhattan/index.html#where-in-the-world-did-all-my-elevators-go",
    "href": "junkdrawer/parameterized-reports/manhattan/index.html#where-in-the-world-did-all-my-elevators-go",
    "title": "Cool graphs about elevators",
    "section": "Where in the world did all my elevators go",
    "text": "Where in the world did all my elevators go"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/index.html#speed-versus-capacity",
    "href": "junkdrawer/parameterized-reports/index.html#speed-versus-capacity",
    "title": "Cool graphs about elevators",
    "section": "Speed versus capacity",
    "text": "Speed versus capacity"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/index.html#where-in-the-world-did-all-my-elevators-go",
    "href": "junkdrawer/parameterized-reports/index.html#where-in-the-world-did-all-my-elevators-go",
    "title": "Cool graphs about elevators",
    "section": "Where in the world did all my elevators go",
    "text": "Where in the world did all my elevators go"
  }
]