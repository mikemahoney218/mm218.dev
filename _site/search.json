[
  {
    "objectID": "portfolio/index.html",
    "href": "portfolio/index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Mike Mahoney: Visual Art\n\n\n\n\n\n\n\n\nVisuals\n\n\nClick any image to see a larger version. Click on the right half of the image (or press the right arrow key) to see the next image; click the left half (or press the left arrow) to go back.\n\n\nJump to section: maps (digital maps, plotter drawings, 3D), generative (trees, curves).\n\n\n\n\n\nMaps\n\n\nDigital Maps\n\n\n                  \n\n\n3D Maps\n\n\n        \n\n\nPlotter Drawings\n\n\nDrawn with an AxiDraw V3 pen plotter.\n\n\n \n\n\n\nGenerative\n\n\nHave a Tree\n\n\nGeometric patterns made by growing lines at various angles and decay rates. Play with the system for yourself at this link!\n\n\n    \n\n\nCurves\n\n\n\n\n                                \n\n\nMiscellaneous"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Presentations\n\nAs lead/presenting author:\n\n2022\n\n\n\nDetecting regenerating forestland at a landscape level.\n\n\nWith Lucas K. Johnson, and Colin M. Beier\n\n\nRegular talk, Ecological Society of America and Canadian Society for Ecology and Evolution Joint Annual Meeting, Montreal, Quebec, Canada, August 15 2022.\n\n\n\n\n\nunifir: A Unifying API for Working with Unity in R.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, useR! 2022 (Virtual), June 22 2022.\n\n\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass.\n\n\nWith Lucas K. Johnson, and Colin M. Beier\n\n\nRegular talk, North American Forest Ecology Workshop (Virtual), June 20 2022.\n\n\n\n\n\nIt’s not what it looks like: Learning to question assumptions when debugging ML models.\n\n\nRegular talk, Data Mishaps Night (Virtual), February 24 2022.\n\n\n\n\nUsing AI/ML to help New York manage lands for net zero carbon.\n\n\nInvited talk, Federation of Earth Science Information Partners (ESIP) January Meeting, Annapolis, MD (Virtual), January 20 2022.\n\n\nRecording available at https://youtu.be/k8AqtJFpYEk?t=3065\n\n\n\n2021\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass.\n\n\nWith Lucas K. Johnson, Eddie Bevilacqua, and Colin M. Beier\n\n\nPoster presentation, American Geophysical Union Fall Meeting, New Orleans LA, December 15 2021.\n\n\nPoster available at https://www.mm218.dev/papers/ground_filtering/poster.pdf\n\n\n\n\nInteractive landscape simulations for visual resource assessment.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, Visual Resources Stewardship Conference, SUNY-ESF, Syracuse, NY, October 21 2021.\n\n\nSlides available at https://mikemahoney218.github.io/2021-10-21-vrs/\n\n\n\n\nProducing Interactive 3D Landscape Visualizations in Unity Using Terrainr R Package.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nInvited workshop, Visual Resources Stewardship Conference, SUNY-ESF, Syracuse, NY, October 19 2021.\n\n\nMaterials available at https://mikemahoney218.github.io/2021-10-19-vrs-workshop/2021-10-19-vrs-workshop.html\n\n\n\n\nterrainr: Spatial Data Access and Visualization in R.\n\n\nInvited talk, Earth Science Information Partners, Severna Park MD, September 9 2021.\n\n\nSlides available at https://mm218.dev/esip2021.\n\n\nVideo recording available at https://youtu.be/xWZ7QQMr_AQ\n\n\n\n\nVirtual Environments: Using R as a Frontend for 3D Rendering of Digital Landscapes.\n\n\nWith Colin M. Beier and Aidan C. Ackerman\n\n\nRegular talk, useR! 2021, ETH Zürich, Zürich, Switzerland, July 6 2021.\n\n\nSlides available at https://mm218.dev/user2021.\n\n\nVideo recording available at https://www.youtube.com/watch?t=2202&v=tbt8ZsHm5eA\n\n\n\n\nAccessing the USGS National Map and Making 3D Maps with terrainr.\n\n\nWith Colleen Nell (lead author) and Lindsay Platt\n\n\nInvited workshop, USGS Center for Data Integration, Lakewood, CO, May 28 2021.\n\n\nMaterials available at https://www.mm218.dev/cdi/cdi.html.\n\n\n\n\nterrainr: Landscape Visualizations Using Data from the National Map.\n\n\nInvited talk, USGS National Geospatial Technical Operations Center, Denver, CO, February 5 2021.\n\n\n\n2018\n\n\n\nBeaver Foraging Preferences and Impacts on Forest Structure in the Adirondack Mountains of New York.\n\n\nWith John C. Stella\n\n\nRegular talk, Forest Ecosystem Monitoring Collective Conference, University of Vermont, Burlington, VT, December 2018.\n\n\n\n\nBeaver Foraging Preferences and Impacts on Forest Structure in the Adirondack Mountains of New York.\n\n\nWith John C. Stella\n\n\nRegular talk, Rochester Academy of Sciences Fall Scientific Paper Session, Geneseo, NY, November 2018.\n\n\n\nAs co-/non-presenting author:\n\n2021\n\n\n\nBroad‑scale forest biomass mapping: generating contiguous high‑resolution predictions using a spatio‑temporal patchwork of LiDAR coverages across a mixed‑use landscape.\n\n\nWith Lucas K. Johnson (lead author), Eddie Bevilacqua, and Colin M. Beier\n\n\nRegular talk, American Geophysical Union Fall Meeting, New Orleans LA, December 15 2021.\n\n\n\n\nGreening Up Before Growing Up: Challenges in Modeling Forest Biomass Recovery Post‑Harvest Using Satellite Imagery.\n\n\nWith Lucas K. Johnson (lead author) and Colin M. Beier\n\n\nRegular talk, Society of American Foresters National Convention, Virtual, November 2021.\n\n\n\n2019\n\n\n\nNutritional Impacts on Invasive Beech Scale Quantification in Beech Bark Disease Aftermath Forests.\n\n\nWith Gretchen A. Dillion (lead author), Stephanie Chase, and  Mariann Johnston.\n\n\nPoster presentation, New York Society of American Foresters Annual Meeting, Syracuse, NY, November 2019.\n\n\n\n2017\n\n\n\nAn Investigation of Nutritional Effects on Beech Bark Disease Causal Organisms.\n\n\nWith Gretchen A. Dillon (lead author),  Mariann Johnston, Vizma Leimanis, and Jason Stoodley\n\n\nPoster presentation, Forest Ecosystem Monitoring Collective Conference, University of Vermont, Burlington, VT, December 2017.\n\n\n\n\nAn Investigation of Nutritional Effects on Beech Bark Disease Causal Organisms.\n\n\nWith Gretchen A. Dillon (lead author),  Mariann Johnston, Vizma Leimanis, and Jason Stoodley\n\n\nPoster presentation, Rochester Academy of Sciences Fall Scientific Paper Session, Geneseo, NY, November 2017."
  },
  {
    "objectID": "junkdrawer/summer2021/index.html",
    "href": "junkdrawer/summer2021/index.html",
    "title": "Summer 2021 Targets",
    "section": "",
    "text": "Otherwise, unless we have more observations or are willing to compute more predictors, I think this is pretty well set\n\n\n\n\n\nHistoric AGB map accuracy pipeline\nI think it makes sense for me to start getting more involved with this end, & give historical models the attention I’ve given the LiDAR-year set. But I haven’t been particularly involved before, I don’t know what this entails beyond:\n\nWork with non-WWE sets\nInclude more predictors\nEnsembling\n\n\n\n\n\n\nBreak up model building notebook into functions; probably its own package\n\n\n\n\n\nCarbon Accounting Tool"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#dissertation-research",
    "href": "junkdrawer/summer2021/index.html#dissertation-research",
    "title": "Summer 2021 Targets",
    "section": "Dissertation Research",
    "text": "Dissertation Research\n\nWrite proposal\nterrain-into-Unity pipeline\nPlace arbitrary object pipeline\nStretch goal: chapter 1 (On Abstraction)\nStretch goal: chapter 2 (survey of environmental visualization)"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#ml-course",
    "href": "junkdrawer/summer2021/index.html#ml-course",
    "title": "Summer 2021 Targets",
    "section": "ML Course",
    "text": "ML Course\n\nWrite course handouts\nWrite assignments"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#personal",
    "href": "junkdrawer/summer2021/index.html#personal",
    "title": "Summer 2021 Targets",
    "section": "Personal",
    "text": "Personal\n\nNo plans yet but going to try to disappear into the woods for a week if I can swing it"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#completed",
    "href": "junkdrawer/summer2021/index.html#completed",
    "title": "Summer 2021 Targets",
    "section": "Completed",
    "text": "Completed\n\nLiDAR-Year AGB\n\nAdd in the missing 30ish plots"
  },
  {
    "objectID": "junkdrawer/riemann/index.html",
    "href": "junkdrawer/riemann/index.html",
    "title": "Multi-scale model assessment with spatialsample",
    "section": "",
    "text": "For this reason, a number of researchers (most notably, Riemann et al. (2010)1) have suggested assessing models at multiple scales of spatial aggregation to ensure cross-scale differences in model accuracy are identified and reported. This post walks through how to do that using the new spatialsample package.\nBecause Riemann et al. were working with data from the US Forest Inventory and Analysis (FIA) program, we’re going to do the same. However, because our main goal is to show how spatialsample can support this type of analysis, we won’t spend a ton of time worrying about any of the quirks of FIA data2 or on feature engineering. Instead, we’re going to use a simple linear model to see if we can predict how much aboveground biomass (“AGB”; all the non-root woody bits) there is in a forest based on how many trees there are. We’ll use all the FIA field data from New York State, USA.\nBecause we’re mostly interested in assessing our models, I’m going to mostly ignore how exactly I downloaded and wrangled the FIA data for this post. If you’re curious, I’ve hidden the code below:\n\nCodelibrary(dplyr)\n\n# Download the FIA database for New York over the internet,\n# and unzip it into our local directory\n#\n# This updates annually, which means that this post likely won't\n# generate the exact same results after 2022\nhttr::GET(\n  \"https://apps.fs.usda.gov/fia/datamart/Databases/SQLite_FIADB_NY.zip\",\n  httr::write_disk(\"SQLite_FIADB_NY.zip\", TRUE)\n)\n\nunzip(\"SQLite_FIADB_NY.zip\")\n\n# We're going to work with the database through dplyr's database connections\n#\n# But first, we need to create a DBI connection to the database and\n# load out tables:\ncon <- DBI::dbConnect(RSQLite::SQLite(), dbname = \"FIADB_NY.db\")\ntrees <- tbl(con, \"TREE\")\n\nplots <- tbl(con, \"PLOT\")\n\n# The FIA database has every measurement ever collected by the program;\n# we'll filter to only the most recent survey for each of the plots.\n#\n# Plots are measured on a rolling 7 year basis, so we'll also cut out any\n# plots which might not be remeasured anymore with a call to filter()\nplots <- plots |> \n  group_by(PLOT) |> \n  filter(INVYR == max(INVYR, na.rm = TRUE)) |> \n  ungroup() |> \n  filter(INVYR > 2009) |> \n  collect()\n\ncopy_to(con, plots, \"newest_plots\", TRUE)\nnewest_plots <- tbl(con, \"newest_plots\")\n\n# Now we'll use a filtering join to select only trees measured in the most\n# recent sample at each plot\n#\n# We'll also count how many trees were at each plot,\n# sum up their AGB, \n# and save out a few other useful columns like latitude and longitude\nplot_measurements <- trees |> \n  right_join(newest_plots, by = c(\"INVYR\", \"PLOT\")) |> \n  group_by(PLOT) |> \n  summarise(\n    yr = mean(INVYR, na.rm = TRUE),\n    plot = mean(PLOT, na.rm = TRUE),\n    lat = mean(LAT, na.rm = TRUE),\n    long = mean(LON, na.rm = TRUE),\n    n_trees = n(),\n    agb = sum(DRYBIO_AG, na.rm = TRUE)\n  ) |> \n  collect() |> \n  mutate(\n    # Because of how we joined, `n_trees` is always at least 1 -- \n    # even if there are 0 trees\n    n_trees = ifelse(is.na(agb) & n_trees == 1, 0, n_trees),\n    agb = ifelse(is.na(agb), 0, agb)\n  )\n\nDBI::dbDisconnect(con)\n\nreadr::write_csv(plot_measurements, \"plot_measurements.csv\")\n\n\nWith that pre-processing done, it’s time for us to load our data and turn it into an sf object. We’re going to reproject our data to use a coordinate reference system that the US government tends to use for national data products, like the FIA:\n\nlibrary(sf)\n\ninvisible(sf_proj_network(TRUE))\n\nplot_measurements <- readr::read_csv(\"https://www.mm218.dev/junkdrawer/riemann/plot_measurements.csv\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) |> \n  st_transform(5070)\n\nAnd this is what we’re going to go ahead and resample. We want to assess our model’s performance at multiple scales, following the approach in Riemann et al. That means we need to do the following:\n\nBlock our study area using multiple sets of regular hexagons of different sizes, and assign our data to the hexagon it falls into within each set.\nPerform leave-one-block-out cross-validation with each of those sets, fitting our model to n - 1 of the n hexagons we’ve created and assessing it on the hold-out hexagon.\nCalculate model accuracy for each size based on those held-out hexes.\n\nSo to get started, we need to block our study area. We can do this using the spatial_block_cv() function from spatialsample. We’ll generate ten different sets of hexagon tiles, using cellsize arguments of between 10,000 and 100,000 meters3. The code to do that, and to store all of our resamples in a single tibble, looks like this4:\n\nset.seed(123)\nlibrary(dplyr)\nlibrary(spatialsample)\ncellsize <- seq(10, 100, 10) * 1000\n\nriemann_resamples <- tibble(\n  resamples = purrr::map(\n    cellsize, \n    \\(cs) {\n      spatial_block_cv(\n        plot_measurements,\n        v = Inf,\n        cellsize = cs,\n        square = FALSE\n      )\n    }\n  ),\n  cellsize = cellsize\n)\n\nIf we want, we can visualize one (or more) of our resamples, to get a sense of what our tiling looks like:\n\nautoplot(riemann_resamples$resamples[[10]])\n\n\n\n\nAnd that’s step 1 of the process completed! Now we need to move on to step 2, and actually fit models to each of these resamples. I want to highlight that this is a lot of models, and so is going to take a while5:\n\npurrr::map_dbl(\n  riemann_resamples$resamples,\n  nrow\n) |> \n  sum()\n\n[1] 2600\n\n\nWith that said, actually fitting those few thousand models is a two part process. First, we’re going to load the rest of the tidymodels packages and use them to define a workflow (from the workflows package), specifying the formula and model that we want to fit to each resample:\n\nlibrary(tidymodels)\n\nlm_workflow <- workflow() |> \n  add_model(linear_reg()) |> \n  add_formula(agb ~ n_trees)\n\nNext, we’ll actually apply that workflow a few thousand times! We’ll calculate two metrics for each run of the model: the root-mean-squared error (RMSE) and the mean absolute error (MAE). We can add these metrics as a new column to our resamples using the following:\n\nriemann_resamples <- riemann_resamples |> \n  mutate(\n    resampled_outputs = purrr::map(\n      resamples, \n      fit_resamples,\n      object = lm_workflow,\n      metrics = metric_set(\n        rmse,\n        mae\n      )\n    )\n  )\n\nThe riemann_resamples object now includes both our original resamples as well as the accuracy metrics associated with each run of the model. A very cool thing about this approach is that we can now visualize our block-level accuracy metrics with a few lines of code.\nFor instance, if we wanted to plot block-level RMSE for our largest assessment scale, we could use the following code to “unnest” our nested metric and resample columns:\n\nriemann_resamples$resampled_outputs[[10]] |> \n  mutate(splits = purrr::map(splits, assessment)) |> \n  unnest(.metrics) |> \n  filter(.metric == \"rmse\") |> \n  unnest(splits) |> \n  st_as_sf() |> \n  ggplot(aes(color = .estimate)) + \n  geom_sf()\n\n\n\n\nWe can also go on to the third step of our assessment process, and get our model accuracy metrics for each aggregation scale we investigated. We’ll create a new data frame with only our cellsize variable and the associated model metrics:\n\nriemann_metrics <- riemann_resamples |> \n  transmute(\n    cellsize = cellsize,\n    resampled_metrics = purrr::map(resampled_outputs, collect_metrics)\n  ) |> \n  unnest(resampled_metrics)\n\nhead(riemann_metrics)\n\n# A tibble: 6 × 7\n  cellsize .metric .estimator  mean     n std_err .config             \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1    10000 mae     standard   5787.  1541    99.9 Preprocessor1_Model1\n2    10000 rmse    standard   6980.  1541   121.  Preprocessor1_Model1\n3    20000 mae     standard   5722.   424   130.  Preprocessor1_Model1\n4    20000 rmse    standard   7644.   424   169.  Preprocessor1_Model1\n5    30000 mae     standard   5637.   205   161.  Preprocessor1_Model1\n6    30000 rmse    standard   7725.   205   218.  Preprocessor1_Model1\n\n\nAnd just like that, we’ve got a multi-scale assessment of our model’s accuracy! We can then use this to investigate and report how well our model does at different levels of aggregation. For instance, by plotting RMSE against MAE at various scales, it appears that our RMSE increases with aggregation while MAE decreases. This hints that, as we aggregate our predictions to larger hexagons, more of our model’s overall error is driven by large outliers:\n\nlibrary(ggplot2)\n\nggplot(riemann_metrics, aes(cellsize, mean, color = .metric)) + \n  geom_line() +\n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\nFootnotes\n\nRiemann, R., Wilston, B. T., Lister, A., and Parks, S. 2010. An effective assessment protocol for continuous geospatial datasets of forest characteristics using USFS Forest Inventory and Analysis (FIA) data. Remote Sensing of Environment, 114, pp. 2337-2353. doi: 10.1016/j.rse.2010.05.010.↩︎\nAmong them that only forested areas are measured, where “forested” means “principally used as forest” which excludes parks but includes recently clear-cut lands, and that plot locations are considered personally identifying information under the farm bill of 1985, and so as to not identify anyone the coordinates in public data are “fuzzed” by a few miles and approximately 20% of plot coordinates are swapped with other plots in the data set. Which is to say, consult your local forester or ecologist if you want to use FIA data to answer real questions in your own work.↩︎\nThis value is in meters because our coordinate reference system is in meters. It represents the length of the apothem, from the center of each polygon to the middle of the side. We’re using hexagons because Riemann et al. also used hexagons.↩︎\nv is Inf because we want to perform leave-one-block-out cross-validation, but we don’t know how many blocks there will be before they’re created. This is the supported way to do leave-one-X-out cross-validation in spatialsample > 0.2.0 (another option is to set v = NULL).↩︎\nLinear regression was invented in 1805, ish, long before the Analytical Engine was a twinkle in Babbage’s eye. Whenever I get frustrated at how long fitting multiple models like this takes, I like to take a step back and recognize that I am asking my poor overworked computer to fit roughly as many models as were used in the first ~100 years of the technique’s life.↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Progress, Purpose, Process\n\n\n\n\n\nReflecting on another year.\n\n\n\n\n\n\nFeb 18, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nterrainr: An R package for creating immersive virtual environments\n\n\n\n\n\nNew open-access paper published in the Journal of Open Source Science.\n\n\n\n\n\n\nJan 14, 2022\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated {drat} uploads with GitHub Actions\n\n\n\n\n\n\n\nR\n\n\nTutorials\n\n\nCI\n\n\nCD\n\n\nGitHub Actions\n\n\n\n\nContinuously deploy your code to personal CRAN-like repos, automatically and for free!\n\n\n\n\n\n\nSep 23, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nInteractive landscape simulations for visual resource assessment\n\n\n\n\n\nNew preprint for the 2021 Visual Resources Stewardship Conference.\n\n\n\n\n\n\nJun 14, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVirtual environments talk at useR! 2021\n\n\n\n\n\nWith slides and a rough outline available on GitHub.\n\n\n\n\n\n\nJun 12, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s new in terrainr 0.4.0?\n\n\n\n\n\nThe new CRAN release of terrainr improves consistency, CRS logic, and fixes some bugs.\n\n\n\n\n\n\nApr 22, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterrainr 0.3.0 is out today\n\n\n\n\n\n\n\nR\n\n\nData science\n\n\nterrainr\n\n\n\n\nNew version, who this?\n\n\n\n\n\n\nFeb 17, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nModel averaging methods: how and why to build ensemble models\n\n\n\n\n\n\n\nR\n\n\nData science\n\n\n\n\nAveraging predictions for fun and profit – and for dealing with the uncertainty of model selection. With examples in R!\n\n\n\n\n\n\nJan 18, 2021\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nSome Updates\n\n\n\n\n\n\n\nR\n\n\nTwitter\n\n\nphd\n\n\nterrainr\n\n\nbeaver\n\n\n\n\nWhere Do We Come From? What Are We? Where Are We Going?\n\n\n\n\n\n\nOct 16, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMake a Retweet Bot in R\n\n\n\n\n\n\n\nR\n\n\nTwitter\n\n\necology_tweets\n\n\n\n\nY’know. If you wanna.\n\n\n\n\n\n\nJun 20, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nInstalling the TIG stack on Raspberry Pi\n\n\n\n\n\n\n\nRaspberry Pi\n\n\nTutorials\n\n\nData Visualization\n\n\nMonitoring\n\n\nTelegraf\n\n\nInfluxDB\n\n\nGrafana\n\n\n\n\nA guide to installing InfluxDB, Telegraf, and Grafana on a Raspberry Pi 4 running Raspbian Buster. Unlike every other guide like this on the internet, this one works.\n\n\n\n\n\n\nMay 3, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA minimalist visualization of Coronavirus rates\n\n\n\n\n\n\n\nR\n\n\n\n\nMade with Shiny and R\n\n\n\n\n\n\nApr 27, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nMaking Excellent Visualizations\n\n\n\n\n\n\n\nData Visualization\n\n\nTutorials\n\n\n\n\nPart 3 in the data visualization series\n\n\n\n\n\n\nApr 22, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nMechanics of Data Visualizations\n\n\n\n\n\n\n\nData Visualization\n\n\nTutorials\n\n\n\n\nPart 2 in the data visualization series\n\n\n\n\n\n\nApr 21, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nTheory of Data Visualizations\n\n\n\n\n\n\n\nData Visualization\n\n\nTutorials\n\n\n\n\nPart 1 in the data visualization series\n\n\n\n\n\n\nApr 20, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing {spacey}, now on CRAN!\n\n\n\n\n\n\n\nR\n\n\nR Packages\n\n\nmaps\n\n\nspacey\n\n\ngeospatial data\n\n\n\n\nUSGS data access and rayshader maps, done cheap.\n\n\n\n\n\n\nMar 24, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing {heddlr}, now on CRAN!\n\n\n\n\n\n\n\nR\n\n\nR Packages\n\n\nR Markdown\n\n\n\n\nWrite less boilerplate, get more done.\n\n\n\n\n\n\nJan 23, 2020\n\n\nMike Mahoney\n\n\n\n\n\n\n  \n\n\n\n\nThesis Now Available in ESF Digital Commons\n\n\n\n\n\n\n\nPublications\n\n\nBeaver\n\n\n\n\nIt’s a real humdinger.\n\n\n\n\n\n\nMar 27, 2019\n\n\nMike Mahoney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vr/index.html",
    "href": "vr/index.html",
    "title": "Virtual Reality is Always Coming Never",
    "section": "",
    "text": "Advances in graphical technology have now made it possible for us to interact with information in innovative ways, most notably by exploring multimedia environments and by manipulating three-dimensional virtual worlds.\n— Mike Scaife & Yvonne Rogers: External cognition: how do graphical representations work?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "I’m a PhD candidate at SUNY-ESF, working on predictive modeling and visualization with a focus on natural systems. My current work focuses on using game engines as GIS to visualize large-scale landscapes with a focus on forested systems as a method for scientific communication and outreach. Past projects have focused on landscape and community ecology, and have included predictive models of forest biomass from remote sensing data and the landscape-level impacts of beaver within New York’s Adirondack State Park. In all of these projects I’ve worked to apply data science techniques to large-scale challenges, frequently serving as the statistical and technical expert within my team.\nOn this site I keep a list of my publications, presentations, and my CV, as well as a technical blog.\n\n\n\n\n\nState University of New York College of Environmental Science and Forestry\nSyracuse, NY\nPh.D. in Environmental Science | August 2020 - Present (Expected 2024)\nB.S. in Forest Ecosystem Science | August 2015 - December 2018\n\n\n\n\n\n\n\nunifir: A Unifying API for Interacting with Unity from R. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388\nterrainr: An R package for creating immersive virtual environments. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software, 7(69): 4060. https://doi.org/10.21105/joss.04060\nStem size selectivity is stronger than species preferences for beaver, a central place forager. Mahoney, M. J. and Stella, J. C. 2020. Forest Ecology and Management, 475, 118331. https://doi.org/10.1016/j.foreco.2020.118331.\n\n\n\nunifir | A Unifying API for Working with Unity in R | 2022\nterrainr | Retrieve Data from the USGS National Map and Transform it for 3D Landscape Visualizations | 2021\nheddlr | Functional Programming Concepts for R Markdown Documents | 2020"
  },
  {
    "objectID": "quotes/index.html",
    "href": "quotes/index.html",
    "title": "Quotes",
    "section": "",
    "text": "We are as gods. We may as well get good at it.\n— Stewart Brand\n\nWithout data, you’re just another person with an opinion\n— W. Edwards Deming\n\nWithout data, you’re just another person with an opinion. […] With data, you’re still just another person with an opinion. Expert analysts understand this in their very bones.\n— Cassie Kozyrkov\n\nThe three golden rules to ensure computer security are: do not own a computer; do not power it on; and do not use it.\n— Robert Morris\n\nMy second meta-principle of statistics is the methodological attribution problem, which is that the many useful contributions of a good statistical consultant, or collaborator, will often be attributed to the statistician’s methods or philosophy rather than to the artful efforts of the statistician himself or herself. Don Rubin has told me that scientists are fundamentally Bayesian (even if they do not realize it), in that they interpret uncertainty intervals Bayesianly. Brad Efron has talked vividly about how his scientific collaborators find permutation tests and p-values to be the most convincing form of evidence. Judea Pearl assures me that graphical models describe how people really think about causality. And so on. I am sure that all these accomplished researchers, and many more, are describing their experiences accurately. Rubin wielding a posterior distribution is a powerful thing, as is Efron with a permutation test or Pearl with a graphical model, and I believe that (a) all three can be helping people solve real scientific problems, and (b) it is natural for their collaborators to attribute some of these researchers’ creativity to their methods.\nThe result is that each of us tends to come away from a collaboration or consulting experience with the warm feeling that our methods really work, and that they represent how scientists really think. In stating this, I am not trying to espouse some sort of empty pluralism — the claim that, for example, we would be doing just as well if we were all using fuzzy sets, or correspondence analysis, or some other obscure statistical method. There is certainly a reason that methodological advances are made, and this reason is typically that existing methods have their failings. Nonetheless, I think we all have to be careful about attributing too much from our collaborators’ and clients’ satisfaction with our methods.\n— Andrew Gelman: Bayesian Statistics Then and Now\n\nEven one of the most elementary things taught on a statistics course, the standard deviation, is more complex than it need be, and is considered here as an example of how convenience for mathematical manipulation often over-rides pragmatism in research methods.\n[…]\nIn essence, the claim made for the standard deviation is that we can compute a number (SD) from our observations that has a relatively consistent relationship with a number computed in the same way form the population figures. This claim, in itself, is of no great value. Reliability alone does not make that number of any valid use. For example, if the computation led to a constant whatever figures were used then there would be a perfectly consistent relationship between the parameters for the sample and population. But to what end? Surely the key issue is not how stable the statistic but whether it encapsulates what we want it to.\n[…]\nOf course, much of the rest of traditional statistics is now based on the standard deviation, but it is important to realise that it need not be.\n— Stephen Gorard: Revisiting a 90-Year-Old Debate: The Advantages of the Mean Deviation\n\nOpinionated software is only cool if you have cool opinions\n— Tom MacWright: soapbox: longitude, latitude is the right way\n\nThere was some discussion in the comments thread here about axis labels and zero. Sometimes zero has no particular meaning (for example when graphing degrees Fahrenheit), but usually it has a clear interpretation, in which case it can be pleasant to include it on the graph. On the other hand, if you’re plotting something that varies from, say 184 to 197, it would typically be a bad idea to extend the axis all the way to zero, as this would destroy your visual resolution.\nThe advice we usually give is: If zero is in the neighborhood, invite it in.\n— Andrew Gelman: Graphing advice\n\nYoung man, in mathematics you don’t understand things. You just get used to them.\n— John Von Neumann\n\nTo deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say ‘fourteen’ to yourself very loudly. Everyone does it.\n— Geoffrey Hinton: A geometrical view of perceptrons\n\nDependencies are an open invitation for other people to break your code.\n— Nathan Eastwood: useR 2021 talk on poorman\n\nRAM is cheap and thinking hurts.\n— Uwe Ligges: R-Help\n\nSoftware people are not alone in facing complexity. Physics deals with terribly complex objects even at the “fundamental” particle level. The physicist labors on, however, in a firm faith that there are unifying principles to be found, whether in quarks or in unified field theories. Einstein repeatedly argued that there must be simplified explanations of nature, because God is not capricious or arbitrary.\nNo such faith comforts the software engineer. Much of the complexity he must master is arbitrary complexity, forced without rhyme or reason by the many human institutions and systems to which his interfaces must confirm. These differ from interface to interface, and from time to time, not because of necessity but only because they were designed by different people, rather than by God.\n— Frederick P. Brooks, Jr.: No Silver Bullet —- Essence and Accident in Software Engineering\n\nOn-prem is a lock-in. Cloud is a lock-in. Every single language you program in is a type of lock-in. Python is easy to get started with, but soon you run into packaging issues and are optimizing the garbage collector. Scala is great, but everyone winds up migrating away from it. And on and on.\nEvery piece of code written in a given language or framework is a step away from any other language, and five more minutes you’ll have to spend migrating it to something else. That’s fine. You just have to decide what you’re willing to be locked into.\n— Vicki Boykis: Commit to your lock-in\n\nI believe we need a ‘Digital Earth’. A multi-resolution, three-dimensional representation of the planet, into which we can embed vast quantities of georeferenced data.\n[…]\nImagine, for example, a young child going to a Digital Earth exhibit at a local museum. After donning a head-mounted display, she sees Earth as it appears from space. Using a data glove, she zooms in, using higher and higher levels of resolution, to see continents, then regions, countries, cities, and finally individual houses, trees, and other natural and man-made objects. Having found an area of the planet she is interested in exploring, she takes the equivalent of a ‘magic carpet ride’ through a 3-D visualization of the terrain. Of course, terrain is only one of the many kinds of data with which she can interact.\n[…]\nShe is able to request information on land cover, distribution of plant and animal species, real-time weather, roads, political boundaries, and population.\n— Al Gore: The Digital Earth: Understanding our Planet in the 21st Century, 1998\n\nTo consult the statistician after an experiment is ﬁnished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.\n— R.A. Fisher\n\nHiawatha, mighty hunter,\nHe could shoot ten arrows upward,\nShoot them with such strength and swiftness\nThat the last had left the bow-string\nEre the first to earth descended.\n- This was commonly regarded\nAs a feat of skill and cunning.\nSeveral sarcastic spirits\nPointed out to him, however,\nThat it might be much more useful\nIf he sometimes hit the target.\n“Why not shoot a little straighter\nAnd employ a smaller sample?”\nHiawatha, who at college\nMajored in applied statistics,\nConsequently felt entitled\nTo instruct his fellow man\nIn any subject whatsoever\n[…]\nHiawatha, in a temper,\nQuoted parts of R. A. Fisher,\nQuoted Yates and quoted Finney,\nQuoted reams of Oscar Kempthorne,\nQuoted Anderson and Bancroft\n(practically in extenso)\nTrying to impress upon them\nThat what actually mattered\nWas to estimate the error.\n- Several of them admitted:\n“Such a thing might have its uses;\nStill,” they said, “he would do better\nIf he shot a little straighter.”\n[…]\nIn a corner of the forest\nSits alone my Hiawatha\nPermanently cogitating\nOn the normal law of errors.\nWondering in idle moments\nIf perhaps increased precision\nMight perhaps be sometimes better\nEven at the cost of bias,\nIf one could thereby now and then\nRegister upon a target.\n— . E. Mientka, “Professor Leo Moser - Reflections of a Visit”\n\nFor in much wisdom is much grief: and he that increaseth knowledge increaseth sorrow.\n— Ecclesiastes 1:18\n\nAn extra year of experience has not changed my belief in a disconnect between traditional statistical regression methods and the pure prediction algorithms. Section 8 of the paper, featuring Table 5, makes the case directly in terms of six criteria. Five of the six emerged more or less unscathed from the discussion. Criteria 2, long-time scientific truth versus possibly short-term prediction accuracy, was doubted by FHT and received vigorous push-back from Yu/Barter:\n\n…but in our experience the “truth” that traditional regression methods supposedly represent is rarely justified or validated…\n\nThis is a hard-line Breimanian point of view. That “rarely” is over the top. The truism that begins “all models are wrong” ends with “but some are useful.” Traditional models tend to err on the side of over-simplicity (not enough interactions, etc.) but still manage to capture at least some aspect of the underlying mechanism. “Eternal truth” is a little too much to ask for, but in the Neonate example we did wind up believing that respiratory strength had something lasting to do with the babies’ survival.\n[…]\nThe fathers of statistical theory — Pearson, Fisher, Neyman, Hotelling, Wald — forgot to provide us with a comprehensive theory of optimal prediction. We will have to count on the current generation of young statisticians to fill the gap and put prediction on a principled foundation.\n— Bradley Efron: Rejoinder to Prediction, Estimation, and Attribution\n\nIt’s important to be realistic: most people don’t care about program performance most of the time. Modern computers are so fast that most programs run fast enough even with very slow language implementations. In that sense, I agree with Daniel’s premise: optimising compilers are often unimportant. But “often” is often unsatisfying, as it is here. Users find themselves transitioning from not caring at all about performance to suddenly really caring, often in the space of a single day.\nThis, to me, is where optimising compilers come into their own: they mean that even fewer people need care about program performance. And I don’t mean that they get us from, say, 98 to 99 people out of 100 not needing to care: it’s probably more like going from 80 to 99 people out of 100 not needing to care. This is, I suspect, more significant than it seems: it means that many people can go through an entire career without worrying about performance. Martin Berger reminded me of A N Whitehead’s wonderful line that “civilization advances by extending the number of important operations which we can perform without thinking about them” and this seems a classic example of that at work.\n— Laurence Tratt: What Challenges and Trade-Offs do Optimising Compilers Face?\n\nPay attention to unexpected data that has no natural constituency, and to lack of data that are in high demand.\n— Whitney R. Robinson: More on meta-epistemology: an epidemiologist’s perspective\n\nHalf of what you’ll learn in medical school will be shown to be either dead wrong or out of date within five years of your graduation; the trouble is that nobody can tell you which half–so the most important thing to learn is how to learn on your own.\n— David Sackett (playing off a common phrase)\n\nFile organization and naming are powerful weapons against chaos. (Jenny Bryan)\nYour closest collaborator is you six months ago, but you don’t reply to emails. (Mark Holder)\nI will let the data speak for itself when it cleans itself. (Allison Reichel)\nWorking with data is not about rules to follow but about decisions to make. (Naupaka Zimmerman)\nI’m not worried about being scooped, I’m worried about being ignored. (Magnus Nordborg)\nTeach stats as you would cake baking: make a few before you delve into the theory of leavening agents. (Jenny Bryan)\nThe opposite of “open” isn’t “closed”. The opposite of “open” is “broken”. (John Wilbanks)\n— Collected by Karl Broman\n\nThis was not meant to be an “emperor has no clothes” kind of story, rather “the emperor has nice clothes but they’re not suitable for every occasion.” Where they are suitable, the pure prediction algorithms can be stunningly successful. When one reads an enthusiastic AI-related story in the press, there’s usually one of these algorithms, operating in enormous scale, doing the heavy lifting. Regression methods have come a long and big way since the time of Gauss.\n— Bradley Efron: Prediction, Estimation, and Attribution.\n\nThere are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.\n— Leo Breiman: Statistical Modeling: The Two Cultures.\n\nPS Please excuse my not mailing this — but I don’t know your new address.\n— Richard Feynman: Letter to his dead wife, Arline, 17 Oct 1946\n\nThe more instructions something has, the worse its design. It’s cheaper to add instructions later than to design something well.\n— Scott Berkun: How Design Makes the World\n\nWe prove that last digits are approximately uniform for distributions with an absolutely continuous distribution function. From a practical perspective, that result, of course, is only moderately interesting. For that reason, we derive a result for ‘certain’ sums of lattice-variables as well. That justification is provided in terms of stationary distributions.\n— Stephan Dlugosz and Ulrich Muller-Funk: The value of the last digit: statistical fraud detection with digit analysis\n\nThe first of McIlroy’s dicta is often paraphrased as “do one thing and do it well”, which is shortened from “Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new ‘features.’”\nMcIlroy’s example of this dictum is:\n\nSurprising to outsiders is the fact that UNIX compilers produce no listings: printing can be done better and more flexibly by a separate program.\n\n[…]\nMcIlroy implies that the problem is that people didn’t think hard enough, the old school UNIX mavens would have sat down in the same room and thought longer and harder until they came up with a set of consistent tools that has “unusual simplicity”. But that was never going to scale, the philosophy made the mess we’re in inevitable. It’s not a matter of not thinking longer or harder; it’s a matter of having a philosophy that cannot scale unless you have a relatively small team with a shared cultural understanding, able to to sit down in the same room.\nIf anyone can write a tool and the main instruction comes from “the unix philosophy”, people will have different opinions about what “simplicity” or “doing one thing” means, what the right way to do things is, and inconsistency will bloom, resulting in the kind of complexity you get when dealing with a wildly inconsistent language, like PHP. People make fun of PHP and javascript for having all sorts of warts and weird inconsistencies, but as a language and a standard library, any commonly used shell plus the collection of widely used *nix tools taken together is much worse and contains much more accidental complexity due to inconsistency even within a single Linux distro and there’s no other way it could have turned out. If you compare across Linux distros, BSDs, Solaris, AIX, etc., the amount of accidental complexity that users have to hold in their heads when switching systems dwarfs PHP or javascript’s incoherence. The most widely mocked programming languages are paragons of great design by comparison.\nTo be clear, I’m not saying that I or anyone else could have done better with the knowledge available in the 70s in terms of making a system that was practically useful at the time that would be elegant today. It’s easy to look back and find issues with the benefit of hindsight. What I disagree with are comments from Unix mavens speaking today; comments like McIlroy’s, which imply that we just forgot or don’t understand the value of simplicity, or Ken Thompson saying that C is as safe a language as any and if we don’t want bugs we should just write bug-free code. These kinds of comments imply that there’s not much to learn from hindsight; in the 70s, we were building systems as effectively as anyone can today; five decades of collective experience, tens of millions of person-years, have taught us nothing; if we just go back to building systems like the original Unix mavens did, all will be well. I respectfully disagree.\n— Dan Luu: The growth of command line options, 1979-Present\n\nThis isn’t to say there’s no cost to adding options – more options means more maintenance burden, but that’s a cost that maintainers pay to benefit users, which isn’t obviously unreasonable considering the ratio of maintainers to users. This is analogous to Gary Bernhardt’s comment that it’s reasonable to practice a talk fifty times since, if there’s a three hundred person audience, the ratio of time spent watching to the talk to time spent practicing will still only be 1:6. In general, this ratio will be even more extreme with commonly used command line tools.\n— Dan Luu: The growth of command line options, 1979-Present\n\nobjects: Everything that exists in R is an object.\nfunctions: Everything that happens in R is a function call.\ninterfaces: Interfaces to other languages are a part of R.\n— John Chambers - S, R, and Data Science\n\nPHP is an embarrassment, a blight upon my craft. It’s so broken, but so lauded by every empowered amateur who’s yet to learn anything else, as to be maddening. It has paltry few redeeming qualities and I would prefer to forget it exists at all.\n[…]\nDo not tell me that “good developers can write good code in any language”, or bad developers blah blah. That doesn’t mean anything. A good carpenter can drive in a nail with either a rock or a hammer, but how many carpenters do you see bashing stuff with rocks? Part of what makes a good developer is the ability to choose the tools that work best.\n[…]\nPHP is built to keep chugging along at all costs. When faced with either doing something nonsensical or aborting with an error, it will do something nonsensical. Anything is better than nothing.\n— PHP: A fractal of bad design\n\nThe joy in mathematics is often the receding of the pain.\n[…]\nBob Morris Sr. asked me when I met what I do.\n“I study math.”\n“For whom?”\n[…]\nIt is difficult to get a man to understand something when his salary depends upon his not understanding it. People tend to pick their ideologies by function.\n— Halvar Flake - OffensiveCon 2020 Keynote\n\nData analysis is hard, and part of the problem is that few people can explain how to do it. It’s not that there aren’t any people doing data analysis on a regular basis. It’s that the people who are really good at it have yet to enlighten us about the thought process that goes on in their heads.\nImagine you were to ask a songwriter how she writes her songs. There are many tools upon which she can draw. We have a general understanding of how a good song should be structured: how long it should be, how many verses, maybe there’s a verse followed by a chorus, etc. In other words, there’s an abstract framework for songs in general. Similarly, we have music theory that tells us that certain combinations of notes and chords work well together and other combinations don’t sound good. As good as these tools might be, ultimately, knowledge of song structure and music theory alone doesn’t make for a good song. Something else is needed.\nIn Donald Knuth’s legendary 1974 essay Computer Programming as an Art, Knuth talks about the difference between art and science. In that essay, he was trying to get across the idea that although computer programming involved complex machines and very technical knowledge, the act of writing a computer program had an artistic component. In this essay, he says that\n\nScience is knowledge which we understand so well that we can teach it to a computer.\n\nEverything else is art.\nAt some point, the songwriter must inject a creative spark into the process to bring all the songwriting tools together to make something that people want to listen to. This is a key part of the art of songwriting. That creative spark is difficult to describe, much less write down, but it’s clearly essential to writing good songs. If it weren’t, then we’d have computer programs regularly writing hit songs. For better or for worse, that hasn’t happened yet.\nMuch like songwriting (and computer programming, for that matter), it’s important to realize that data analysis is an art. It is not something yet that we can teach to a computer.\n— Roger D. Peng and Elizabeth Matsui: The Art of Data Science\n\nThere are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models andtreats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.\n— Leo Breiman: Statistical Modeling: The Two Cultures\n\nAt first glance Leo Breiman’s stimulating paper looks like an argument against parsimony and scientific insight, and in favor of black boxes with lots of knobs to twiddle. At second glance it still looks that way, but the paper is stimulating, and Leo has some important points to hammer home\n[…]\nRule 1. New methods always look better than old ones. Neural nets are better than logistic regression, support vector machines are better than neural nets, etc. In fact it is very difficult to run an honest simulation comparison, and easy to inadvertently cheat by choosing favorable examples, or by not putting as much effort into optimizing the dull old standard as the exciting new challenger.\nRule 2. Complicated methods are harder to criticize than simple ones. By now it is easy to check the efficiency of a logistic regression, but it is no small matter to analyze the limitations of a support vector machine.\n— Brad Efron: Comment on Statistical Modeling: The Two Cultures\n\nOccasionally, one sees frequentism defined in careerist terms, e.g., “A statistician who always rejects null hypotheses at the 95% level will over time make only 5% errors of the first kind.” This is not a comforting criterion for the statistician’s clients.\n[…]\nSomething important changed in the world of statistics in the new millennium. Twentieth-century statistics, even after the heated expansion of itslate period, could still be contained within the classic Bayesian–frequentist–Fisherian inferential triangle (Figure 14.1). This is not so in the twenty-first century. Some of the topics discussed in Part III—false-discovery rates,post-selection inference, empirical Bayes modeling, the lasso—fit within the triangle but others seem to have escaped, heading south from the frequentist corner, perhaps in the direction of computer science.\n— Bradley Efron & Trevor Hastie: Computer Age Statistical Inference\n\nPeople sometimes think (or complain) that working with quantitative data like this inures you to the reality of the human lives that lie behind the numbers. Numbers and measures are crude; they pick up the wrong things; they strip out the meaning of what’s happening to real people; they make it easy to ignore what can’t be counted. There’s something to those complaints. But it’s mostly a lazy critique. In practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture—what they mean, and for whom. Those regular spikes in the driving data are the pulse of everyday life as people go out to have a good time at the weekend. That peak there is the Mardi Gras parade in New Orleans. That bump in Detroit was a Garth Brooks concert. Right across the country, that is the sudden shock of the shutdown the second weekend in March. It was a huge collective effort to buy time that, as it turns out, the federal government has more or less entirely wasted. And now through May here comes the gradual return to something like the baseline level of activity from January, proceeding much more quickly in some cities than in others.\nI sit at my kitchen-counter observatory and look at the numbers. Before my coffee is ready, I can quickly pull down a few million rows of data courtesy of a national computer network originally designed by the government to be disaggregated and robust, because they were convinced that was what it would take for communication to survive a nuclear war. I can process it using software originally written by academics in their spare time, because they were convinced that sophisticated tools should be available to everyone for free. Through this observatory I can look out without even looking up, surveying the scale and scope of the country’s ongoing, huge, avoidable failure. Everything about this is absurd.\n— Kieran Healy: The Kitchen Counter Observatory\n\nSurgisphere appears to be the Theranos, or possibly the Cornell Food and Brand Lab, of medical research, and Lancet is a serial enabler of research fraud (see this news article by Michael Hiltzik), and it’s easy to focus on that. But remember all the crappy papers these journals publish that don’t get retracted, cos they’re not fraudulent, they’re just crappy. Retracting papers just cos they’re crappy—no fraud, they’re just bad science—I think that’s never ever ever gonna happen. Retraction is taken as some kind of personal punishment meted out to an author and a journal. This frustrates me to no end. What’s important is the science, not the author. But it’s not happening. So, when we hear about glamorous/seedy stories of fraud, remember the bad research, the research that’s not evilicious but just incompetent, maybe never even had a chance of working. That stuff will stay in the published literature forever, and journals love publishing it.\nAs we say in statistics, the shitty is the enemy of the good.\n\nOpen code, open data, open review . . .\n\nSo, you knew I’d get to this…\nJust remember, honesty and transparency are not enuf. Open data and code don’t mean your work is any good. A preregistered study can be a waste of time. The point of open data and code is that it makes it easier to do post-publication review. If you’re open, it makes it easier for other people to find flaws in your work. And that’s a good thing.\nAn egg is just a chicken’s way of making another egg.\nAnd the point of science and policy analysis is not to build beautiful careers. The purpose is to learn about and improve the world.\n— Andrew Gelman: bla bla bla PEER REVIEW bla bla bla\n\nSometimes somebody says something to me, like a whisper of a hint of an echo of something half-forgotten, and it lands on me like an invocation. The mania sets in, and it isn’t enough to believe; I have to know.\n[…]\nSo: the technical reason we started counting arrays at zero is that in the mid-1960’s, you could shave a few cycles off of a program’s compilation time on an IBM 7094. The social reason is that we had to save every cycle we could, because if the job didn’t finish fast it might not finish at all and you never know when you’re getting bumped off the hardware because the President of IBM just called and fuck your thesis, it’s yacht-racing time.\nThere are a few points I want to make here.\nThe first thing is that as far as I can tell nobody has ever actually looked this up.\nWhatever programmers think about themselves and these towering logic-engines we’ve erected, we’re a lot more superstitious than we realize. We tell and retell this collection of unsourced, inaccurate stories about the nature of the world without ever doing the research ourselves, and there’s no other word for that but “mythology”. Worse, by obscuring the technical and social conditions that led humans to make these technical and social decisions, by talking about the nature of computing as we find it today as though it’s an inevitable consequence of an immutable set of physical laws, we’re effectively denying any responsibility for how we got here. And worse than that, by refusing to dig into our history and understand the social and technical motivations for those choices, by steadfastly refusing to investigate the difference between a motive and a justification, we’re disavowing any agency we might have over the shape of the future. We just keep mouthing platitudes and pretending the way things are is nobody’s fault, and the more history you learn and the more you look at the sad state of modern computing the the more pathetic and irresponsible that sounds.\n— mhoye: Citation Needed\n\nOf course, someone has to write for loops. It doesn’t have to be you.\n— Jenny Bryan: Row Oriented Workflows in R With the Tidyverse\n\nAn over-simplified and dangerously reductive diagram of a data system might look like this:\nCollection → Computation → Representation\nWhenever you look at data — as a spreadsheet or database view or a visualization, you are looking at an artifact of such a system. What this diagram doesn’t capture is the immense branching of choice that happens at each step along the way. As you make each decision — to omit a row of data, or to implement a particular database structure or to use a specific colour palette you are treading down a path through this wild, tall grass of possibility. It will be tempting to look back and see your trail as the only one that you could have taken, but in reality a slightly divergent you who’d made slightly divergent choices might have ended up somewhere altogether different. To think in data systems is to consider all three of these stages at once.\n— Jer Thorp: You Say Data, I Say System\n\nThe thing that is most alluring about the Gini coefficient also turns out to be its greatest shortcoming. By collapsing the whole rainbow of the income distribution into a single statistical point of white light, it necessarily conceals much of great interest. That is of course true of any single summary measure… The best measures are those that match our purpose, or pick up on the places where important changes are happening. We should pick and mix with that in mind.\n— Angus Deaton and Angela Case\n\nAfter describing the perverse interaction between wealth gaps, education, mortality trends and political economy, Case and Deaton note that “You could crunch the Gini coefficient to as many decimal places as you like, and you’d learn next to nothing about what’s really going on here.” Quite so, but surely it is unreasonable to demand of one measure of inequality along one dimension that it shed light on complex social interactions or detect causal relationships.\nThat would be like urging us to abandon the Centigrade scale as a measure of temperature because it so badly fails to inform us about how climate change plays havoc with rainfall patterns, the incidence of extreme weather events, or the extent of sea level rises. You could calculate average global temperature increases to as many decimal places in degrees Celsius as you liked, and you would be none the wiser about the diversity of consequences of climate change around the world.\n— Francisco Ferreira\n\nSoftware has been around since the 1940s. Which means that people have been faking their way through meetings about software, and the code that builds it, for generations. Now that software lives in our pockets, runs our cars and homes, and dominates our waking lives, ignorance is no longer acceptable. The world belongs to people who code. Those who don’t understand will be left behind. (Josh Tyrangiel, header)\n[…]\nA computer is a clock with benefits. They all work the same, doing second-grade math, one step at a time: Tick, take a number and put it in box one. Tick, take another number, put it in box two. Tick, operate (an operation might be addition or subtraction) on those two numbers and put the resulting number in box one. Tick, check if the result is zero, and if it is, go to some other box and follow a new set of instructions.\n[…]\nIt’s a good and healthy exercise to ponder what your computer is doing right now. Maybe you’re reading this on a laptop: What are the steps and layers between what you’re doing and the Lilliputian mechanisms within? When you double-click an icon to open a program such as a word processor, the computer must know where that program is on the disk. It has some sort of accounting process to do that. And then it loads that program into its memory—which means that it loads an enormous to-do list into its memory and starts to step through it. What does that list look like?\nMaybe you’re reading this in print. No shame in that. In fact, thank you. The paper is the artifact of digital processes. Remember how we put that “a” on screen? See if you can get from some sleepy writer typing that letter on a keyboard in Brooklyn, N.Y., to the paper under your thumb. What framed that fearful symmetry?\nThinking this way will teach you two things about computers:\nOne, there’s no magic, no matter how much it looks like there is. There’s just work to make things look like magic.\nAnd two, it’s crazy in there.\n[…]\nYou can tell how well code is organized from across the room. Or by squinting or zooming out. The shape of code from 20 feet away is incredibly informative. Clean code is idiomatic, as brief as possible, obvious even if it’s not heavily documented. Colloquial and friendly. As was written in Structure and Interpretation of Computer Programs (aka SICP), the seminal textbook of programming taught for years at MIT, “A computer language is not just a way of getting a computer to perform operations – it is a novel formal medium for expressing ideas about methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute.” A great program is a letter from current you to future you or to the person who inherits your code. A generous humanistic document.\nOf course all of this is nice and flowery; it needs to work, too.\n— Paul Ford: What is Code?\n\nThe Postulates of Mathematics Were Not on the Stone Tablets that Moses Brought Down from Mt. Sinai. It is necessary to emphasize this. We begin with a vague concept in our minds, then we create various sets of postulates, and gradually we settle down to one particular set. In the rigorous postulational approach, the original concept is now replaced by what the postulates define. This makes further evolution of the concept rather difficult and as a result tends to slow down the evolution of mathematics. It is not that the postulation approach is wrong, only that its arbitrariness should be clearly recognized, and we should be prepared to change postulates when the need becomes apparent.\n— Richard Hamming\n\nTeachers should prepare the student for the student’s future, not for the teacher’s past.\n[…]\nEducation is what, when, and why to do things. Training is how to do it.\n[…]\nIn science, if you know what you are doing, you should not be doing it. In engineering, if you do not know what you are doing, you should not be doing it. Of course, you seldom, if ever, see either pure state.\n—Richard Hamming: The Art of Doing Science and Engineering\n\nI note with fear and horror that even in 1980, language designers and users have not learned this lesson [mandatory run-time checking of array bounds]. In any respectable branch of engineering, failure to observe such elementary precautions would have long been against the law.\n[…]\nI conclude that there are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult. It demands the same skill, devotion, insight, and even inspiration as the discovery of the simple physical laws which underlie the complex phenomena of nature. It also requires a willingness to accept objectives which are limited by physical, logical, and technological constraints, and to accept a compromise when conflicting objectives cannot be met. No committee will ever do this until it is too late.\n— C.A.R. Hoare: The Emperor’s Old Clothes\n\nI like this definition: A tool addresses human needs by amplifying human capabilities. That is, a tool converts what we can do into what we want to do. A great tool is designed to fit both sides.\n— Bret Victor: A Brief Rant on the Future of Interaction Design\n\nDiscoverability is often cited as npm’s biggest flaw. Many blog posts – scratch that, entire websites – have been created to try and mitigate the difficulty of finding what you need on npm. Everyone has an idea about how to make it easier to find needles in the haystack, but no-one bothers to ask what all this hay is doing here in the first place.\n— Rich Harris: Small modules: it’s not quite that simple\n\nBrave to write it, all by itself, and then brave to show it. It is like opening your ribcage, and letting someone see the little bird you have inside. What if they don’t love the bird? It’s not like you can change it. I mean.. That’s your bird.\n— Tycho: Fan Fiction\n\nLook, you have two choices. You can say, “I’m a pessimist, nothing’s gonna work, I’m giving up, I’ll help ensure that the worst will happen.” Or you can grasp onto the opportunities that do exist, the rays of hope that exist, and say, “Well, maybe we can make it a better world.” It’s not a much of a choice.\n— Noam Chomskey: Interview\n\nThere’s really only one important question worth asking, which is: what is a life well-lived?… That’s a question that can’t be answered, but one thing we can say, with a lot of certainty, is that a life well-lived is not going to be a life in which every moment is scrutinized.\n— Frank Lantz: Hearts and Minds\n\nI hate abstraction. Here are some examples.\n— Adam Cadre: Fatal abstraction\n\nI find that the single thing which inhibits young professionals, new students most severely, is their acceptance of standards that are too low. If I ask a student whether her design is as good as Chartres, she often smiles tolerantly at me as if to say, “Of course not, that isn’t what I am trying to do…. I could never do that.”\nThen, I express my disagreement, and tell her: “That standard must be our standard. If you are going to be a builder, no other standard is worthwhile. That is what I expect of myself in my own buildings, and it is what I expect of my students.” Gradually, I show the students that they have a right to ask this of themselves, and must ask this of themselves. Once that level of standard is in their minds, they will be able to figure out, for themselves, how to do better, how to make something that is as profound as that.\n— Christopher Alexander: Foreward to Patterns of Software\n\nThe Stone Age didn’t end for lack of stone, and the Oil Age will end long before the world runs out of oil.\n— Sheik Ahmed Zaki Yamani\n\n[I told the fourth-graders] I was thinking of a number between 1 and 10,000. … They still cling stubbornly to the idea that the only good answer is a yes answer. This, of course, is the result of miseducation in which “right answers” are the only ones that pay off. They have not learned how to learn from a mistake, or even that learning from mistakes is possible. If they say, “Is the number between 5,000 and 10,000?” and I say yes, they cheer; if I say no, they groan, even though they get exactly the same amount of information in either case. The more anxious ones will, over and over again, ask questions that have already been answered, just for the satisfaction of hearing a yes.\n— John Holt: How Children Fail\n\nIan is a game design teacher and a professional skeptic. People call him a “curmudgeon”, but they don’t really understand how much love, how much actual faith, that kind of skepticism takes. On a pretty regular basis one of us will IM the other something like “help” or “fuck” or “people are terrible”.\nOnly when you fully believe in how wonderful something is supposed to be does every little daily indignity start to feel like some claw of malaise. At least, that’s how I explain Ian to other people.\n— Leigh Alexander: The Unearthing\n\nQ: So it’s fine to say, everybody should learn a little bit about how to program and this way of thinking because it’s valuable and important. But then maybe that’s just not realistic. Donald Knuth told me that he thinks two percent of the population have brains wired the right way to think about programming.\nA: That same logic would lead you to say that one percent of the US’s population is wired to understand Mandarin. The reasoning there is equivalent.\n— [Hal Abselson: Interview]\n\nMany professions require some form of programming. Accountants program spreadsheets; musicians program synthesizers; authors program word processors; and web designers program style sheets.\n[…]\nThe typical course on programming teaches a “tinker until it works” approach. When it works, students exclaim “It works!” and move on. Sadly, this phrase is also the shortest lie in computing, and it has cost many people many hours of their lives.\n[…]\nBy “good programming,” we mean an approach to the creation of software that relies on systematic thought, planning, and understanding from the very beginning, at every stage, and for every step. To emphasize the point, we speak of systematic program design and systematically designed programs. Critically, the latter articulates the rationale of the desired functionality. Good programming also satisfies an aesthetic sense of accomplishment; the elegance of a good program is comparable to time-tested poems or the black-and-white photographs of a bygone era. In short, programming differs from good programming like crayon sketches in a diner from oil paintings in a museum. No, this book won’t turn anyone into a master painter. But, we would not have spent fifteen years writing this edition if we didn’t believe that\neveryone can design programs\nand\neveryone can experience the satisfaction that comes with creative design.\n[…]\nWhen you were a small child, your parents taught you to count and perform simple calculations with your fingers: “1 + 1 is 2”; “1 + 2 is 3”; and so on. Then they would ask “what’s 3 + 2?” and you would count off the fingers of one hand. They programmed, and you computed. And in some way, that’s really all there is to programming and computing. Now it is time to switch roles.\n— How to Design Programs\n\nYou can’t sell someone the solution before they’ve bought the problem.\n— Chip Morningstar: Smart people can rationalize anything\n\nwhen you don’t create things, you become defined by your tastes rather than ability. your tastes only narrow & exclude people. so create\n— why the lucky stiff\n\nWhen you believe you have a future, you think in terms of generations and years. When you do not, you live not just by the day – but by the minute.\n— Iris Chang – Suicide Note\n\nIt seems like most people ask: “How can I throw my life away in the least unhappy way?”\n— Stewart Brand\n\nQ: Are you ever afraid of someone stealing your thunder?\nA: I think anybody really good is going to want to do their own thing. Anybody who’s not really good, you don’t have to worry too much about.\n— Chris Hecker: Interview\n\nLiving organisms are shaped by evolution to survive, not necessarily to get a clear picture of the universe. For example, frogs’ brains are set up to recognize food as moving objects that are oblong in shape. So if we take a frog’s normal food – flies – paralyze them with a little chloroform and put them in front of the frog, it will not notice them or try to eat them.\nIt will starve in front of its food! But if we throw little rectangular pieces of cardboard at the frog it will eat them until it is stuffed! The frog only sees a little of the world we see, but it still thinks it perceives the whole world.\nNow, of course, we are not like frogs! Or are we?\n— Alan Kay: The Center of “Why?”\n\nPick a plane or a cave wall to project the shadow of the Real World onto, and tell a story about the outlines it makes. The trick is to shrug and smile and pick another plane and do it all again to get a completely different shadow, until you find the one most useful for the day. It’s a magic trick for most folks.\n— Down is just the most common way out\n\nTo an architect, imagination is mostly about the future. To invent the future, one must live in it, which means living (at least partly) in a world that does not yet exist. Just as a driver whizzing along a highway pays more attention to the front window than the rear, the architect steers by looking ahead. This can sometimes make them seem aloof or absent-minded, as if they are someplace else. In fact, they are. For them, the past is a lesson, the present is fleeting; but the future is real. It is infinite and malleable, brimming with possibility.\n— Danny Hillis: The Power of Conviction\n\nIf the first line of your [R] script is setwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\"), I will come into your lab and SET YOUR COMPUTER ON FIRE.\n— Jenny Bryan: here, here\n\nIt becomes important in such a climate of opinion to emphasize that books do not store knowledge. They contain symbolic codes that can serve us as external mnemonics for knowledge. Knowledge can exist only in living human minds.\n— Kieran Egan: The Educated Mind: How Cognitive Tools Shape Our Understanding\n\nPeople are much smarter when they can use their full intellect and when they can relate what they are learning to situations or phenomena which are real to them. The natural reaction, when someone is having trouble understanding what you are explaining, is to break up the explanation into smaller pieces and explain the pieces one by one. This tends not to work, so you back up even further and fill in even more details.\nBut human minds do not work like computers: it is harder, not easier, to understand something broken down into all the precise little rules than to grasp it as a whole. It is very hard for a person to read a computer assembly language program and figure out what it is about…\nStudying mathematics one rule at a time is like studying a language by first memorizing the vocabulary and the detailed linguistic rules, then building phrases and sentences, and only afterwards learning to read, write, and converse. Native speakers of a language are not aware of the linguistic rules: they assimilate the language by focusing on a higher level, and absorbing the rules and patterns subconsciously. The rules and patterns are much harder for people to learn explicitly than is the language itself.\n— William Thurston: Mathematical Education\n\nA lot of the stuff going on [in AI] is not very ambitious. In machine learning, one of the big steps that happened in the mid-’80s was to say, “Look, here’s some real data – can I get my program to predict accurately on parts of the data that I haven’t yet provided to it?” What you see now in machine learning is that people see that as the only task.\n— Stuart Russell\n\nLike most readers, I had functionally consigned [our game] to the furnace. I had let it float away on one of those little lantern boats in a way that brought me closure, if no one else. Insufficient.\nFucking insufficient.\nYou have to get back on the horse. Somehow, and I don’t know how this kind of thing starts, we have started to lionize horseback-not-getting-on: these casual, a priori assertions of inevitable failure, which is nothing more than a gauze draped over your own pulsing terror. Every creative act is open war against The Way It Is. What you are saying when you make something is that the universe is not sufficient, and what it really needs is more you. And it does, actually; it does. Go look outside. You can’t tell me that we are done making the world.\n— Tycho: A Matter of Scale\n\nQ: At the [NYU] Game Center, we’re interested in the role of the university as an alternate place for thinking about games… What in your opinion are some of the big interesting problems that students should be working on?\nA: My advice for students is… I question the question. I don’t think there are problems that students should be working on. I think students should be making games that are interesting and push the boundaries, and those will generate the problems.\n— Chris Hecker\n\nA complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.\n— John Gall: Systemantics\n\nWe have this limited bubble of experience. We can only have so many experiences in our lifetime to build models from, and we’re abstracting from that data. We’ve found, through evolution, two ways to get more data, to build more elaborate models of the world. One is to have toy experiences, little counterfeit experiences. The other one is to learn from the experience of others. When somebody tells you a story, you can actually learn from that story, incorporate it into your model of the world to make your model more accurate based upon that data that you got from somebody else. So over time, we have come to call one of these things “play” and the other one “storytelling”. These are both fundamentally educational technologies that allow us to build more elaborate models of the world around us, by supplanting our limited experience with other experiences.\n— Will Wright: Gaming Reality\n\nJohn [McCarthy]’s world is a world of ideas, a world in which ideas don’t belong to anyone, and when an idea is wrong, just the idea - not the person - is wrong. A world in which ideas are like young birds, and we catch them and proudly show them to our friends. The bird’s beauty and the hunter’s are distinct….\nSome people won’t show you the birds they’ve caught until they are sure, certain, positive that they - the birds, or themselves - are gorgeous, or rare, or remarkable. When your mind can separate yourself from your bird, you will share it sooner, and the beauty of the bird will be sooner enjoyed. And what is a bird but for being enjoyed?\n— Richard Gabriel: The Design of Parallel Programming Languages"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Mike Mahoney",
    "section": "",
    "text": "Papers\n\nClick “[PDF]” to download each paper.\n\n\nIn Review\n\n\nJohnson, L. K., Mahoney, M. J., Bevilacqua, E., Stehman, S. V., Domke, G. M., and Beier, C. M. In Review. High-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages. In review at Environmental Research Letters. https://doi.org/10.48550/arXiv.2205.08530 [PDF]\n\n\nMahoney, MJ, Johnson, J. K., and Beier, C. M. In Review. Classification and mapping of low-statured ‘shrubland’ cover types in post-agricultural landscapes of the US Northeast. In review at ISPRS Journal of Photogrammetry and Remote Sensing. https://doi.org/10.48550/arXiv.2205.05047 [PDF]\n\n\n2022\n\n\nMahoney, MJ, Johnson, J. K., Bevilacqua, E., and Beier, C. M. In Review. Ground noise filtering produces inferior models of forest aboveground biomass. In press at GIScience and Remote Sensing. https://doi.org/10.31223/X5HG99 [PDF]\n\n\nMahoney, MJ, Beier, CM, and Ackerman, AC. 2022. unifir: A Unifying API for Interacting with Unity from R. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388 [PDF]\n\n\nTamiminia, H., Salehi, B., Mahdianpari, M., Beier, C. M., Johnson, L. K., Phoenix, D. B., and, Mahoney, M. J. 2022. Decision tree-based machine learning models for above-ground biomass estimation using multi-source remote sensing data and object-based image analysis. Geocarto International. https://doi.org/10.1080/10106049.2022.2071475\n\n\nMahoney, MJ, Beier, CM, and Ackerman, AC. 2022. terrainr: An R package for creating immersive virtual environments. Journal of Open Source Software 7(69): 4060. https://doi.org/10.21105/joss.04060 [PDF]\n\n\n2021\n\n\nMahoney, MJ, Beier, CM, and Ackerman, AC. 2021. Interactive landscape simulations for visual resource assessment. VRSC 2021 Conference Proceedings. [PDF]\n\n\n2020\n\n\nMahoney, MJ, and Stella, JC. 2020. Stem size selectivity is stronger than species preferences for beaver, a central place forager. Forest Ecology and Management 475: 118331. https://doi.org/10.1016/j.foreco.2020.118331 [PDF]"
  },
  {
    "objectID": "papers/ground_filtering/index.html",
    "href": "papers/ground_filtering/index.html",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass",
    "section": "",
    "text": "See the poster!\n\nAirborne LiDAR has become an essential data source for large-scale, high-resolution modeling of forest biomass and carbon stocks, enabling predictions with much higher resolution and accuracy than can be achieved using optical imagery alone. Ground noise filtering – that is, excluding returns from LiDAR point clouds based on simple height thresholds – is a common practice meant to improve the ‘signal’ content of LiDAR returns by preventing ground returns from masking useful information about tree size and condition contained within canopy returns. Although this procedure originated in LiDAR-based estimation of mean tree and canopy height, ground noise filtering has remained prevalent in LiDAR pre-processing, even as modelers have shifted focus to forest aboveground biomass (AGB) and related characteristics for which ground returns may actually contain useful information about stand density and openness. In particular, ground returns may be helpful for making accurate biomass predictions in heterogeneous landscapes that include a patchy mosaic of vegetation heights and land cover types.\nIn this paper, we applied several ground noise filtering thresholds while mapping two study areas in New York (USA), one a forest-dominated area and the other a mixed-use landscape. We observed that removing ground noise via any height threshold systematically biases many of the LiDAR-derived variables used in AGB modeling. By fitting random forest models to each of these predictor sets, we found that that ground noise filtering yields models of forest AGB with lower accuracy than models trained using predictors derived from unfiltered point clouds. The relative inferiority of AGB models based on filtered LiDAR returns was much greater for the mixed land-cover study area than for the contiguously forested study area. Our results suggest that ground filtering should be avoided when mapping biomass, particularly when mapping heterogeneous and highly patchy landscapes, as ground returns are more likely to represent useful ‘signal’ than extraneous ‘noise’ in these cases.\n\n\n\n\n\n\nFigure 1: Distributions of common LiDAR-derived metrics (including density percentiles, decile heights, L-moments, and quadratic mean height) for the pooled dataset at various levels of ground noise filtering. Filtering reduces the variance in many metrics, reducing the total amount of information available to models.\n\n\n\n\n\n\nFigure 2: Height threshold-based filtering of LiDAR returns produces inferior models across all landscape types, with more notable impacts in mixed-use landscapes"
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html",
    "href": "papers/ground_filtering_manuscript/index.html",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "",
    "text": "1 Graduate Program in Environmental Science, State University of New York College of Environmental Science and Forestry, 1 Forestry Drive, Syracuse, NY, 13210\n2 Department of Sustainable Resources Management, State University of New York College of Environmental Science and Forestry, 1 Forestry Drive, Syracuse, NY, 13210"
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#sec:sites",
    "href": "papers/ground_filtering_manuscript/index.html#sec:sites",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "LiDAR Data Sets and Site Characteristics",
    "text": "LiDAR Data Sets and Site Characteristics\nIn order to identify the impacts of ground filtering on predictive models of forest AGB, we obtained sixteen separate leaf-off LiDAR data sets for areas within New York State (USA; Figure 1). We refer to these data sets as representing different “regions”, though these LiDAR regions do not align with administrative or ecological boundaries. This data, collected as part of a number of cross-agency federal initiatives, resembles the relatively low-density and leaf-off LiDAR relied upon in similar forest AGB modeling work (see for instance Nilsson et al. (2017), Huang et al. (2019)), and closely resembles the remote sensing data used in typical modeling practice. Data had pulse densities between 1.98 and 3.24 points per square meter. All LiDAR data had a vertical accuracy RMSE of \\(\\leq\\) 10 cm. While horizontal accuracy was not typically reported, expected horizontal RMSE for all data sets would be \\(\\leq\\) 68 cm based upon sensor altitude, GNSS positional error, and INS angular error (ASPRS, 2014). Where regions overlapped (as shown by overlapping boundary lines in Figure 1), data representing the most recent LiDAR collection was used. Additional information about individual LiDAR data sets is included as Supplementary Materials S1.\n\n\n\n\n\nFigure 1: Locations of LiDAR regions within New York State. Overlapping boundary lines represent overlapping data sets; in areas where LiDAR regions overlapped one another, the most recent LiDAR collection was used. More information about each region and LiDAR data set is included as Supplementary Materials S1."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#sec-field",
    "href": "papers/ground_filtering_manuscript/index.html#sec-field",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Field Data",
    "text": "Field Data\nField measurements of AGB for all trees measuring \\(\\geq\\) 12.7 cm (5 in) diameter at breast height were taken as part of the United States Department of Agriculture (USDA) Forest Inventory and Analysis (FIA) program (Gray et al., 2012). The FIA provides data from a standardized forest inventory, with field plots on forested land being resampled on a rolling seven-year basis. FIA plots are composed of four identical circular subplots with radii of 7.32 m (24 ft), with one subplot centered at the center of the plot and the other three subplot centers located 36.6 m (120 ft) away at azimuths of 360\\(^{\\circ}\\), 120\\(^{\\circ}\\), and 240\\(^{\\circ}\\). We obtained true plot centroid locations under agreement with the USDA, and used data aggregated from subplots to the plot level for all analyses and models. LiDAR data was clipped to only the measured subplot areas, with subplot locations estimated based upon the macroplot centroid, and then pooled prior to predictor derivation. Limitations of this data source include the exclusion of trees below 12.7 cm diameter, which likely results in underestimates of forest AGB, particularly in younger forests and more fragmented landscapes, and the relatively high positional inaccuracy of FIA plot locations (reported in 2005 to average approximately 5 m) (Hoppus and Lister, 2005). These limitations make associating FIA field measurements of forest AGB with specific LiDAR data difficult, particularly given the high positional inaccuracy of FIA plot locations compared to LiDAR data collections (ASPRS, 2014). Despite these limitations, however, the FIA remains an essential source for forest AGB modeling, and we follow the same procedures as the majority of USA-focused remote sensing forest AGB modeling studies (Chen et al., 2016; Huang et al., 2019; Hurtt et al., 2019; Johnson et al., 2022).\nPlots entirely classified as nonforest (which are not assigned AGB by the FIA) were excluded from the dataset. Only FIA plots sampled the same year as LiDAR flights, or FIA plots with measurements both before and after the LiDAR acquisition year with a difference in AGB \\(\\geq\\) -5% (to allow for forest growth or small-scale disturbance) were used for training and evaluating models. In situations where FIA year did not match LiDAR acquisition year, forest AGB was calculated by linearly interpolating between the values measured in the temporally closest FIA samples. Plots were additionally excluded if any subplots were marked as nonsampled, if FIA measurements indicated 0 \\(\\operatorname{Mg\\ ha^{-1}}\\) of forest AGB but maximum LiDAR return heights at the plot exceeded 10 meters, or if the convex hull of all LiDAR returns for a subplot contained less than 90% of the subplot’s area. This methodology was chosen to closely resemble the existing literature on forest AGB mapping (see for instance Huang et al. (2019)). Forest AGB measurements were recorded in pounds, then converted and area-normalized to units of megagrams per hectare (\\(\\operatorname{Mg\\ ha^{-1}}\\))."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#lidar-pre-processing",
    "href": "papers/ground_filtering_manuscript/index.html#lidar-pre-processing",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "LiDAR Pre-Processing",
    "text": "LiDAR Pre-Processing\nA digital terrain model (DTM) was calculated for all sites using a k-nearest-neighbors inverse-distance weighting imputation algorithm (using k = 5) as implemented in the lidR R package (Roussel et al., 2020), fit using the points classified as “ground” within the raw LiDAR point cloud data set. The calculated terrain was then subtracted from each point’s z value to create a height-normalized point cloud. Ground noise filtering rules were then applied to create five separate points clouds for each site, each representing a different ground noise filtering approach: one point cloud containing all points in the original file (hereafter referred to as “unfiltered”), one removing all points classified as “ground” in the original metadata (“ground”, equivalent to a 0m threshold), and three removing all points with normalized z values below a 0.1, 1, or 2 meter threshold (“0.1m”, “1m”, and “2m”, respectively). This process is shown as a schematic in Figure 2.\n\n\n\n\n\nFigure 2: A diagram representing the LiDAR pre-processing workflow.\n\n\n\n\nSeparate sets of 40 predictors, chosen due to their prevalence in published models of AGB and forest structure, were derived from each of these point clouds using the lidR R package (Table 1) (Hawbaker et al., 2010; Huang et al., 2019; Pflugmacher et al., 2014, 2012; Roussel et al., 2020). Predictors computed for FIA plot locations were derived from only the pooled returns coincident with the sampled subplot locations, so as to not include any returns from the unsampled regions of the macroplot. For plots where ground noise filtering resulted in the removal of all points, variables were set to a default value of 0. As highly correlated predictor variables may provide the random forest model less information for forest AGB predictions, relationships between predictors were assessed using Spearmans’s correlation coefficient. Changes in predictor distributions under different filtering methodologies were assessed using Kolmogorov-Smirnov statistics (Massey, 1951).\n\n\n\n\nTable 1:  Definitions of LiDAR-derived predictors used for model fitting. \n \n  \n    Predictor \n    Definition \n  \n \n\n  \n    H0, H10, ... H100, H95, H99 \n    Decile heights of returns, in meters, as well as 95th and 99th percentile return heights. \n  \n  \n    D10, D20... D90 \n    Density of returns above a certain height, as a proportion. After return height is divided into 10 equal bins ranging from 0 to the maximum height of returns, this value reflects the proportion of returns at or above each breakpoint. \n  \n  \n    N \n    Number of LiDAR returns clipped to the given FIA plot or map pixel \n  \n  \n    ZMEAN, ZMEAN_C \n    Mean height of all returns (ZMEAN) and all returns above 2.5m (ZMEAN_C) \n  \n  \n    Z_KURT, Z_SKEW \n    Kurtosis and skewness of height of all returns \n  \n  \n    QUAD_MEAN, QUAD_MEAN_C \n    Quadratic mean height of all returns (QUAD_MEAN) and all returns above 2.5m (QUAD_MEAN_C) \n  \n  \n    CV, CV_C \n    Coefficient of variation for heights of all returns (CV) and all returns above 2.5m (CV_C) \n  \n  \n    L2, L3, L4, L_CV, L_SKEW, L_KURT \n    L-moments and their ratios as defined by Hosking (1990), calculated for heights of all returns \n  \n  \n    CANCOV \n    Ratio of returns above 2.5m to all returns (Pflugmacher et al. 2012) \n  \n  \n    HVOL \n    CANCOV * ZMEAN (Pflugmacher et al. 2012) \n  \n  \n    RPC1 \n    Ratio of first returns to all returns (Pflugmacher et al. 2012)"
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#model-fitting",
    "href": "papers/ground_filtering_manuscript/index.html#model-fitting",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Model Fitting",
    "text": "Model Fitting\nModels were fit using the ranger R package’s implementation of the random forest algorithm (Breiman, 2001; Wright and Ziegler, 2017a), a popular machine learning technique for predicting forest aboveground biomass across landscapes (see for instance Huang et al., 2019; Hudak et al., 2020). Separate models were fit on predictors calculated using each level of ground noise filtering (“unfiltered”, “ground”, “0.1m”, “1m”, and “2m” thresholds) for each LiDAR region and a combination of all LiDAR regions, for a total of 85 separate models. Each model used data representing all available FIA plots within the relevant LiDAR region (Section 2.2). Models were fit using only LiDAR derived predictors, as it was expected that including non-LiDAR derived variables might mediate and confound the impacts of ground noise filtering.\nEach of these models were tuned separately using a standard uniform grid search, with each model evaluated using the same 8,892 combinations of hyperparameters detailed in Supplementary Materials S2. Models from this set were ranked on the basis of mean root mean squared error (RMSE) from 5-fold cross validation (Stone, 1974) (Equation 5), with 5 folds chosen to reduce computational demands. In order to ensure the best model was chosen for each combination, the top 100 models as determined from 5-fold cross validation were then evaluated again using leave-one-out cross validation (Lachenbruch and Mickey, 1968), with the final model fit using the hyperparameter set with the lowest RMSE. This method ensured that each random forest compared is the best version of the model that could be fit to these predictors, with the intention that any difference in model performance will be due to ground noise filtering and not stochastic differences between models or effort spent in tuning hyperparameters. Recent work has suggested cross validation assessments of model accuracy are likely overoptimistic compared to actual predictive accuracy (Bates et al., 2021), which does not impact our aim of comparing ground noise filtering approaches within a single study, but should be kept in mind when assessing these models as forest AGB estimators in their own right.\nAll modeling work was done using R version 4.0.5 (R Core Team, 2021), using the dplyr (Wickham et al., 2022a), landscapemetrics (Hesselbarth et al., 2019; McGarigal and Marks, 1995), nlme (Pinheiro and Bates, 2000), purrr (Henry and Wickham, 2020), ranger (Wright and Ziegler, 2017b), raster (Hijmans, 2022a), readr (Wickham et al., 2022b), terra (Hijmans, 2022b), tibble (Müller and Wickham, 2022), and tidyr (Wickham and Girlich, 2022) packages."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#sec-landscapemetrics",
    "href": "papers/ground_filtering_manuscript/index.html#sec-landscapemetrics",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\nThe LiDAR regions included in this study represent a diversity of landscapes, including both highly developed regions and large swaths of contiguous forest (Figure 3). While all field data was collected at plots located entirely within forested areas, forest structure and composition is highly affected by the surrounding landscape matrix (Franklin, 2006; Watling, 2018). In particular, edge effects (the impacts of adjacent nonforest environments) influence forest structure and composition in a number of ways, typically increasing species richness, the abundance of non-native species, shrub and herbaceous cover, and understory stem and foliage density while additionally increasing tree mortality and decreasing canopy tree abundance and canopy cover (Harper et al., 2005), with some of these effects remaining significant over 500 m from the edge itself (Murcia, 1995). The net effect of these impacts is that forests near a forest edge are consistently characterized by more openings in the canopy and more biomass in the understory. As a result, it stands to reason that the impacts of ground noise filtering on model performance may be associated with landscape structure, and that in particular model accuracy would be most impacted in forests located closer to a forest edge, a more common occurrence in highly fragmented landscapes (as characterized by increased patch and edge density).\n\n\n\n\n\nFigure 3: Land cover classifications across LiDAR regions, using land cover classifications from LCMAP (Brown et al., 2020). Lines represent LiDAR data set boundaries. In areas where LiDAR data sets overlapped, LCMAP matching the acquisition year of the newest LiDAR data was used.\n\n\n\n\nAs such, we investigated how changes in model accuracy due to ground noise filtering varied with differences in landscape structure. Landscape structure was quantified for each LiDAR region using temporally matching land use/land cover classifications from USGS LCMAP (Brown et al., 2020). We computed the proportion of 30 m pixels classified as forest (Equation 1) and the topographic ruggeddness index (TRI, Equation 2); DeGloria and Elliot (1999)) using the terra R package (Hijmans, 2022b), as well as edge density (Equation 3) in units of meter per hectare and patch density (Equation 4) in units of number of patches per 100 hectares for each individual LiDAR region using the landscapemetrics R package (Hesselbarth et al., 2019; McGarigal and Marks, 1995).\n\\[\n\\operatorname{Forest\\ Cover\\ \\%} = \\frac{F}{A}\n\\tag{1}\\]\n\\[\n\\operatorname{TRI} = \\frac{\\sum_{i=1}^{8}{\\left|x - x_{i}\\right|}}{8}\n\\tag{2}\\]\n\\[\n\\operatorname{Edge\\ Density} = \\frac{E}{A} \\cdot 10000\n\\tag{3}\\]\n\\[\n\\operatorname{Patch\\ Density} = \\frac{N}{A} \\cdot 10000 \\cdot 100\n\\tag{4}\\]\nWhere \\(F\\) is the area classified as forest in square meters, \\(A\\) the total landscape area in square meters, \\(x\\) the elevation of a grid cell and \\(x_{i}\\) the elevation of its eight neighbors, \\(E\\) the total landscape edge in meters, and \\(N\\) the number of patches in the region. For the purposes of this study, patches were defined as contiguous areas of 30m pixels assigned the same land cover classification by LCMAP (Brown et al., 2020), and edges defined as the perimeters of these patches."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#sec-asst",
    "href": "papers/ground_filtering_manuscript/index.html#sec-asst",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Model Assessment",
    "text": "Model Assessment\nModels were evaluated using multiple performance metrics in order to capture the variety of ways model performance can vary. Performance metrics calculated included mean absolute error (MAE, Equation 7), which captures the mean magnitude of errors across all observations, root-mean-squared error in \\(\\operatorname{Mg\\ ha^{-1}}\\) (RMSE, Equation 5), which weights larger errors more heavily than MAE, root-mean-squared error as a percentage of mean plot forest AGB (RMSE %, Equation 6),\nwhich allows for direct comparison of RMSE across regions with differing AGB distributions, and the coefficient of determination (\\(R^2\\), Equation 8), which measures the strength of the linear association between FIA measurements and model predictions (but does not directly reflect predictive accuracy). Given the scarcity of field data available for some LiDAR regions, metrics were calculated via leave-one-out cross validation (Lachenbruch and Mickey, 1968).\n\\[\n\\operatorname{RMSE} = \\sqrt{(\\frac{1}{n})\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}\n\\tag{5}\\]\n\\[\n\\operatorname{RMSE\\ \\%} = 100 \\cdot \\frac{\\operatorname{RMSE}}{\\bar{y}}\n\\tag{6}\\]\n\\[\n\\operatorname{MAE} = (\\frac{1}{n})\\sum_{i=1}^{n}\\left | y_{i} -\\hat{y_{i}} \\right |\n\\tag{7}\\]\n\\[\n\\operatorname{R^2} = 1 - \\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^2}{\\sum_{i=1}^{n}\\left(y_i - \\bar{y}\\right)^2}\n\\tag{8}\\]\nWhere \\(n\\) is the number of FIA plots included in the data set, \\(\\hat{y_i}\\) is the predicted value of forest AGB, \\(y_{i}\\) the forest AGB value measured at the corresponding location, and \\(\\bar{y}\\) the mean forest AGB value from FIA field measurements. The difference in model performance across filtering thresholds was quantified through a linear mixed-effect model representing plot absolute error (\\(|y_i - \\hat{y}_i|\\)) as a function of filtering threshold as a fixed effect with LiDAR region as a random effect, fit using the nlme R package (Pinheiro and Bates, 2000). Lastly, the relationship between changes in model accuracy due to ground noise filtering and each landscape structure metric (Section 2.5) was measured using Spearman’s correlation coefficient (\\(\\rho\\))."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#landscape-structure",
    "href": "papers/ground_filtering_manuscript/index.html#landscape-structure",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Landscape Structure",
    "text": "Landscape Structure\n\n\n\nEdge density ranged from 38.73 to 100.17 meters per hectare, patch density from 8.63 to 23.70 patches per 100 hectares, forest coverage from 15.38% to 83.29%, and TRI ranged from 0.33 to 3.34. of each LiDAR region (Figure 4). Edge density and patch density were strongly positively correlated (Spearman’s \\(\\rho\\) = 0.953), as were forest coverage and TRI (Spearman’s \\(\\rho\\) = 0.741). LiDAR regions had between 9 and 126 FIA plots available for models after applying plot inclusion rules, for a total of 874 plots in the combined data set (Table 3).\n\n\n\n\n\nFigure 4: Landscape fragmentation metrics and forest cover percentage derived from LCMAP LULC classifications, and topographic ruggedness index (TRI) derived from a digital terrain model, for all LiDAR regions used in this project at year of LiDAR acquisition."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#variable-distribution",
    "href": "papers/ground_filtering_manuscript/index.html#variable-distribution",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Variable Distribution",
    "text": "Variable Distribution\nFiltering out ground noise resulted in shifts in predictor distributions (Figure 5). Filtering returns based upon z-thresholds or ground classifications resulted in systematically elevated height percentile and return density predictors (the H and D prefixed predictors in Table 1; Figure 5), with differences persisting into the highest percentiles. Notable differences in distributions also existed for all L-moment based predictors, with increasing height thresholds associated with increased magnitude of difference. Mean predictor values for each ground noise filtering method, alongside Kolmogorov-Smirnov test statistic values comparing the distributions of filtered predictors to that of the unfiltered predictors, are presented in Supplementary Materials S3. Shifts in predictor distributions resulted in changes to covariance among variables, as measured via Spearman correlation coefficients. More aggressive filtering approaches were generally associated with stronger positive correlations and collinearity between all variables (Figure 6).\n\n\n\n\n\nFigure 5: Selected LiDAR-derived predictor distributions for five ground noise filtering approaches, using all LiDAR regions combined. Each subplot is scaled independently so that the X-axis represents the full range of that predictor and the Y-axis represents the full range of the kernel density estimate of that predictor.\n\n\n\n\n\n\n\n\n\nFigure 6: Mean Spearman correlation coefficients between LiDAR-derived variables calculated from point clouds processed with five different ground noise filtering methodologies across the combined data set. Variables with standard deviations of 0 after filtering (such as when minimum return height at all plots became 0 due to ground noise filtering) were excluded from calculations."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#model-performance",
    "href": "papers/ground_filtering_manuscript/index.html#model-performance",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Model Performance",
    "text": "Model Performance\nModels fit to the unfiltered set of predictors were almost always equally or more accurate than those fit to predictors derived from filtered point clouds (Figure 7, Table 2, Table 3). Model accuracy generally decreased as filtering thresholds increased, with RMSE % for models fit to all regions combined increasing from 37.18% when using the unfiltered data set to 39.06% when using a threshold of 2 meters (Figure 7). An exception to this pattern was the Erie, Genesee, & Livingston LiDAR region, which saw improvements in accuracy with filtering procedures; this is likely related to the small sample size available for this region (with only 9 FIA plots available for models) making this region highly susceptible to small changes in the predictor space or hyperparameter space. In the linear mixed model representing plot absolute error as a function of filtering thresholds, with LiDAR region as a random effect, all thresholds had positive coefficients, indicating an increase in the magnitude of errors at the plot level (Table 4).\n\n\n\n\n\nFigure 7: Model accuracy metrics at each ground noise filtering height threshold. Red line indicates models fit to all LiDAR regions (874 FIA plots), while grey lines represent each individual LiDAR region model with more than 10 FIA plots. Metrics are defined in Section 2.5.\n\n\n\n\n\n\n\n\nTable 2:  Model accuracy metrics for the model fit to the combined data set at various ground filtering height thresholds. The complete set of model accuracy metrics for all LiDAR regions is included as Supplementary Materials S4. \n \n  \n      \n    Unfiltered \n    0m \n    0.1m \n    1m \n    2m \n  \n \n\n  \n    RMSE \n    43.826 \n    45.608 \n    46.622 \n    45.734 \n    46.044 \n  \n  \n    RMSE (%) \n    37.177 \n    38.689 \n    39.548 \n    38.795 \n    39.058 \n  \n  \n    MAE \n    33.560 \n    35.048 \n    35.974 \n    35.271 \n    35.540 \n  \n  \n    R2 \n    0.609 \n    0.577 \n    0.558 \n    0.574 \n    0.568 \n  \n\n\n\n\n\n\n\n\n\n\nTable 3:  RMSE for each LiDAR region at various ground filtering height thresholds. The complete set of model accuracy metrics for all LiDAR regions is included as Supplementary Materials S4. \n \n  \n    Region \n    # Plots \n    Unfiltered \n    0m \n    0.1m \n    1m \n    2m \n  \n \n\n  \n    All Regions \n    874 \n    43.826 \n    45.608 \n    46.622 \n    45.734 \n    46.044 \n  \n  \n    Allegany & Steuben \n    38 \n    43.478 \n    43.102 \n    42.702 \n    44.589 \n    44.577 \n  \n  \n    3 County \n    117 \n    49.238 \n    50.479 \n    53.164 \n    52.394 \n    53.238 \n  \n  \n    Cayuga & Oswego \n    19 \n    23.873 \n    39.584 \n    34.126 \n    36.687 \n    39.947 \n  \n  \n    Clinton, Essex & Franklin \n    126 \n    37.255 \n    39.742 \n    40.821 \n    39.135 \n    38.952 \n  \n  \n    Columbia & Rensselaer \n    23 \n    42.689 \n    39.721 \n    43.885 \n    48.731 \n    51.126 \n  \n  \n    Erie, Genesee & Livingston \n    9 \n    56.942 \n    51.461 \n    30.960 \n    32.279 \n    49.731 \n  \n  \n    Franklin & St. Lawrence \n    113 \n    36.818 \n    37.411 \n    38.121 \n    38.538 \n    38.143 \n  \n  \n    Fulton, Saratoga, Herkimer & Franklin \n    47 \n    37.840 \n    40.823 \n    39.105 \n    36.496 \n    37.610 \n  \n  \n    Great Lakes \n    64 \n    33.790 \n    36.419 \n    37.395 \n    35.569 \n    35.497 \n  \n  \n    Long Island \n    26 \n    38.047 \n    41.796 \n    49.667 \n    41.893 \n    42.107 \n  \n  \n    Madison & Otsego \n    58 \n    39.014 \n    40.252 \n    41.412 \n    39.937 \n    40.072 \n  \n  \n    Oneida Subbasin \n    17 \n    40.490 \n    42.839 \n    43.741 \n    45.677 \n    42.455 \n  \n  \n    Schoharie \n    30 \n    52.186 \n    57.639 \n    55.185 \n    58.110 \n    56.344 \n  \n  \n    Southwest (spring) \n    37 \n    43.921 \n    47.921 \n    44.715 \n    45.297 \n    44.806 \n  \n  \n    Southwest (fall) \n    34 \n    57.744 \n    64.114 \n    66.464 \n    66.126 \n    61.060 \n  \n  \n    Warren, Washington & Essex \n    116 \n    41.072 \n    39.656 \n    40.816 \n    41.054 \n    41.678 \n  \n\n\n\n\n\n\n\n\n\n\nTable 4:  Results of a linear mixed model representing plot absolute error as a function of filtering threshold as a fixed effect, with LiDAR region as a random effect. \n \n  \n      \n    Coefficient \n    Standard error \n    t \n    p \n  \n \n\n  \n    Intercept \n    33.215 \n    1.563 \n    21.257 \n    0.000 \n  \n  Filtering threshold\n\n    0m \n    1.370 \n    0.950 \n    1.442 \n    0.149 \n  \n  \n    0.1m \n    2.246 \n    0.950 \n    2.364 \n    0.018 \n  \n  \n    1m \n    1.779 \n    0.950 \n    1.872 \n    0.061 \n  \n  \n    2m \n    1.941 \n    0.950 \n    2.043 \n    0.041 \n  \n\n\n\n\n\n\nModel accuracy was impacted most by filtering when the area mapped was highly fragmented or contained large tracts of non-forested land (Table 5). Increasing edge and patch densities were both positively correlated with \\(\\Delta\\) RMSE following ground noise filtering, indicating greater increases in RMSE after filtering in more heterogenous landscapes, while increasing forest cover and TRI were negatively correlated with \\(\\Delta\\) RMSE (Table 5).\n\n\n\n\nTable 5:  Correlation (Spearman’s rho) between Delta RMSE (%) and landscape structural metrics at various filtering thresholds. Delta RMSE (%) represents the difference between RMSE (%) for the filtered scenario compared to RMSE (%) without filtering; positive correlations represent increasing relative error (i.e, difference in RMSE % versus the unfiltered case) as the landscape metric increases. Note that negative correlations indicate lessened impact from ground filtering, but not improved models; filtering almost always results in higher RMSE values. \n \n  \n    Filtering threshold \n    Edge density \n    Patch density \n    % Forest cover \n    TRI \n  \n \n\n  \n    0m \n    0.026 \n    0.056 \n    -0.368 \n    -0.438 \n  \n  \n    0.1m \n    0.141 \n    0.218 \n    -0.382 \n    -0.426 \n  \n  \n    1m \n    0.365 \n    0.332 \n    -0.388 \n    -0.194 \n  \n  \n    2m \n    0.321 \n    0.326 \n    -0.388 \n    -0.103"
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#ground-noise-filtering-produces-inferior-predictive-models",
    "href": "papers/ground_filtering_manuscript/index.html#ground-noise-filtering-produces-inferior-predictive-models",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Ground noise filtering produces inferior predictive models",
    "text": "Ground noise filtering produces inferior predictive models\nOur study demonstrates that the ground noise filtering approaches commonly used in preprocessing data for models of forest AGB systematically biases LiDAR-derived variables, with an end result being inferior models that produce less accurate predictions than models fit on unfiltered data sets (Figure 5, Figure 7, Table 2). Increasing intensity of ground noise filtering was generally, but not universally, associated with worse model performance (Table 2, Table 3). These patterns were generally stronger as landscape fragmentation increased, with the correlation between model errors and landscape fragmentation increasing as filtering intensity increased.\nThese results are intuitive when thinking about the actual stand characteristics that may lead to an abundance or lack of ground returns. Dense forest stands making full use of the available light should be expected to have fewer returns reaching below the uppermost branches, while regions with many gaps in the canopy will have more such returns. If we conceive of our returns as providing information about the height structure of the stand as a whole, rather than about individual trees, it stands to reason that variables calculated using all returns are more informative about stand metrics such as forest AGB than those using filtered point clouds which may sacrifice information about stand openness. This could explain the impact of ground noise filtering seen in this study using leaf-off LiDAR; we might expect this impact to be even more pronounced were we to use leaf-on LiDAR in its place.\nOur results also make sense mechanistically given the properties of the random forest algorithm used to construct forest AGB models in this study. Random forests excel at predicting outcomes based upon the consensus of weak learners (Breiman, 2001), individual decision trees which themselves rely upon small and ephemeral correlations between predictor variables and the outcome of interest. As shown in Figure 6, ground noise filtering approaches increase positive correlations between predictor variables, with the resulting increased collinearities shrinking the number and magnitude of possible weak correlations between individual variables and forest AGB (Langford et al., 2001). While the decision trees comprising the random forest may be able to take advantage of the correlations between predictor variables and the outcome to achieve similar accuracy as when trained on unfiltered data sets, we would not expect that a process that uniformly increases the positive linear correlation between variables would be associated with improved predictions.\nIn the cases where models improved post ground noise filtering, the improvement in RMSE was generally minimal and restricted to the 0 and 0.1 m thresholds (Table 3). These regions were generally the most contiguously forested (Supplementary Materials S1), which may imply fewer gaps in the canopy and therefore the filtered points contain less information on stand structure (Section 4.2). The only two regions with notable improvements in RMSE had particularly little field data available (Erie, Genessee & Livingston having 9 field plots or approximately 1% of all field data, and Columbia & Rensselaer having 23 or approximately 2.5%), resulting in models with many more predictors available than observations. This makes it challenging to generalize from these models, particularly when compared to similar landscapes with more field data available which did not see the same improvements. It may be the case that these landscapes, or others not represented by the data available to this study, would benefit from some measure of LiDAR filtering. However, based upon our combined, wall-to-wall statewide model we suggest that ground noise filtering produces inferior models of forest aboveground biomass; when mapping smaller regions with less variety in landscape characteristics, modelers may wish to investigate the impacts of ground filtering for themselves.\nInsights drawn from these results may not be limited to only machine learning based models. Anderson and Bolstad (2013) briefly note that, when fitting linear models to predict forest AGB, models based on unfiltered point clouds always provided better results than those fit to predictors calculated using only returns above 2 meters. However, few other forest AGB modeling studies have performed similar investigations, necessitating our current study. Our conclusions may not apply to AGB models of non-forest systems; investigations of ground noise filtering as a preprocessing step for models of corn AGB found improvements in predictive accuracy with relatively low height thresholds (Luo et al., 2016), emphasizing that commonly accepted data processing practices cannot be assumed to transfer across systems or domains to new questions of interest."
  },
  {
    "objectID": "papers/ground_filtering_manuscript/index.html#sec-land",
    "href": "papers/ground_filtering_manuscript/index.html#sec-land",
    "title": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes",
    "section": "Filtering thresholds and landscape characteristics",
    "text": "Filtering thresholds and landscape characteristics\nAlthough we found models fit using predictors derived from unfiltered point clouds to be the most consistently accurate, the degree to which ground noise filtering damaged predictive accuracy and the relationship between filtering intensity and accuracy varied between regions. More fragmented landscapes tended to have model accuracy be more impacted by ground noise filtering, with model error increasing the most in landscapes with greater patch and edge densities (Table 5). Given that our field measurements (and therefore accuracy assessment) only include areas delineated as forest by the FIA, this relationship is likely driven by the dramatic impacts of edge effects on forest structure and composition (Harper et al., 2005). Higher patch and edge density reflect landscapes with large amounts of marginal forestland, which likely have stands with more gaps in the canopy and therefore more LiDAR returns at the lower heights investigated by this study. Areas with greater forest cover and TRI were among the least impacted by ground filtering, though the correlation between these variables makes interpreting this result difficult. It is possible that a contiguous forest permits fewer LiDAR returns to the near-ground level, meaning that less information is removed through filtering procedures; this could explain why the correlation between forest coverage and \\(\\Delta\\) RMSE % is stable across filtering thresholds. Alternatively, it is possible that rougher landscapes (associated with a higher TRI) result in more vertical scattering among low-level returns, with the result that a point which might be accurately labeled (and filtered) at 0.1 meters in a smooth landscape is inaccurately assigned a higher elevation and not captured in the filtering procedure, retaining more information for the model. This may explain why the magnitude of correlation between TRI and \\(\\Delta\\) RMSE % decreases monotonically with increasing filtering thresholds. We stress that these are conjectures, however, and that we cannot establish any causal linkages between forest cover, TRI, and the impacts of ground noise filtering in the current study."
  },
  {
    "objectID": "posts/2022-02-18-progress-purpose-process/index.html",
    "href": "posts/2022-02-18-progress-purpose-process/index.html",
    "title": "Progress, Purpose, Process",
    "section": "",
    "text": "Today is my birthday, so please forgive me one of the most self-indulgent things I will ever write.\nTwo years ago I was living in Boston, working in tech, and a good bit bored. I was recognizing that I had spent most of the past year focusing on goals I had because they were the obvious next step, not because they were building me a life I wanted to live. I had lost a sense of agency over the future.\nLuckily, at the time, I had just come across this podcast called “Cortex”. Over the holiday season, the hosts had discussed how they structure their goals around a “yearly theme” – not New Years’ Resolutions, not specific targets that you either achieve or fail at, but a broad concept that they use to inform their decisions over the course of the year. The point is to gently nudge your life in the direction you want it to go. Here’s a video with more information about this; I recommend it.\nSo I decided that 23 would be my year of Progress; that I’d try and make a habit of making decisions that helped build momentum and kept me moving forward. I quit my job, started a PhD, moved states, and started building the personal and professional networks I rely on today. It wasn’t all perfect – about a month in there was a global pandemic I really wish I could have done something to prevent – but it nudged my life’s trajectory in a direction I’ve been happy with.\nSo when 24 came and it was time to change my theme, I decided to make it the year of Purpose. The year of Progress had involved saying yes to every opportunity that came my way; with Purpose, I wanted to make sure that I was being intentional with how I spent my time and making explicit decisions about where I’d invest my energy. I’m still not great at it, but I’ve gotten better at saying no when needed. I spent time improving my organization and time management to help me efficiently work through the things I said yes to. And I made a point of learning to consciously decide when to take time off, when to disconnect, and when to purposefully relax, so that I can now make those decisions without worrying about all the things I “should” be doing instead.\nNow it’s 25, and it’s time for a new theme. I think my plan is for the year of Process: I like where I am with my life right now, and I like how I spend my time. But I could be better and more careful about how I approach both of those things. I’m going to try to get better with details, to improve my memory around small things.\nI told you this would be indulgent. I know this sounds froofy. But it’s worked for me for two years now and I’m betting on round three. And I’m looking forward to seeing where this year takes me."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "",
    "text": "This is a quick walkthrough of how to use GitHub actions to automatically upload packages to personal {drat} repos.\nIf you just want the good stuff, here’s the link to the GitHub action I’m using as well as the GitHub workflow that uploads terrainr to my own drat repo."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#what-are-we-doing",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#what-are-we-doing",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "What are we doing?",
    "text": "What are we doing?\n{drat} is an R package that helps you set up CRAN-like repositories, hosted (primarily) on GitHub Pages. GitHub Actions is a continuous integration/continuous deployment service1, also hosted by GitHub, which lets you trigger compute workloads based on a CRON schedule, activity on a hosted repository, or manually.\nThis post walks through setting up a GitHub Actions CD workflow2 to automatically build and upload an R package to a drat repo based on a schedule or repository activity."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#why-are-we-doing-it",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#why-are-we-doing-it",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "Why are we doing it?",
    "text": "Why are we doing it?\nMy lab has a number of internal packages we use for our day-to-day work which handle things like downloading data from our central server and producing standardized model assessment outputs. All of these packages live on GitHub in our central organization, and for a long time our workflow for installing these packages has been to clone the repository and then devtools::install the package.\nRecently, though, we’ve had to share one of our packages with a number of outside collaborators who we aren’t going to be adding to the organization. We’ve also started working with a number of people who are less familiar with git and GitHub, both externally and within the lab, which means we need to provide a bit more instruction than simply saying “clone the repo and install that”3. This also means that solutions like “use remotes::install_github with a PAT” aren’t feasible; we want people to be able to get the packages they need without needing to know anything about GitHub itself. Therefore, we needed a simpler method to actually distribute our code, both to people we’d happily give repo access and people we wouldn’t want to.\nFor public repositories and organizations, I think the easiest solution to this problem is to set up your own R Universe, which rOpenSci has written fantastic instructions for getting started with; this offloads your responsibility for managing CI to the rOpenSci team. For this project, however, we wanted a separate and mostly hidden repository page, which made {drat} a perfect candidate.\nThe only problem was how to make sure we were always publishing the newest versions of our package. We’re a small team doing a lot of different things, so being able to automate away any repetitive task (like publishing patch releases to a {drat} repo) can really help reduce the cognitive load and number of mistakes associated with updating our shared codebase."
  },
  {
    "objectID": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#how-did-we-do-it",
    "href": "posts/2021-09-22-automated-drat-uploads-with-github-actions/index.html#how-did-we-do-it",
    "title": "Automated {drat} uploads with GitHub Actions",
    "section": "How did we do it?",
    "text": "How did we do it?\nBecause the original version of this workflow is on a private repo, I’ll walk through the workflow that uploads terrainr to my own personal drat repo. The YAML file that runs the workflow looks like this:\non:\n  push:\n    branches:\n      - 'main'\n    paths:\n      - 'DESCRIPTION'\n  workflow_dispatch:\n\njobs:\n  drat-upload:\n    runs-on: ubuntu-20.04\n    name: Drat Upload\n    steps:\n      - uses: mikemahoney218/upload-to-drat-repo@v0.1\n        with:\n          drat_repo: 'mikemahoney218/drat'\n          token: \"${{ secrets.DRAT_TOKEN }}\"\n          commit_message: \"Automated update (add terrainr)\"\n          commit_email: \"mike.mahoney.218@gmail.com\"\nThis script has two main components to it: running the job and configuring the action itself.\n\nRunning the job\nAt the top of terrainr’s drat workflow script is the following chunk:\non:\n  push:\n    branches:\n      - 'main'\n    paths:\n      - 'DESCRIPTION'\n  workflow_dispatch:\nThis sets up two different ways to trigger the workflow. First off, any push commit to the main branch that touches the DESCRIPTION file will automatically build the package and push it to my drat repo. Because terrainr is published on CRAN and has a handful of users, I do my best to update version numbers whenever I make changes – even during development, I try and bump development versions whenever there’s a fix or update worth mentioning.\nSince the version number is stored in DESCRIPTION, this means I’ll only push updated package versions when I’ve actually updated something in the package; my small edits to the CI files, for instance, won’t create a new release.\nIf you want to be freer about updating your repo, you can drop any of these restrictions. For instance, our internal packages have workflow triggers that look like this:\non:\n  push:\n    branches: 'main'\nIn this case, we push an update any time we push any commit to the main branch. This can cause problems – for instance, if we don’t update version numbers then update.packages won’t recognize that local installations are out of date – but makes more sense for our small team and small group of users.\nThe second method this workflow uses to upload a new version is the workflow_dispatch: option, which lets me manually trigger the workflow from the GitHub UI. This is super helpful in case I accidentally mess up my drat repo, or make a small edit without changing the version number in the DESCRIPTION; without this line I’d have to rebuild the package on my laptop and update the drat repo from my local copy.\nThere’s a lot of other ways you can set to trigger workflows – see the full list here – but the other one I want to highlight is that you can also set the workflow to trigger on a schedule using a crontab. This snippet for instance will release daily builds every day at 0:00 UTC:\non:\n  schedule: \n    - cron: '0 0 * * *'\nYou can use this syntax to create incredibly complex schedules; I personally always use https://crontab.guru/ to make sure my crontabs are right when I’m trying to schedule jobs.\n\n\nConfiguring the action\nThe second half of the workflow script actually calls the action to build the package and upload it to your drat repo:\njobs:\n  drat-upload:\n    runs-on: ubuntu-20.04\n    name: Drat Upload\n    steps:\n      - uses: mikemahoney218/upload-to-drat-repo@v0.1\n        with:\n          drat_repo: 'mikemahoney218/drat'\n          token: \"${{ secrets.DRAT_TOKEN }}\"\n          commit_message: \"Automated update (add terrainr)\"\n          commit_email: \"mike.mahoney.218@gmail.com\"\nThis will, by default, spin up an Ubuntu 20.04 runner and run v0.1 of the upload action, which is the current version. This action will automatically check out the repo the workflow is called from, build and install the R package in that repo, then insert it into a drat repo.\nThere’s a few input values for this workflow which you must specify, which are under the “with” step:\n\ndrat_repo: The GitHub repository for the drat repo you’re trying to push your package to. If you don’t have a drat repo yet, you can create one locally using drat::initRepo, and then push the created directory up to GitHub.\ncommit_email: The author to write the drat repo commit as; used to set git config user.email. You must provide a value here.\ntoken: A personal access token (PAT). The action will use this PAT to clone your package and drat repo, as well as to push your drat repo, so make sure to authorize the PAT to do so. It’s a good idea to use a service account with the fewest permissions possible for this job.\n\nYou can also customize the job further with a few additional inputs:\n\ncommit_message: The message to use when committing to drat_repo.\ncommit_author: The author to write the commit as; used to set git config user.name.\npackage: The GitHub repository for the package you want to upload. Defaults to the repository the action is running in, but this can be used to run your workflow elsewhere (for instance, if you want to have all your upload jobs scheduled in the drat repo itself).\n\nAnd for most use cases, this should be enough to automatically deploy your package whenever you desire! So far, I’ve tested this workflow on a handful of my own packages (terrainr and then our internal packages), and things are working well so far – if you find any problems or have any suggestions, feel free to open an issue on the action repo here."
  },
  {
    "objectID": "posts/2022-01-13-terrainr-an-r-package-for-creating-immersive-virtual-environments/index.html",
    "href": "posts/2022-01-13-terrainr-an-r-package-for-creating-immersive-virtual-environments/index.html",
    "title": "terrainr: An R package for creating immersive virtual environments",
    "section": "",
    "text": "Left: Where typical geodata-to-IVE workflows leave you. Right: Where terrainr gets you.\n\n\n\n\nThe JOSS workflow is absolutely fascinating. I made a git branch in the terrainr repo with the files needed to generate the paper, then (through a web form) opened a pre-review issue on GitHub. The editors used that issue to assign a handling editor, then moved things over to a review issue, where the paper was processed and prepared for submission via an automatically generated PR. The whole thing was incredibly efficient, and also entirely transparent – the logs of the paper processing workflow will be available until GitHub finally shuts down, a refreshing change compared to standard peer review.\nThe paper itself is a general overview of the terrainr package, trying to be a new enough contribution that it’s not just repeating the package documentation but not adding enough to spin off into an entirely separate research project. It’s also very short – a nice feature of JOSS papers – so rather than summarize here I’ll just suggest you read it yourself."
  },
  {
    "objectID": "posts/2021-06-14-interactive-landscape-simulations-for-visual-resource-assessment/index.html",
    "href": "posts/2021-06-14-interactive-landscape-simulations-for-visual-resource-assessment/index.html",
    "title": "Interactive landscape simulations for visual resource assessment",
    "section": "",
    "text": "Everything the light touches can see & be seen by the red dot. Users can interactively walk around the landscape to see for themselves if the viewshed algorithm gets things exactly right.\n\n\n\n\nThis was a really fun project, forcing me to push these visualizations in a new direction. The conference itself is in October, and I’ve posted a preprint of the paper at this link.\n[Update - 2021-06-24] And the code and manuscript used for this paper are now on Github at this link."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#introduction",
    "href": "posts/2021/01/model-averaging/index.html#introduction",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Introduction",
    "text": "Introduction\nBuilding models is hard. Choosing what models to build can be even harder. With seemingly infinite different modeling approaches to select between (and somehow even more individual implementations), it can be difficult to guess what methods will be the best fit for your data – particularly if you’re working with data that will change over time with new observations or predictors being added to the mix.\nUsually, we disclose this sort of uncertainty with things like confidence intervals and standard errors. Yet when it comes to selecting a single model, we often don’t discuss how confident we are in that model being the right one – instead, we present and report only our final choice as if there was no chance other candidate models would be as good or even better fits.\nEnsemble models prove a way to deal with that uncertainty (Wintle et al. 2003). By averaging predictions from a handful of candidate models, ensembles acknowledge that there might be multiple models that could be used to describe our data – and by weighting the average we can communicate how confident we are in each individual model’s view of the world. Of course, while this is all nice and flowery, it needs to work too – and model averaging delivers, typically reducing prediction errors beyond even above even the best individual component model (Dormann et al. 2018).\nThere are a ton of approaches to model averaging1. The rest of this post will walk through a few of the simplest – equal-weight averaging, fit-based averages, and model-based combinations – that you can easily implement yourself without needing to worry about slowing down your iteration time or making your modeling code too complex.\n\nGetting Started\nWe’ll be using the following libraries for data manipulation and visualization:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\nAdditionally, we’ll be using both ranger and lightgbm to develop component models:\n\nlibrary(ranger)\nlibrary(lightgbm)\n\n\nAttaching package: 'lightgbm'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nAnd finally, we need the actual data we’re modeling. For this example, we’ll build models predicting the arrival delay of the flights included in the nycflights13 package using both flight details and weather data. This next chunk of code will preprocess our data into a model-ready format:\n\nflights <- nycflights13::flights\nweather <- nycflights13::weather %>% \n  select(-wind_gust) # About 80% missing values, so we'll drop this column\n\n# combine the two data frames into one complete set\nflight_data <- flights %>% \n  left_join(weather,\n            by = c(\"year\", \"month\", \"day\", \"origin\", \"hour\", \"time_hour\")) %>% \n  drop_na()\n\nflight_data <- flight_data %>% \n  # Drop 37 pretty dramatic outliers\n  filter(arr_delay <= 500) %>% \n  # Get rid of useless predictors -- \n  # these each cause problems with at least one of our regressions\n  select(-year, -time_hour, -minute) %>% \n  # Skip the work of encoding non-numeric values, to save my poor laptop\n  select_if(is.numeric)\n\nAnd for one final pre-processing step, we’ll split our data into training, validation, and testing sets (sticking 20% into both validation and testing and dumping the rest into training). We’ll be using model performance against the validation set to determine weights for our averages.\n\nset.seed(123)\n# Generate a random sequence to subset our data into train/validate/test splits\nrow_index <- sample(nrow(flight_data), nrow(flight_data))\n\n# Testing gets the 20% of data with the highest random index values\nflight_testing <- flight_data[row_index >= nrow(flight_data) * 0.8, ]\n\n# Validation gets the next highest 20%\nflight_validation <- flight_data[row_index >= nrow(flight_data) * 0.6 &\n                                   row_index < nrow(flight_data) * 0.8, ]\n\n# Training gets the rest\nflight_training <- flight_data[row_index < nrow(flight_data) * 0.6, ]\n\n# LightGBM requires matrices, rather than data frames and formulas:\nxtrain <- as.matrix(select(flight_training, -arr_delay))\nytrain <- as.matrix(flight_training[[\"arr_delay\"]])\n\nxvalid <- as.matrix(select(flight_validation, -arr_delay))\nxtest <- as.matrix(select(flight_testing, -arr_delay))\n\nSo with that out of the way, it’s time to start training our models!"
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#component-models",
    "href": "posts/2021/01/model-averaging/index.html#component-models",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Component Models",
    "text": "Component Models\n\nLinear Model\nLet’s start off with a simple linear regression model, using all of our predictors in the flight dataset to try and estimate arrival delays:\n\nlinear_model <- lm(arr_delay ~ ., flight_training)\nsummary(linear_model)\n\n\nCall:\nlm(formula = arr_delay ~ ., data = flight_training)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.895  -9.133  -1.538   7.076 159.388 \n\nCoefficients:\n                  Estimate  Std. Error  t value           Pr(>|t|)    \n(Intercept)    -4.61324673  5.99244467   -0.770           0.441394    \nmonth           0.03825040  0.01117238    3.424           0.000618 ***\nday             0.02220492  0.00410860    5.404 0.0000000650770797 ***\ndep_time        0.00009509  0.00027953    0.340           0.733722    \nsched_dep_time -0.00349249  0.00189448   -1.844           0.065257 .  \ndep_delay       1.01251264  0.00106768  948.332            < 2e-16 ***\narr_time        0.00088164  0.00011818    7.460 0.0000000000000868 ***\nsched_arr_time -0.00471343  0.00014783  -31.884            < 2e-16 ***\nflight         -0.00004692  0.00002541   -1.846           0.064863 .  \nair_time        0.75629859  0.00307431  246.006            < 2e-16 ***\ndistance       -0.09791613  0.00039245 -249.500            < 2e-16 ***\nhour            0.59997173  0.18707035    3.207           0.001341 ** \ntemp            0.11726625  0.02231781    5.254 0.0000001487014873 ***\ndewp            0.03632142  0.02404661    1.510           0.130928    \nhumid           0.01860018  0.01228626    1.514           0.130053    \nwind_dir       -0.00607627  0.00040085  -15.158            < 2e-16 ***\nwind_speed      0.19198999  0.00753768   25.471            < 2e-16 ***\nprecip         26.88470146  3.01386317    8.920            < 2e-16 ***\npressure       -0.01634187  0.00561852   -2.909           0.003631 ** \nvisib          -0.46031686  0.03238825  -14.212            < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.82 on 170687 degrees of freedom\nMultiple R-squared:  0.8707,    Adjusted R-squared:  0.8707 \nF-statistic: 6.048e+04 on 19 and 170687 DF,  p-value: < 2.2e-16\n\n\nCool! We have our first model – and it seems to be a pretty ok fit, with an R^2 of 0.87. We could probably make this model better by being a bit more selective with our terms or throwing in some interaction terms – but as a first stab at a model that we’ll incorporate into our average, this is pretty alright.\nOf course, we want to make sure this model can generalize outside of the data it was trained with – let’s use it to make predictions against our validation set, too:\n\nflight_validation$lm_pred <- predict(\n  linear_model,\n  newdata = flight_validation\n)\n\nsqrt(mean((flight_validation$lm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lm_pred, flight_validation))$r.squared\n\n\n\n[1] 14.73962\n\n\n[1] 0.8684178\n\n\nR^2 remains at about 0.87 and RMSE comes in at about 14.74 minutes – which, considering delays in the validation set range from -75 to 485 minutes, feels not too bad for a naively implemented linear model.\n\n\nRandom Forest\nSo we have our first model sorted, but we need more than that to take an average! While we could average out a number of linear models with different parameters, it feels more interesting to combine a few different algorithms as component models. So let’s use ranger to implement a random forest to represent our data – fair warning, this one takes a little while to train!\n\nranger_model <- ranger::ranger(arr_delay ~ ., data = flight_training)\nsqrt(ranger_model$prediction.error)\nranger_model$r.squared\n\n\n\n[1] 11.08573\n\n\n[1] 0.9276561\n\n\nSo this model has an RMSE of 11.09 and an R^2 of 0.93 – an improvement over our linear model! While we could eke out some improvements with careful tuning, it looks like this version is a good enough fit to use as an example in our ensemble. As before, we want to check out how well this model generalizes by using it to generate predictions for our validation set:\n\nranger_predictions <- predict(\n    ranger_model,\n    data = flight_validation\n  )\n\nflight_validation$ranger_pred <- ranger_predictions$predictions\n\nsqrt(mean((flight_validation$ranger_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ ranger_pred, flight_validation))$r.squared\n\n\n\n[1] 10.96209\n\n\n[1] 0.9302306\n\n\nOur model actually performs (extremely) slightly better on the validation set!\n\n\nGBM\nSo that’s two models sorted! For completeness sake, let’s implement a third and final component model, this time using the LightGBM package to fit a gradient boosting machine. Similar to the last two, we won’t do a ton to parameterize this model – the only change I’ll make to the model fit defaults is to use 100 rounds, to let the boosting algorithm get into the same performance range as our other two models.\n\nlightgbm_model <- lightgbm::lightgbm(xtrain, \n                                     ytrain, \n                                     nrounds = 100, \n                                     obj = \"regression\", \n                                     metric = \"rmse\",\n                                     # Suppress output\n                                     force_col_wise = TRUE,\n                                     verbose = 0L)\n\nThe lightgbm_model doesn’t have the same easy method for evaluating in-bag performance as our linear model and random forests did. We’ll skip right to the validation set instead:\n\nflight_validation$lightgbm_pred <- predict(\n  lightgbm_model,\n  xvalid\n)\n\nsqrt(mean((flight_validation$lightgbm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lightgbm_pred, flight_validation))$r.squared\n\n\n\n[1] 10.4088\n\n\n[1] 0.9347398\n\n\nSo it looks like LightGBM model performs about as well (if not marginally better than) our random forest! For reference, here are the RMSE values from each of our candidate models:\n\nprediction_values <- flight_validation %>% \n  # Only select our y and y-hat columns\n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots <- prediction_values %>% \n  pivot_longer(cols = -arr_delay) %>% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, levels = c(\"lightgbm\", \"ranger\", \"lm\")))\n\nprediction_plots %>% \n  group_by(Model = name) %>% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %>% \n  arrange(RMSE) %>% \n  knitr::kable()\n\n\n\n\nModel\nRMSE\n\n\n\n\nlightgbm\n10.40880\n\n\nranger\n10.96209\n\n\nlm\n14.73962\n\n\n\n\n\nOf course, individual metrics don’t tell the whole story – it can be helpful to look at diagnostic plots of our predictions to try and understand patterns in how our predictions match the data. For instance, “linear models are about four minutes worse on average” is all well and good in the abstract, but graphics like the one below can help us see that – for instance – linear models tend to do a bit worse around 0 minute delays (where most of the data is clustered) while our random forest performs worse on higher extremes:\n\nprediction_plots %>% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")"
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#model-averaging",
    "href": "posts/2021/01/model-averaging/index.html#model-averaging",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Model Averaging",
    "text": "Model Averaging\nWith our candidate models in tow, we’re now fully ready to move on to model averaging methods! We’ll walk through basic implementations of three methods (equal weighting, fit-based weights, and model-based estimates) and then evaluate our ensembles at the end.\n\nEqual Weights\nPerhaps the most obvious way to average models is to take the simple arithmetic mean of your model predictions. This method presupposes that each of your models are equally good representations of your underlying data; since that isn’t the case here, we might expect this method to not substantially reduce error overall.\nA benefit of this method, though, is that implementation takes no time at all:\n\nprediction_values <- prediction_values %>% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\n\n\nFit-Based Weights\nA slightly more involved method is to weight models based on some metric of their performance. Almost any metric with a standard definition across component models can be used (so for instance, AIC or BIC with nested models or MSE and MAPE); as we’ve been using RMSE so far, we’ll use it to weight our errors.\nWeighting models based on fit statistics is also relatively easy in the grand scheme of data science. First, calculate the fit statistic for each of your models:\n\nmodel_rmse <- vapply(\n  prediction_values,\n  function(x) sqrt(mean((x - prediction_values$arr_delay)^2)),\n  numeric(1)\n  )[1:3] # Only our 3 component models!\nmodel_rmse\n\n      lm_pred   ranger_pred lightgbm_pred \n     14.73962      10.96209      10.40880 \n\n\nThen, depending on your statistic, you may need to take the reciprocal of each value – as lower RMSEs are better, we need to do so here:\n\nrmse_weights <- (1 / (model_rmse))\n\nLastly, calculate your weights as proportion of the whole set of – you can view these values as the proportion of the ensemble prediction contributed by each component:\n\nrmse_weights <- rmse_weights / sum(rmse_weights)\nrmse_weights\n\n      lm_pred   ranger_pred lightgbm_pred \n    0.2659099     0.3575422     0.3765479 \n\n\nMaking predictions with the ensemble is then relatively easy – just multiply each of your predicted values by their proportion and sum the results:\n\nprediction_values <- prediction_values %>% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\n\n\nModel-Based Weights\nThe last averaging method we’ll walk through is a little more involved, but still pretty comprehensible: take your model outputs, turn around, and use them as model inputs.\n\nOur toy example here is a pretty good fit for this method – we already saw in our graphics that a strong linear relationship exists between our predictions and the true value, and this relationship is a little different for each model:\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFrom this plot, we can guess that a linear model combining our component predictions as features will be a good fit2 for averaging these models. To do so, we simply need to build a linear model:\n\npredictions_model <- lm(arr_delay ~ lm_pred * ranger_pred * lightgbm_pred, \n                        data = prediction_values)\n\nAnd then use it to generate predictions just like our original component linear model:\n\nprediction_values$model_based_pred <- predict(\n  predictions_model,\n  newdata = prediction_values\n)\n\nNote that if we saw non-linear relationships between our predictions and true values, we’d want to rely on non-linear methods to average out predictions; it just so happens that our models are already pretty strong fits for the underlying data and can be well-represented with simple linear regression."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#howd-we-do",
    "href": "posts/2021/01/model-averaging/index.html#howd-we-do",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "How’d We Do?",
    "text": "How’d We Do?\nNow that we have our ensemble models prepared, it’s time to evaluate all of our models out against our testing set!\nThe first step is to generate predictions for the test set using our component models:\n\nflight_testing$lm_pred <- predict(\n  linear_model,\n  newdata = flight_testing\n)\n\nranger_predictions <- predict(\n    ranger_model,\n    data = flight_testing\n  )\n\nflight_testing$ranger_pred <- ranger_predictions$predictions\n\nflight_testing$lightgbm_pred <- predict(\n  lightgbm_model,\n  xtest\n)\n\nWe can use those predictions to generate our ensemble predictions. Note that we’re still using the weights and models calibrated on the validation data – we (theoretically) shouldn’t know the “true” values for the test set, so we can’t re-weight our averages now!\n\nflight_testing <- flight_testing %>% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\nflight_testing <- flight_testing %>% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\nflight_testing$model_based_pred <- predict(\n  predictions_model,\n  newdata = flight_testing\n)\n\n\n\n\nSo how’d we do? Let’s check out the RMSE for each of our models:\n\nprediction_values <- flight_testing %>% \n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots <- prediction_values %>% \n  pivot_longer(cols = -arr_delay) %>% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, \n                       levels = c(\"lightgbm\", \"ranger\", \"lm\",\n                                  \"model_based\", \"fit_based\", \"equal_weight\")))\n\nprediction_plots %>% \n  group_by(Model = name) %>% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %>% \n  arrange(RMSE) %>% \n  knitr::kable()\n\n\n\n\nModel\nRMSE\n\n\n\n\nmodel_based\n9.492409\n\n\nlightgbm\n10.290113\n\n\nranger\n10.968544\n\n\nfit_based\n11.057728\n\n\nequal_weight\n11.311836\n\n\nlm\n14.621943\n\n\n\n\n\n\nprediction_plots %>% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\nCool – our model-based ensemble actually performed better than any of the components! While the equal weight and fit-based averages were pretty middle-of-the-road, in other settings these methods can also help to reduce bias in predictions and produce estimates with less variance than any of the component models."
  },
  {
    "objectID": "posts/2021/01/model-averaging/index.html#conclusion",
    "href": "posts/2021/01/model-averaging/index.html#conclusion",
    "title": "Model averaging methods: how and why to build ensemble models",
    "section": "Conclusion",
    "text": "Conclusion\nModel averaging can be a powerful tool for reducing model bias and addressing the implicit uncertainty in attempting to pick the “best” model for a situation. While plenty of complex and computationally expensive approaches to averaging exist – and can greatly improve model performance – simpler ensemble methods can provide the same benefits without necessarily incurring the same costs."
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html",
    "href": "posts/2021/02/terrainr/index.html",
    "title": "terrainr 0.3.0 is out today",
    "section": "",
    "text": "terrainr is in review with rOpenSci and the first review just came back! I’ve been working through the comments over the past week or so, and today that work has culminated in the release of terrainr version 0.3.0.\nThis is a big release with a handful of breaking changes, so I felt like I should give a brief overview of the biggest user-facing changes."
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html#breaking-changes",
    "href": "posts/2021/02/terrainr/index.html#breaking-changes",
    "title": "terrainr 0.3.0 is out today",
    "section": "Breaking Changes",
    "text": "Breaking Changes\n\nObject Classes Are Dead; Long Live Object Classes\nThe single largest change is that terrainr specific classes are no longer exported, and users shouldn’t need to worry about getting data into or out of those formats anymore. Instead, use any sf or Raster object in their place. For instance, workflows that used to look like this:\n\n# Doesn't run:\nlibrary(terrainr)\nsimulated_data <-  data.frame(id = seq(1, 100, 1),\n                              lat = runif(100, 44.04905, 44.17609), \n                              lng = runif(100, -74.01188, -73.83493))\n\nbbox <- get_bbox(lat = simulated_data$lat, lng = simulated_data$lng) \n\noutput_tiles <- get_tiles(bbox = bbox,\n                          services = c(\"elevation\", \"ortho\"),\n                          resolution = 90)\n\nNow look like this:\n\nlibrary(terrainr)\nsimulated_data <-  data.frame(id = seq(1, 100, 1),\n                              lat = runif(100, 44.04905, 44.17609), \n                              lng = runif(100, -74.01188, -73.83493))\n\nsimulated_data <- sf::st_as_sf(simulated_data, coords = c(\"lng\", \"lat\"))\nsimulated_data <- sf::st_set_crs(simulated_data, 4326)\n\noutput_tiles <- get_tiles(data = simulated_data,\n                          services = c(\"elevation\", \"ortho\"),\n                          resolution = 90)\n\nAs part of this change, get_bbox, get_coordinate_bbox, and all class creation and export functions are gone now. Use sf (or Raster*) objects in their place instead.\n\n\nNew Names, Who This?\nget_tiles now uses the services argument to name its output list:\n\nnames(output_tiles)\n\n[1] \"elevation\" \"ortho\"    \n\n\nThis means that if you request the service elevation you can retrieve your tiles using the name elevation. If you request the same endpoint with multiple names, get_tiles will use whatever name was first in the vector.\n\n\nFewer Utilities, More Useful\nUtility functions calc_haversine_distance, convert_distance, point_from_distance, rad_to_deg, and deg_to_rad have been removed (or removed from exports). For unit conversions, check out the units package. This shouldn’t impact the main uses of the package, but is still worth flagging."
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html#show-me-what-you-got",
    "href": "posts/2021/02/terrainr/index.html#show-me-what-you-got",
    "title": "terrainr 0.3.0 is out today",
    "section": "Show Me What You Got",
    "text": "Show Me What You Got\nterrainr 0.3.0 adds a ggplot2 geom, geom_spatial_rgb, for plotting 3 band RGB rasters:\n\nlibrary(ggplot2)\n\nggplot() + \n  geom_spatial_rgb(data = output_tiles[[\"ortho\"]],\n                   # Required aesthetics r/g/b specify color bands:\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nNote that above we just passed the file path to our raster; we can also pass a RasterStack:\n\northo <- raster::stack(output_tiles[[\"ortho\"]])\n\nggplot() + \n  geom_spatial_rgb(data = ortho,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nOr a data.frame:\n\northo_df <- raster::as.data.frame(ortho, xy = TRUE)\nnames(ortho_df) <- c(\"x\", \"y\", \"red\", \"green\", \"blue\")\n\nggplot() + \n  geom_spatial_rgb(data = ortho,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nNote that each step here gives you a little more control over the output image – for instance, if your raster bands aren’t in RGB order (or you have more than RGBA bands), you’ll need to provide a data.frame to get a true color image.\nYou can then use these basemaps like most other ggplot geoms:\n\nggplot() + \n  geom_spatial_rgb(data = ortho_df,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  geom_sf(data = simulated_data) + \n  coord_sf(crs = 4326)"
  },
  {
    "objectID": "posts/2021/02/terrainr/index.html#new-docs-who-this",
    "href": "posts/2021/02/terrainr/index.html#new-docs-who-this",
    "title": "terrainr 0.3.0 is out today",
    "section": "New Docs, Who This?",
    "text": "New Docs, Who This?\nThose are just a few of the changes in 0.3.0; you can find a longer list in the NEWS file.\nOne thing not mentioned in the NEWS file, though, is that this version of terrainr included a complete rewrite of the documentation. The docs were mostly written while the package was being conceptually developed, and as a result gave a bit too much emphasis to some ideas while completely ignoring others. So I’ve rewritten all of the documentation that lives on the terrainr website – let me know what you think about the new versions (or if you catch anything I’ve missed!)."
  },
  {
    "objectID": "posts/2019/03/index.html",
    "href": "posts/2019/03/index.html",
    "title": "Thesis Now Available in ESF Digital Commons",
    "section": "",
    "text": "In the thesis, we look at the impacts beavers have on the forest community around them as they remove trees for food and building dams. While people had looked at these impacts in other parts of beaver’s range, the Adirondacks are a strange enough ecosystem - being largely protected from anthropogenic disturbances, most of the forest landscape exhibits only one or two age classes - that we weren’t sure how applicable conclusions from these regions would be. What we found was that while the broad conclusions of these studies held true - beavers still operate as central place foragers and create large disturbances in the landscape - the lack of early-successional species throughout the Adirondacks seriously shifted which stems were harvested preferrentially. We also found a lot of variance in the patterns of how individual species were utilized - for instance, beaver harvested almost any size speckled alder they could find, so long as it was close to their dam, but would harvest red maple at any distance, so long as the stem was small.\n\nWe’re currently working on a journal article version of the thesis, using an expanded dataset and focusing more closely on the patterns in forage selectivity we found, and how they differ from other regions. That should hopefully be in the review process within the next few weeks."
  },
  {
    "objectID": "posts/2020/05/index.html",
    "href": "posts/2020/05/index.html",
    "title": "Installing the TIG stack on Raspberry Pi",
    "section": "",
    "text": "Do the following in a shell you’ve already auth’d into sudo on:\nsudo apt update\nsudo apt upgrade\n\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb\nsudo systemctl unmask influxdb\nsudo systemctl enable influxdb\nsudo systemctl start influxdb\n\n# you can find the current telegraf release here: https://portal.influxdata.com/downloads/\nwget https://dl.influxdata.com/telegraf/releases/telegraf-1.14.2_linux_armhf.tar.gz\ntar xf telegraf-1.14.2_linux_armhf.tar.gz\nsudo systemctl enable --now telegraf\nrm telegraf-1.14.2_linux_armhf.tar.gz\n\nsudo apt-get install -y adduser libfontconfig1\n# you can find the current grafana release here: https://grafana.com/grafana/download\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nThis should cause all three services to start on system boot. You’ll need to configure Telegraf to actually write to your local Influx instance at http://127.0.0.1:8086 (there’s a sample config under the Telegraf part of the post), then set up Grafana to read from Influx (at the same port) via the UI at localhost:3000.\n\n\n\n\n\n\nI’m getting a little cabin-fevery as the 2020 quarantine moves into its third month. To try and defray some of the extra energy, I’ve been hacking on a Pi I set up with a Pi-hole and openvpn server about a month ago.\nOne of the cool things about the Pi-hole is that it gives you a little at-a-glance view of how your machine is doing, including CPU load, memory utilization, and temperature. This window into system stats made me realize that my little box is packing heat:\n\nI’m running a Pi 4, which is known for generating more heat than it can handle, so temperatures of ~60 C (the upper range of “safe”) isn’t too shocking – but with summer coming and me planning to add some load to this machine in the near future, I wanted to set up monitoring to make sure my box wasn’t going to melt on me. This also has the side benefit that I’ll have a metrics system already in place for anything else I stand up on this machine.\nEnter the TIG stack. TIG – Telegraf, InfluxDB, and Grafana – is a suite of open-source solutions for collecting, storing, and visualizing time-series data, like the sort you’ll get from repeatedly measuring system temperature.\nThis tutorial will walk you through setting up each of these services separately. These steps were tested on a Raspberry Pi 4 running Raspbian Buster, so other configurations might require some tweaking.\nAll of the code here should be run in a terminal on your Raspberry Pi unless I specify it needs to go somewhere else. To make sure you’re not going to run into dependency hell, it’s a good idea to run sudo apt update && sudo apt upgrade before installing any of the stack.\n\n\n\n\n\n\nFirst up, we need to set up our InfluxDB instance. This database is where our Telegraf instance will send metrics and where Grafana will read from, so it makes sense to stand it up first!\nInstalling the service is easy enough – we just need to add Influx’s authentication key, add their repository to our trusted sources, and then install it via apt:\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb influxdb-client\n\n\nNow we want to actually start the database, and tell our system to start it after reboots – since we’re expecting to always be collecting metrics via Telegraf, we need to make sure that we always have a place to write to, as well. This is a quick two-liner using systemctl – we first need to unmask Influx, which will let us add it as a service, then tell our Pi to start the service both right now and every time the system restarts via the enable --now command:\nsudo systemctl unmask influxdb\nsudo systemctl enable --now influxdb\n \n \nAfter this, you should be able to run systemctl status influxdb to see the service status – if everything went according to plan, you should see Active: active (running) around line 3 of the output.\nAt this point, it’s probably healthy to add authentication to your Influx instance if your pi is exposed to external networks. You can set up a basic admin account via:\ninflux\nCREATE USER admin WITH PASSWORD '<password>' WITH ALL PRIVILEGES\n \n \nYou can then force HTTP authentication by adding the following under the HTTP header in /etc/influxdb/influxdb.conf:\n[HTTP]\nauth-enabled = true\npprof-enabled = true\npprof-auth-enabled = true\nping-auth-enabled = true\n \n \nThe changes take effect the next time your service starts, which you can trigger via sudo systemctl restart influxdb.\n\n\n\n\n\n\nWith Influx up and running, it’s time for us to start writing records, which means standing up Telegraf!\nTelegraf is updated pretty frequently, so it’s a good idea to check the release page to see what version you should be installing. At the time of writing, the current version is 1.14.2, so I ran the following to install Telegraf on my machine:\nwget https://dl.influxdata.com/telegraf/releases/telegraf_1.14.2-1_armhf.deb\nsudo dpkg -i telegraf_1.14.2-1_armhf.deb\nrm telegraf_1.14.2-1_armhf.deb\n \n \nWe now have Telegraf installed on our machine, but the service won’t do us much good before we set up our configuration, located at /etc/telegraf/telegraf.conf. Telegraf operates by coordinating a bunch of “plugins”, which work to collect and write data to and from different sources. You can see the full list of plugins at Telegraf’s GitHub repo, and activate each by copying the configuration from the plugin’s readme into your /etc/telegraf/telegraf.conf file.\nI spent far too much time pouring over the various plugins and wound up with the following configuration file – you can use this to overwrite your default telegraph.conf file and start collecting metrics right away, or you can spend the time now to set up your instance to suit your own particular needs. Just make sure you edit your [[outputs.influxdb]] to include the following:\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n \n \nMy full configuration looks like this:\n[agent]\n   # Batch size of values that Telegraf sends to output plugins.\n   metric_batch_size = 1000\n   # Default data collection interval for inputs.\n   interval = \"30s\"\n   # Added degree of randomness in the collection interval.\n   collection_jitter = \"5s\"\n   # Send output every 5 seconds\n   flush_interval = \"5s\"\n   # Buffer size for failed writes.\n   metric_buffer_limit = 10000\n   # Run in quiet mode, i.e don't display anything on the console.\n   quiet = true\n[[inputs.ping]] # # Ping given url(s) and return statistics\n## urls to ping\nurls = [\"www.github.com\",\"www.amazon.com\",\"1.1.1.1\",\"www.mm218.dev\"]\n## number of pings to send per collection (ping -c )\ncount = 3\n## interval, in s, at which to ping. 0 == default (ping -i )\nping_interval = 15.0\n## per-ping timeout, in s. 0 == no timeout (ping -W )\ntimeout = 10.0\n## interface to send ping from (ping -I )\ninterface = \"wlan0\"\n[[inputs.system]]\n[[inputs.influxdb]]\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n  ## http request & header timeout\n  timeout = \"5s\"\n[[inputs.disk]]\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n[[inputs.diskio]]\n[[inputs.internal]]\n  ## If true, collect telegraf memory stats.\n  collect_memstats = true\n[[inputs.mem]]\n[[inputs.processes]]\n# custom temperature script\n# https://github.com/mikemahoney218/pi-admin/blob/master/telegraf-scripts/systemp.sh\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n   ## The target database for metrics (telegraf will create it if not exists).\n   database = \"pi_logs\" # required\n   ## Name of existing retention policy to write to.  Empty string writes to\n   ## the default retention policy.\n   retention_policy = \"\"\n   ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\"\n   write_consistency = \"any\"\n   ## Write timeout (for the InfluxDB client), formatted as a string.\n   ## If not provided, will default to 5s. 0s means no timeout (not recommended).\n   timeout = \"10s\"\n \n \nIn putting all this together, I found out that the Telegraf plugin to measure system temperature – the thing that got me down this rabbit hole in the first place – doesn’t actually work on Raspberry Pi systems. As a workaround, I threw together a simple one-liner in Bash:\necho \"systemp temp=`cat /sys/class/thermal/thermal_zone0/temp`\"\n \n \nI saved that script off to /tmp/telegraf-scripts/systemp.sh, then added it to my telegraf.conf in the brick:\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n \n \nIf you’re not worried about measuring temperature, you don’t need (or want) to include that section in your telegraf.conf.\nIf you set up HTTP authentication for your Influx instance, you’re going to want to add username and password fields under the [[outputs.influxdb]]\nWith our configuration in place, all that’s left now is to start and enable the Telegraf service:\nsudo systemctl enable --now telegraf\n \n \nAs before, you should be able to see that the service is running without issue by running systemctl status telegraf.\nNow that your service is running, any changes that you make to your telegraf.config file will only take effect after the service restarts. You can always restart the service using sudo systemctl restart telegraf, but I personally kept forgetting to do so (and then was surprised when my metrics weren’t showing up in Influx). To deal with that, I wrote an extremely-micro service that restarts Telegraf for me.\n\n\n\n\n\n\nWe’re finally onto our last service, the G in the TIG stack, Grafana. A quick word of warning: don’t try to sudo apt install grafana. The main repository has an outdated version of Grafana, which will leave you stuck at a blank screen when you try to log on for the first time.\nInstead, we’ll install Grafana via dpkg, like we did with Telegraf. Check for the most current version at Grafana’s downloads page. At the time of writing, I was installing version 6.7.3, so my commands to install looked like this:\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nUnlike Influx and Telegraf, Grafana can be managed almost entirely from a UI. Boot up localhost:3000 on your Pi and log in using admin for both your username and password – you’ll be prompted to change it once you’re logged in for the first time.\nYou’ll then want to add your local Influx instance as a datasource for Grafana. Assuming you’ve followed along until now, the URL for your Influx instance is http://localhost:8086. You’ll also want to add whatever database Telegraf is writing to – in the sample configuration I posted, the database name is pi_logs, but you can find yours by looking for the database field under [[outputs.influxdb]]. If you added authentication to your Influx instance, you’ll also want to turn on basic auth and provide your database credentials.\n\n\n\n\n\n\nAnd with that, you should have everything you need to start monitoring your Pi – and, with a little elbow grease, anything your Pi can touch! While it certainly feels a little like overkill, I’ve now got state-of-the art tracking and system metrics for my Pi, letting me confirm beyond a shadow of a doubt that… my Pi is running too hot. With all the time I spent on this, maybe I should have just bought a fan.\nBut hey – would a fan look this good?"
  },
  {
    "objectID": "posts/2020/01/index.html",
    "href": "posts/2020/01/index.html",
    "title": "Announcing {heddlr}, now on CRAN!",
    "section": "",
    "text": "You can find out more on heddlr’s documentation website, proudly made in R via pkgdown. This first version on CRAN is 0.5.0, with 0.1 -> 0.4 previously released on GitHub."
  },
  {
    "objectID": "posts/2020/04/mechanics-of-viz/index.html",
    "href": "posts/2020/04/mechanics-of-viz/index.html",
    "title": "Mechanics of Data Visualizations",
    "section": "",
    "text": "Let’s move from theoretical considerations of graphing to the actual building blocks you have at your disposal. As we do so, we’re also going to move on to mantra #2:\n\nEverything should be made as simple as possible – but no simpler.\nGraphs are inherently a 2D image of our data:\n\nThey have an x and a y scale, and - as in our scatter plot here - the position a point falls along each scale tells you how large its values are. But this setup only allows us to look at two variables in our data - and we’re frequently interested in seeing relationships between more than two variables.\nSo the question becomes: how can we visualize those extra variables? We can try adding another position scale:\n\nknitr::include_graphics(\"unnamed-chunk-2-1.png\")\n\n\n\n\nBut 3D images are hard to wrap your head around, complicated to produce, and not as effective in delivering your message. They do have their uses - particularly when you’re able to build real, physical 3D models, and not just make 3D shapes on 2D planes - but frequently aren’t worth the trouble.\nSo what tools do we have in our toolbox? The ones that are generally agreed upon (no, really - this is an area of active debate) fall into four categories:\n\nPosition (like we already have with X and Y)\nColor\nShape\nSize\n\nThese are the tools we can use to encode more information into our graphics. We’re going to call these aesthetics, but any number of other words could work - some people refer to them as scales, some as values. I call them aesthetics because that’s what my software of choice calls them - but the word itself comes from the fact that these are the things that change how your graph looks.\nFor what it’s worth, we’re using an EPA data set for this unit, representing fuel economy data from 1999 and 2008 for 38 popular models of car. “Hwy” is highway mileage, “displ” is engine displacement (so volume), and “cty” is city mileage. But frankly, our data set doesn’t matter right now - most of our discussion here is applicable to any data set you’ll pick up.\nWe’re going to go through each of these aesthetics, to talk about how you can encode more information in each of your graphics. Along the way, remember our mantras:\n\nA good graphic tells a story\nEverything should be made as simple as possible - but no simpler\nUse the right tool for the job\nInk is cheap. Electrons are even cheaper\n\nWe’ll talk about how these are applicable throughout this section.\n\n\nPosition\nLet’s start off discussing these aesthetics by finishing up talking about position. The distance of values along the x, y, or – in the case of our 3D graphic – z axes represents how large a particular variable is. People inherently understand that values further out on each axis are more extreme - for instance, imagine you came across the following graphic (made with simulated data):\n\nWhich values do you think are higher?\nMost people innately assume that the bottom-left hand corner represents a 0 on both axes, and that the further you get from that corner the higher the values are. This – relatively obvious – revelation hints at a much more important concept in data visualizations: perceptual topology should match data topology. Put another way, that means that values which feel larger in a graph should represent values that are larger in your data. As such, when working with position, higher values should be the ones further away from that lower left-hand corner – you should let your viewer’s subconscious assumptions do the heavy lifting for you.\nApplying this advice to categorical data can get a little tricky. Imagine that we’re looking at the average highway mileages for manufacturers of the cars in our data set:\n\nIn this case, the position along the x axis just represents a different car maker, in alphabetical order. But remember, position in a graph is an aesthetic that we can use to encode more information in our graphics. And we aren’t doing that here – for instance, we could show the same information without using x position at all:\n\nTry to compare Pontiac and Hyundai on the first graph, versus on this second one. If anything, removing our extraneous x aesthetic has made it easier to compare manufacturers. This is a big driver behind our second mantra – that everything should be made as simple as possible, but no simpler. Having extra aesthetics confuses a graph, making it harder to understand the story it’s trying to tell.\nHowever, when making a graphic, we should always be aiming to make important comparisons easy. As such, we should take advantage of our x aesthetic by arranging our manufacturers not alphabetically, but rather by their average highway mileage: \nBy reordering our graphic, we’re now able to better compare more similar manufacturers. It’s now dramatically faster to understand our visualization – closer comparisons are easier to make, so placing more similar values closer together makes them dramatically easier to grasp. Look at Pontiac vs Hyundai now, for instance. Generally speaking, don’t put things in alphabetical order - use the order you place things to encode additional information.\nAs a quick sidenote, I personally believe that, when working with categorical values along the X axis, you should reorder your values so the highest value comes first. For some reason, I just find having the tallest bar/highest point (or whatever is being used to show value) next to the Y axis line is much cleaner looking than the alternative:\n\nFor what it’s worth, I’m somewhat less dogmatic about this when the values are on the Y axis. I personally believe the highest value should always be at the top, as humans expect higher values to be further from that bottom left corner: \nHowever, I’m not as instantly repulsed by the opposite ordering as I am with the X axis, likely because the bottom bar/point being the furthest looks like a more natural shape, and is still along the X axis line: \nFor this, at least, your mileage may vary. Also, it’s worth pointing out how much cleaner the labels on this graph are when they’re on the Y axis - flipping your coordinate system, like we’ve done here, is a good way to display data when you’ve got an unwieldy number of categories.\n\n\nColor\nWhile we’ve done a good job covering the role position plays in communicating information, we’re still stuck on the same question we started off with: How can we show a third variable on the graph?\nOne of the most popular ways is to use colors to represent your third variable. It might be worth talking through how color can be used with a simulated data set. Take for example the following graph: \nAnd now let’s add color for our third variable: \nRemember: perceptual topology should match data topology. Which values are larger?\nMost people would say the darker ones. But is it always that simple? Let’s change our color scale to compare: \nSure, some of these colors are darker than others – but I wouldn’t say any of them tell me a value is particularly high or low.\nThat’s because humans don’t percieve hue – the actual shade of a color – as an ordered value. The color a point is doesn’t communicate that the point has a higher or lower value than any other point on the graph. Instead, hue works as an unordered value, which only tells us which points belong to which groupings. In order to tell how high or low a point’s value is, we instead have to use luminescence – or how bright or dark the individual point is.\nThere’s one other axis you can move colors along in order to encode value – how vibrant a color is, known as chroma:\n\nJust keep in mind that luminescence and chroma – how light a color is and how vibrant it is – are ordered values, while hue (or shade of color) is unordered This becomes relevant when dealing with categorical data. For instance, moving back to the scatter plot we started with:\n\nIf we wanted to encode a categorical variable in this – for instance, the class of vehicle – we could use hue to distinguish the different types of cars from one another:\n\nIn this case, using hue to distinguish our variables clearly makes more sense than using either chroma or luminesence:\n\nThis is a case of knowing what tool to use for the job - chroma and luminescence will clearly imply certain variables are closer together than is appropriate for categorical data, while hue won’t give your audience any helpful information about an ordered variable. Note, though, that I’d still discourage using the rainbow to distinguish categories in your graphics – the colors of the rainbow aren’t exactly unordered values (for instance, red and orange are much more similar colors than yellow and blue), and you’ll wind up implying connections between your categories that you might not want to suggest. Also, the rainbow is just really ugly:\n\nSpeaking of using the right tool for the job, one of the worst things people like to do in data visualizations is overuse color. Take for instance the following example:\n\nIn this graph, the variable “class” is being represented by both position along the x axis, and by color. By duplicating this effort, we’re making our graph harder to understand – encoding the information once is enough, and doing it any more times than that is a distraction. Remember the second mantra: Everything should be made as simple as possible – but no simpler. The best data visualization is one that includes all the elements needed to deliver the message, and no more.\nYou can feel free to use color in your graphics, so long as it adds more information to the plot - for instance, if it’s encoding a third variable:\n\nBut replicating as we did above is just adding more junk to your chart.\nThere’s one last way you can use color effectively in your plot, and that’s to highlight points with certain characteristics:\n\nDoing so allows the viewer to quickly pick out the most important sections of our graph, increasing its effectiveness. Note that I used shape instead of color to separate the class of vehicles, by the way – combining point highlighting and using color to distinguish categorical variables can work, but can also get somewhat chaotic:\n\nThere’s one other reason color is a tricky aesthetic to get right in your graphics: about 5% of the population (10% of men, 1% of women) can’t see colors at all. That means you should be careful when using it in your visualizations – use colorblind-safe color palettes (google “ColorBrewer” or “viridis” for more on these), and pair it with another aesthetic whenever possible.\n\n\nShape\nThe easiest aesthetic to pair color with is the next most frequently used – shape. This one is much more intuitive than color – to demonstrate, let’s go back to our scatter plot:\n\nWe can now change the shape of each point based on what class of vehicle it represents: \nImagine we were doing the same exercise as we did with color earlier – which values are larger?\nI’ve spoiled the answer already by telling you what the shapes represent – none of them are inherently larger than the others. Shape, like hue, is an unordered value.\nThe same basic concepts apply when we change the shape of lines, not just points. For instance, if we plot separate trendlines for front-wheel, rear-wheel, and four-wheel drive cars, we can use linetype to represent each type of vehicle:\n\nBut even here, no one linetype implies a higher or lower value than the others.\nThere are two caveats to be made to this rule, however. For instance, if we go back to our original scatter plot and change which shapes we’re using:\n\nThis graph seems to imply more connection between the first three classes of car (which are all different types of diamonds) and the next three classes (which are all types of triangle), while singling out SUVs. In this way, we’re able to use shape to imply connection between our groupings - more similar shapes, which differ only in angle or texture, imply a closer relationship to one another than to other types of shape. This can be a blessing as well as a curse - if you pick, for example, a square and a diamond to represent two unrelated groupings, your audience might accidentally read more into the relationship than you had meant to imply.\nIt’s also worth noting that different shapes can pretty quickly clutter up a graph. As a general rule of thumb, using more than 3-4 shapes on a graph is a bad idea, and more than 6 means you need to do some thinking about what you actually want people to take away.\n\n\nSize\nOur last aesthetic is that of size. Going back to our original scatter plot, we could imagine using size like this:\n\nSize is an inherently ordered value - large size points imply larger values. Specifically, humans perceive larger areas as corresponding to larger values - the points which are three times larger in the above graph are about three times larger in value, as well.\nThis becomes tricky when size is used incorrectly, either by mistake or to distort the data. Sometimes an analyst maps radius to the variable, rather than area of the point, resulting in graphs as the below:\n\nIn this example, the points representing a cty value of 10 don’t look anything close to 1/3 as large as the points representing 30. This makes the increase seem much steeper upon looking at this chart – so be careful when working with size as an aesthetic that your software is using the area of points, not radius!\nIt’s also worth noting that unlike color – which can be used to distinguish groupings, as well as represent an ordered value – it’s generally a bad idea to use size for a categorical variable. For instance, if we mapped point size to class of vehicle:\n\nWe seem to be implying relationships here that don’t actually exist, like a minivan and midsize vehicle being basically the same. As a result, it’s best to only use size for continuous (or numeric) data.\n\n\nA Tangent\nNow that we’ve gone over these four aesthetics, I want to go on a quick tangent. When it comes to how quickly and easily humans perceive each of these aesthetics, research has settled on the following order:\n\nPosition\nSize\nColor (especially chroma and luminescence)\nShape\n\nAnd as we’ve discussed repeatedly, the best data visualization is one that includes exactly as many elements as it takes to deliver a message, and no more. Everything should be made as simple as possible, but no simpler.\nHowever, we live in a world of humans, where the scientifically most effective method is not always the most popular one. And since color is inherently more exciting than size as an aesthetic, the practitioner often finds themselves using colors to denote values where size would have sufficed. And since we know that color should usually be used alongside shape in order to be more inclusive in our visualizations, size often winds up being the last aesthetic used in a chart. This is fine - sometimes we have to optimize for other things than “how quickly can someone understand my chart”, such as “how attractive does my chart look” or “what does my boss want from me”. But it’s worth noting, in case you see contradictory advice in the future - the disagreement comes from if your source is teaching the most scientifically sound theory, or the most applicable practice.\n\n\nSummary\nWe started off this section with our second mantra: that everything should be made as simple as possible, but no simpler. The first half of that cautions us against overusing aesthetics and against adding too much to a graphic, lest we erode its efficency in conveying information:\n\nThe second half cautions us against not using all the aesthetics it takes to tell our story, in case we don’t produce the most expressive graphic possible: \nInstead, we should use exactly as many aesthetics as it takes to tell our story, carefully choosing each to encode the most information possible into our graphics: \nAs for the specific takeaways from this section, I can think of the following:\n\nMatch perceptual and data topology – if a color or position feels like a higher value, use it to represent data that is a higher value\nMake important comparisons easy – place them near each other, call attention to them\nUse aesthetics to encode more information into your graphics\n\nUse exactly as many aesthetics as you need – no more, no less.\n\nDon’t place things in alphabetical order\nDon’t use the rainbow for a color scheme\nUse ordered aesthetics (like position, chroma, luminescence, and size) to show ordered values (like numeric data)\nUse unordered aesthetics (like hue or shape) to show unordered values\n\nLet’s transition away from aesthetics, and towards our third mantra:\n\n\nUse the right tool for the job.\nThink back to our first chart:\n\nAs you already know, this is a scatter plot - also known as a point graph. Now say we added a line of best fit to it:\n\nThis didn’t stop being a scatter plot once we drew a line on it – but the term scatter plot no longer really encompasses everything that’s going on here. It’s also obviously not a line chart, as even though there’s a line on it, it also has points.\nRather than quibble about what type of chart this is, it’s more helpful to describe what tools we’ve used to depict our data. We refer to these as geoms, short for geometries – because when you get really deep into things, these are geometric representations of how your data set is distributed along the x and y axes of your graph. I don’t want to get too far down that road – I just want to explain the vocabulary so that we aren’t talking about what type of chart that is, but rather what geoms it uses. Framing things that way makes it easier to understand how things can be combined and reformatted, rather than assuming each type of chart can only do one thing.\n\n\nTwo continuous variables\nThis chart uses two geoms that are really good for graphs that have a continuous y and a continuous x - points and lines. This is what people refer to most of the time when they say a line graph - a single smooth trendline that shows a pattern in the data. However, a line graph can also mean a chart where each point is connected in turn:\n\nIt’s important to be clear about which type of chart you’re expected to produce! I always refer to the prior as a trendline, for clarity.\nThese types of charts have enormous value for quick exploratory graphics, showing how various combinations of variables interact with one another. For instance, many analysts start familiarizing themselves with new data sets using correlation matrices (also known as scatter plot matrices), which create a grid of scatter plots representing each variable:\n\nIn this format, understanding interactions between your data is quick and easy, with certain variable interactions obviously jumping out as promising avenues for further exploration.\nTo back up just a little, there’s one major failing of scatter plots that I want to highlight before moving on. If you happen to have more than one point with the same x and y values, a scatter plot will just draw each point over the previous, making it seem like you have less data than you actually do. Adding a little bit of random noise - for instance, using RAND() in Excel - to your values can help show the actual densities of your data, especially when you’re dealing with numbers that haven’t been measured as precisely as they could a have been. \nOne last chart that does well with two continuous variables is the area chart, which resembles a line chart but fills in the area beneath the line: \nArea plots make sense when 0 is a relevant number to your data set – that is, a 0 value wouldn’t be particularly unexpected. They’re also frequently used when you have multiple groupings and care about their total sum:\n\n(This new data set is the “diamonds” data set, representing 54,000 diamonds sizes, qualities, cut, and sale prices. We’ll be going back and forth using it and the EPA data set from now on.)\nNow one drawback of stacked area charts is that it can be very hard to estimate how any individual grouping shifts along the x axis, due to the cumulative effects of all the groups underneath them. For instance, there are actually fewer “fair” diamonds at 0.25 carats than at 1.0 – but because “ideal” and “premium” spike so much, your audience might draw the wrong conclusions. In situations where the total matters more than the groupings, this is alright – but otherwise, it’s worth looking at other types of charts as a result.\n\n\nOne continuous variable\nIf instead you’re looking to see how a single continuous variable is distributed throughout your data set, one of the best tools at your disposal is the histogram. A histogram shows you how many observations in your data set fall into a certain range of a continuous variable, and plot that count as a bar plot:\n\nOne important flag to raise with histograms is that you need to pay attention to how your data is being binned. If you haven’t picked the right width for your bins, you might risk missing peaks and valleys in your data set, and might misunderstand how your data is distributed – for instance, look what shifts if we graph 500 bins, instead of the 30 we used above: \nAn alternative to the histogram is the frequency plot, which uses a line chart in the place of bars to represent the frequency of a value in your data set: \nAgain, however, you have to pay attention to how wide your data bins are with these charts – you might accidentally smooth over major patterns in your data if you aren’t careful! \nOne large advantage of the frequency chart over the histogram is how it deals with multiple groupings – if your groupings trade dominance at different levels of your variable, the frequency graph will make it much more obvious how they shift than a histogram will.\n(Note that I’ve done something weird to the data in order to show how the distributions change below.) \n\n\nOne categorical variable, one continuous\nIf you want to compare a categorical and continuous variable, you’re usually stuck with some form of bar chart:\n\nThe bar chart is possibly the least exciting type of graph in existence, mostly because of how prevalent it is – but that’s because it’s really good at what it does. Bar charts are one of the most easily interpreted and effective types of visualizations, no matter how exciting they are.\nHowever, some people are really intent on ruining that. Take, for instance, the stacked bar chart, often used to add a third variable to the mix:\n\nCompare Fair/G to Premium/G. It’s next to impossible to accurately compare the boxes – they don’t share a top or a bottom line, so you can’t really make a comparison. In these situations, it’s a better idea to use a dodged bar chart instead:\n\nDodged bar charts are usually a better choice for comparing the actual numbers of different groupings. However, this chart does a good job showing one of the limitations dodged bar charts come up against – once you get past 4 or 5 groupings, making comparisons is tricky. In these cases, you’re probably trying to apply the wrong chart for the job, and should consider either breaking your chart up into smaller ones – remember, ink is cheap, and electrons or cheaper – or replacing your bars with a few lines.\nThe one place where stacked bar charts are appropriate, however, is when you’re comparing the relative proportions of two different groups in each bar. For instance, take the following graph:\n\nIn this case, making comparisons across groups is trivial, made simple by the fact that the groupings all share a common line - at 100% for group 1, and at 0% for group 2. This point of reference solves the issue we had with more than two groupings – though note we’d still prefer a dodged bar chart if the bars didn’t always sum to the same amount.\n\nA Quick Tangent\nThis is usually where most people will go on a super long rant about pie charts and how bad they are. They’re wrong, but in an understandable way.\nPeople love to hate on pie charts, because they’re almost universally a bad chart. However, if it’s important for your viewer to be able to quickly figure out what proportion two or more groupings make up of the whole, a pie chart is actually the fastest and most effective way to get the point across. For instance, compare the following pie and bar charts, made with the same data set: \n\nIt’s a lot easier to tell that, say, A is smaller than C through F in the pie chart than the bar plot, since humans are better at summing angles than areas. In these instances, feel free to use a pie chart – and to tell anyone giving you flack that I said it was OK.\n\n\n\nTwo categorical variables\nOur last combination is when you’re looking to have a categorical variable on both the x and y axis. These are trickier plots to think about, as we no longer encode value in position based on how far away a point is from the lower left hand corner, but rather have to get creative in effectively using position to encode a value. Remember that a geom is a geometric representation of how your data set is distributed along the x and y axes of your graph. When both of your axes are categorical, you have to get creative to show that distribution.\nOne method is to use density, as we would in a scatter plot, to show how many datapoints you have falling into each combination of categories graphed. You can do this by making a “point cloud” chart, where more dense clouds represent more common combinations: \nEven without a single number on this chart, its message is clear - we can tell how our diamonds are distributed with a single glance. A similar way to do this is to use a heatmap, where differently colored cells represent a range of values:\n\nI personally think heatmaps are less effective – partially because by using the color aesthetic to encode this value, you can’t use it for anything else – but they’re often easier to make with the resources at hand."
  },
  {
    "objectID": "posts/2020/04/theory-of-data-viz/index.html",
    "href": "posts/2020/04/theory-of-data-viz/index.html",
    "title": "Theory of Data Visualizations",
    "section": "",
    "text": "Data visualization – our working definition will be “the graphical display of data” – is one of those things like driving, cooking, or being funny: everyone thinks they’re really great at it, because they’ve been doing it for a while, and yet many – if not most – people don’t even know where they could start learning how much better they could be doing things. For something so essential to so many people’s daily work, data visualization is so rarely directly taught, and is usually assumed to be something people will pick up with time.\nHowever, that isn’t the best approach. Data visualization is a skill like any other, and even experienced practitioners could benefit from honing their skills in the subject. Hence, this series.\nThis series doesn’t set out to teach you how to make a specific graphic in a specific software. I don’t know what softwares might be applicable to your needs in the future, or what visualizations you’ll need to formulate when, and quite frankly Google exists – so this isn’t a cookbook with step-by-step instructions. The goal here is not to provide you with recipes for the future, but rather to teach you what flour is – to introduce you to the basic concepts and building blocks of effective data visualizations.\n\nThe mantras\nAs much as possible, I’ve collapsed those concepts into four mantras we’ll return to throughout this course. The mantras are:\n\nA good graphic tells a story.\nEverything should be made as simple as possible, but no simpler.\nUse the right tool for the job.\nInk is cheap. Electrons are even cheaper.\n\nEach mantra serves as the theme for a section, and will also be interwoven throughout. The theme of this section is, easily enough:\n\n\nA good graphic tells a story\nWhen making a graphic, it is important to understand what the graphic is for. After all, you usually won’t make a chart that is an exact depiction of your data – modern data sets tend to be too big (in terms of number of observations) and wide (in terms of number of variables) to depict every datapoint on a single graph. Instead, the analyst consciously chooses what elements to include in a visualization in order to identify patterns and trends in the data in the most effective manner possible. In order to make those decisions, it helps a little to think both about why and how graphics are made.\n\n\nWhy do we tell a story?\nAs far as the why question goes, the answer usually comes down to one of two larger categories:\n\nTo help identify patterns in a data set, or\nTo explain those patterns to a wider audience\n\nThese are the rationales behind creating what are known as, respectively, exploratory and explanatory graphics. Exploratory graphics are often very simple pictures of your data, built to identify patterns in your data that you might not know exist yet. Take for example a simple graphic, showing tree circumference as a function of age:\n\nThis visualization isn’t anything too complex – two variables, thirty-five observations, not much text – but it already shows us a trend that exists in the data. We could use this information, if we were so inspired, to start investigating the whys of why tree growth changes with age, now that we’re broadly aware of how it changes.\nExplanatory graphs, meanwhile, are all about the whys. Where an exploratory graphic focuses on identifying patterns in the first place, an explanatory graphic aims to explain why they happen and – in the best examples – what exactly the reader is to do about them. Explanatory graphics can exist on their own or in the context of a larger report, but their goals are the same: to provide evidence about why a pattern exists and provide a call to action. For instance, we can reimagine the same tree graph with a few edits in order to explain what patterns we’re seeing:\n\nknitr::include_graphics(\"theory2.png\")\n\n\n\n\nI want to specifically call out the title here: “Orange tree growth tapers by year 4.” A good graphic tells a story, remember. As such, whatever title you give your graph should reflect the point of that story – titles such as “Tree diameter (cm) versus age (days)” and so on add nothing that the user can’t get from the graphic itself. Instead, use your title to advance your message whenever it makes sense – otherwise, if it doesn’t add any new information, you’re better off erasing it altogether.\nThe important takeaway here is not that explanatory graphics are necessarily more polished than exploratory ones, or that exploratory graphics are only for the analyst – periodic reporting, for instance, will often use highly polished exploratory graphics to identify existing trends, hoping to spur more intensive analysis that will identify the whys. Instead, the message is that knowing the end purpose of your graph – whether it should help identify patterns in the first place or explain how they got there – can help you decide what elements need to be included to tell the story your graphic is designed to address.\n\n\nHow do we tell a story?\nThe other important consideration when thinking about graph design is the actual how you’ll tell your story, including what design elements you’ll use and what data you’ll display. My preferred paradigm when deciding between the possible “hows” is to weigh the expressiveness and effectiveness of the resulting graphic – as defined by Jeffrey Heer at the University of Washington, that means:\n\nExpressiveness: A set of facts is expressible in a visual language if the sentences (i.e. the visualizations) in the language express all the facts in the set of data, and only the facts in the data.\n\nEffectiveness: A visualization is more effective than another visualization if the information conveyed by one visualization is more readily perceived than the information in the other visualization.\n\nOr, to simplify:\n\nTell the truth and nothing but the truth (don’t lie, and don’t lie by omission)\nUse encodings that people decode better (where better = faster and/or more accurate)\n\nKeep this concept in the back of your mind as we move into the mechanics of data visualization in our next post – it should be your main consideration while deciding which elements you use! We’ll keep returning to these ideas of explanatory and exploratory, as well as expressiveness and effectiveness, throughout the rest of this article."
  },
  {
    "objectID": "posts/2020/04/making-excellent-viz/index.html",
    "href": "posts/2020/04/making-excellent-viz/index.html",
    "title": "Making Excellent Visualizations",
    "section": "",
    "text": "Ink is cheap. Electrons are even cheaper.\nThis is a fancy, dogmatic way to say: Make more than one chart. It’s rare that your first try is going to produce your best looking output. Play around with your data set, try out different visuals, and keep the concepts we’ve talked about in mind. Your graphs will be all the better for it. In this section, we’ll talk about solutions to some of the most common problems people have with making charts:\n\n\nDealing with big data sets\nThink back to the diamonds data set we used in the last section. It contains data on 54,000 individual diamonds, including the carat and sale price for each. If we wanted to compare those two continuous variables, we might think a scatter plot would be a good way to do so:\n\nUnfortunately, it seems like 54,000 points is a few too many for this plot to do us much good! This is a clear case of what’s called overplotting – we simply have too much data on a single graph.\nThere are three real solutions to this problem. First off, we could decide simply that we want to refactor our chart, and instead show how a metric – such as average sale price – changes at different carats, rather than how our data is distributed:\n\nThere are all sorts of ways we can do this sort of refactoring – if we wanted, we could get a very similar graph by binning our data and making a bar plot:\n\nEither way, though, we’re not truly showing the same thing as was in the original graph – we don’t have any indication of the actual distribution of our data set along these axes.\nThe second solution solves this problem much more effectively – make all your points semi-transparent:\n\nBy doing this, we’re now able to see areas where our data is much more densely distributed, something that was lost in the summary statistics – for instance, it appears that low-carat diamonds are much more tighly grouped than higher carat ones. We can also see some dark stripes at “round-number” values for carat – that indicates to me that our data has some integrity issues, if appraisers are more likely to give a stone a rounded number.\nThe challenge with this approach comes when we want to map a third variable – let’s use cut – in our graphic. We can try to change the aesthetics of our graph as usual:\n\nBut unfortunately the sheer number of points drowns out most of the variance in color and shape on the graphic. In this case, our best option may be to turn to option number three and facet our plots – that is, to split our one large plot into several small multiples:\n\nRemember: Ink is cheap. Electrons are even cheaper. Make more than one graph.\nBy splitting out our data into several smaller graphics, we’re much better able to see how the distribution shifts between our categories. In fact, we could use this technique to split our data even further, into a matrix of scatter plots showing how different groups are distributed:\n\nOne last, extremely helpful use of faceting is to split apart charts with multiple entangled lines: \nThese charts, commonly referred to as “spaghetti charts”, are usually much easier to use when split into small multiples:\n\nNow, one major drawback of facet charts is that they can make comparisons much harder – if, in our line chart, it’s more important to know that most clarities are similar in price at 2 carats than it is to know how the price for each clarity changes with carat, then the first chart is likely the more effective option. In those cases, however, it’s worth reassessing how many lines you actually need on your graph – if you only care about a few clarities, then only include those lines, and if you only care about a narrow band of prices or carats, window your data so that’s all you show. The goal is to make making comparisons easy, with the understanding that some comparisons are more important than others.\n\n\nDealing with chartjunk\nCast your mind back to the graphic I used as an example of an explanatory chart:\n\nYou might have noticed that this chart is differently styled from all the others in this course – it doesn’t have the grey background or grid lines or anything else.\nThink back to our second mantra: everything should be made as simple as possible, but no simpler. This chart reflects that goal. We’ve lost some of the distracting elements – the colored background and grid lines – and changed the other elements to make the overall graphic more effective. The objective is to have no extraneous element on the graph, so that it might be as expressive and effective as possible. This usually means using minimal colors, minimal text, and no grid lines. (After all, those lines are usually only useful in order to pick out a specific value – and if you’re expecting people to need specific values, you should give them a table!)\nThose extraneous elements are known as chartjunk. You see this a lot with graphs made in Excel – they’ll have dark backgrounds, dark lines, special shading effects or gradients that don’t encode information, or – worst of all – those “3D” bar/line/pie charts, because these things can be added with a single click. However, they tend to make your graphics less effective as they force the user to spend more time separating data from ornamentation. Everything should be made as simple as possible, but no simpler; every element of your graphic should increase expressiveness or effectiveness. In short: don’t try to pretty up your graph with non-useful elements.\nAnother common instance of chartjunk is animation in graphics. While animated graphics are exciting and trendy, they tend to reduce the effectiveness of your graphics because as humans, when something is moving we can’t focus on anything else. Check out these examples from the Harvard Vision Lab – they show just how hard it is to notice changes when animation is added. This isn’t to say you can never use animation – but its uses are best kept to times when your graphic looking cool is more important than it conveying information.\n\n\nCommon Mistakes\nAs we wind down this section, I want to touch on a few common mistakes that didn’t have a great home in any other section – mostly because we were too busy talking about good design principles.\n\n\nDual y axes\nChief amongst these mistakes are plots with two y axes, beloved by charlatans and financial advisors since days unwritten. Plots with two y axes are a great way to force a correlation that doesn’t really exist into existence on your chart, through manipulation of your units and axes. In almost every case, you should just make two graphs – ink is cheap. Electrons are even cheaper.\nFor an extremely entertaining read on this subject, check out this link. I’ve borrowed Kieran’s code for the below viz – look at how we can imply different things, just by changing how we scale our axes!\n\n\n\nOvercomplex visualizations\nAnother common issue in visualizations comes from the analyst getting a little too technical with their graphs. For instance, think back to our original diamonds scatter plot: \nLooking at this chart, we can see that carat and price have a positive correlation – as one increases, the other does as well. However, it’s not a linear relationship; instead, it appears that price increases faster as carat increases.\nThe more statistically-minded analyst might already be thinking that we could make this relationship linear by log-transforming the axes – and they’d be right! We can see a clear linear relationship when we make the transformation:\n\nUnfortunately, transforming your visualizations in this way can make your graphic hard to understand – in fact, only about 60% of professional scientists can even understand them. As such, transforming your axes like this tends to reduce the effectiveness of your graphic – this type of visualization should be reserved for exploratory graphics and modeling, instead.\n\n\nConclusion\nAnd that just about wraps up this introduction to the basic concepts of data visualizations. Hopefully you’ve picked up some concepts or vocabulary that can help you think about your own visualizations in your daily life. I wanted to close out here with a list of resources I’ve found helpful in making graphics – I’ll keep adding to this over time:\n\nWhen picking colors, I often find myself reaching for one of the following tools:\n\nColorBrewer provided most of the palettes for these graphics\nColorSupply makes picking custom colors easier\nViridis provides beautiful, colorblind-friendly palettes for use (though this resource is a little harder to understand)\n\nI used the following resources in putting this post together:\n\nHadley Wickham’s Stat 405 Course, particularly the lecture on effective visualizations (I’ve lifted “perceptual topology should match data toplogy”, “make important comparisons easy”, and “visualization is only one part of data analysis” directly from his slides)\nJeffrey Heer’s CSE 442 lecture on visualizations, particularly the definitions for expressiveness and effectiveness"
  },
  {
    "objectID": "posts/2020/04/corona-viz/index.html",
    "href": "posts/2020/04/corona-viz/index.html",
    "title": "A minimalist visualization of Coronavirus rates",
    "section": "",
    "text": "knitr::include_graphics(\"covid.png\")\n\n\n\n\nThe app lives at this link. Thanks to R Studio, who are providing free hosting for coronavirus apps through the pandemic."
  },
  {
    "objectID": "posts/2020/06/index.html",
    "href": "posts/2020/06/index.html",
    "title": "Make a Retweet Bot in R",
    "section": "",
    "text": "I didn’t want to get into the business of content filtering, so instead I started looking for other markers that a tweet was – or wasn’t – worth promoting, and have gotten to what I believe is a respectable place with my filtration. So I’ve open-sourced the code (without the specific values I use to filter) for anyone else who might be interested in setting up their own automated retweet app."
  },
  {
    "objectID": "posts/2020/03/index.html",
    "href": "posts/2020/03/index.html",
    "title": "Announcing {spacey}, now on CRAN!",
    "section": "",
    "text": "The package has a documentation website built with pkgdown – check it out for more information!"
  },
  {
    "objectID": "posts/2020/10/index.html",
    "href": "posts/2020/10/index.html",
    "title": "Some Updates",
    "section": "",
    "text": "It’s been a busy few months! Rather than try to make separate update posts for everything I’ve been up to, here’s a list of the top bullets from my past four months."
  },
  {
    "objectID": "posts/2020/10/index.html#new-digs-and-new-gigs",
    "href": "posts/2020/10/index.html#new-digs-and-new-gigs",
    "title": "Some Updates",
    "section": "New Digs and New Gigs",
    "text": "New Digs and New Gigs\nI’ve left Boston and left Wayfair to take a PhD position at SUNY-ESF in Syracuse, working with Colin Beier and Aidan Ackerman to make 3D landscape visualizations as a way to improve the interpretability of ecological models and help democratize scientific outputs."
  },
  {
    "objectID": "posts/2020/10/index.html#paper-stem-size-selectivity-is-stronger-than-species-preferences-for-beaver-a-central-place-forager",
    "href": "posts/2020/10/index.html#paper-stem-size-selectivity-is-stronger-than-species-preferences-for-beaver-a-central-place-forager",
    "title": "Some Updates",
    "section": "Paper: Stem size selectivity is stronger than species preferences for beaver, a central place forager",
    "text": "Paper: Stem size selectivity is stronger than species preferences for beaver, a central place forager\nMy first first-authored paper is out in Forest Ecology and Management! I wrote a little about the process of this paper, from start to finish, on my extremely short-lived newsletter."
  },
  {
    "objectID": "posts/2020/10/index.html#tweetbots",
    "href": "posts/2020/10/index.html#tweetbots",
    "title": "Some Updates",
    "section": "Tweetbots",
    "text": "Tweetbots\nI’ve now got a small army of robots living in the corner of my apartment, tweeting out to their heart’s content. At the moment, I’ve got three retweet bots running off the original ecology_tweets codebase, namely @ecology_tweets, @rstats_tweets (a more heavily-filtered alternative to the more popular rstatstweets bot), and @30daymap_tweets, built for the #30DayMapChallenge.\nMore interesting are the two GPT-2 “AI” tweetbots now running, including @fortunes_teller and @fund_me_please_. The former is trained against a collection of fortunes packages from various *nix distros and the R fortunes package, while the latter was run against 150 GRFP personal statements and is now tweeting out some frankly bizarre applications of its own making."
  },
  {
    "objectID": "posts/2020/10/index.html#terrainr",
    "href": "posts/2020/10/index.html#terrainr",
    "title": "Some Updates",
    "section": "{terrainr}",
    "text": "{terrainr}\nI’ve got a new R package out, the first real “product” from my PhD. {terrainr} wraps the USGS National Map family of APIs to help users download geospatial data for their areas of interest, and provides functionality to turn those files into tiles that can be imported into Unity for 3D landscape visualization:\n\n\n\n\n\nA 3D landscape visualization of the area southeast of Mt. Whitney, California, USA.\n\n\n\n\nI love writing packages with visual outputs. I love writing packages with visual outputs that look like this."
  },
  {
    "objectID": "posts/2020/10/index.html#misc",
    "href": "posts/2020/10/index.html#misc",
    "title": "Some Updates",
    "section": "Misc",
    "text": "Misc\nI bought a pen plotter."
  },
  {
    "objectID": "posts/2021-06-12-virtual-environments-talk-at-user-2021/index.html",
    "href": "posts/2021-06-12-virtual-environments-talk-at-user-2021/index.html",
    "title": "Virtual environments talk at useR! 2021",
    "section": "",
    "text": "The video should go up on YouTube after the conference, and the slides (and a rough script for the talk) are online at https://github.com/mikemahoney218/user2021 . My favorite line from the script is “R is where our users are” – I didn’t quite realize I had any Boston accent left until I tripped over that sentence a few times."
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "",
    "text": "terrainr version 0.4.0 is now on CRAN! This version is a relatively minor update that shouldn’t impact most workflows, but makes some changes to improve the logic and consistency of the package. The rest of this post runs through the changes you can expect if you update.packages() any time soon!"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#whats-a-terrainr",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#whats-a-terrainr",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "What’s a terrainr?",
    "text": "What’s a terrainr?\nterrainr is an R package for the retrieval and visualization of spatial data. It provides functions to download elevation data and basemap tiles for points within the United States (using public domain data from the USGS National Map), visualize them in R via ggplot2, and process them for 3D visualization using the Unity 3D engine. You can see the GitHub repo here!"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#merge_rasters-can-handle-tiles-with-different-numbers-of-bands",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#merge_rasters-can-handle-tiles-with-different-numbers-of-bands",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "merge_rasters can handle tiles with different numbers of bands",
    "text": "merge_rasters can handle tiles with different numbers of bands\nThe old implementation of merge_rasters was very bulky, read all your map tiles into memory at once, and was a bit of a mess to maintain thanks to the large number of paths you could theoretically take through the code. The commit (suggested via rOpenSci review!) replacing it with gdalwarp via sf is probably the single best code improvement I’ve made to this repo. Unfortunately, the old method could also handle merging rasters with differing numbers of bands, while the simple gdalwarp fix couldn’t.\nSo the old implementation is back as an internal method while I look for a better solution to this problem. merge_rasters will now attempt to use gdalwarp to merge your input files and then fall back to (a massively simplified version of) the older version if gdalwarp fails.\nAs for why you’d want to automatically merge rasters with different numbers of bands, well…"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#get_tiles-doesnt-auto-download-transparency-values-for-naip",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#get_tiles-doesnt-auto-download-transparency-values-for-naip",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "get_tiles doesn’t auto-download transparency values for NAIP",
    "text": "get_tiles doesn’t auto-download transparency values for NAIP\nNAIP orthophotography provides fantastic continuous 1-meter images for the continental United States. When downloading these photos with the argument transparency = true, which used to be the default, most photos don’t have any transparent pixels to talk about and as such are returned and saved as 3 band rasters (RGB images). Some photos, however, do have such pixels and are returned with a 4th alpha band. This causes problems with gdalwarp as well as image editing software, and the majority of the time users are not better served by these pixels being transparent.\nAs a result, this version changes the default transparency argument for get_tiles and hit_national_map_api to false when downloading NAIP images (no other data source is impacted). This is one of the reasons this version gets a 0.x.0 number – while it should be a small change, the same inputs to functions no longer returns the same outputs (though I doubt people would notice), so I’m counting this as a breaking change.\nThere’s a slightly more impactful breaking change worth noting though:"
  },
  {
    "objectID": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#functions-pay-attention-to-the-provided-crs",
    "href": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/index.html#functions-pay-attention-to-the-provided-crs",
    "title": "What’s new in terrainr 0.4.0?",
    "section": "Functions pay attention to the provided CRS",
    "text": "Functions pay attention to the provided CRS\nThis header is actually about two distinct changes.\nFirst, another new behavior with get_tiles is that rather than assuming the provided data and downloaded image should both be using the WGS84 CRS (EPSG 4326), get_tiles will now infer the EPSG CRS from any provided sf or Raster object. If the numeric code is missing, this function will still assume 4326.\nSimilarly, rather than specifying target_crs in vector_to_overlay, this function will now return an overlay projected to match reference_raster’s CRS. Missing CRS are handled slightly differently here – if the error_crs argument is NULL, this function will warn; if FALSE it will assume 4326, and if TRUE it will interrupt the function with an error.\nThose are the major changes to this iteration! On top of these there are some minor changes to the package internals, slowly removing dead code paths and simplifying things behind the scenes. If you have any problems (bugs or missing features) with the package, feel free to open an issue!"
  },
  {
    "objectID": "cdi/index.html",
    "href": "cdi/index.html",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "",
    "text": "This is an R Markdown document containing code for the workshop “Accessing the USGS National Map and making 3D maps with terrainr”, held virtually on 2021-05-28. If you want to follow along with the workshop as we go, click this link to download the R Markdown notebook.\nIf you’re not familiar with R Markdown, this document lets us write both plain text and code in a single document. Document sections inside three ` marks are called “code chunks”:\nA single line in a code chunk can be run by pressing Control+Enter with your cursor on the line. The entire chunk can be run by pressing Control+Shift+Enter with your cursor inside the chunk."
  },
  {
    "objectID": "cdi/index.html#example",
    "href": "cdi/index.html#example",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "Example",
    "text": "Example\nNow that we’ve seen a basic workflow using terrainr to pull layers from the National Map and plot with ggplot2, let’s have some fun with it! Starting with the latitude and longitude coordinates for a new site, we’ll create an sf object and add a spatial buffer around it like before.\n\n# yosemite NP\nsite <- data.frame(lat = 37.7456, lng = -119.5521)\n\n# convert to sf object and assign coordinate reference system \nsite_sf <- st_as_sf(site, coords = c(\"lng\",\"lat\"), crs = 4326)\n\n# buffer around point\nsite_bbox <- set_bbox_side_length(site_sf, 10, \"km\")\n\nNow, to pull some data using terrainr. In the previous example we looked at elevation and orthoimagery. Other supported services are: nhd, govunits, contours, geonames, NHDPlus_HR, structures, transportation, and wbd. More info here: https://docs.ropensci.org/terrainr/#available-datasets\nLet’s try elevation and contours this time:\n\nwith_progress(\n  site_tiles <- get_tiles(site_bbox, \n                            output_prefix = \"yosemite_np_5m\",\n                            services = c(\"elevation\", \"contours\"),\n                            resolution = 5)\n)\n\nsite_tiles\n\n$elevation\n[1] \"yosemite_np_5m_3DEPElevation_1_1.tif\"\n\n$contours\n[1] \"yosemite_np_5m_contours_1_1.tif\"\n\n\nNow we can convert the elevation and contour layers to plot with ggplot2. The contours layer is an RGBA multi-band raster layer, similar to the orthoimagery.\n\nelevation <- raster(site_tiles$elevation)\ncontours <- stack(site_tiles$contours)\n\n# look at structure of contours raster - has 4 layers\ncontours\n\nclass      : RasterStack \ndimensions : 2000, 1999, 3998000, 4  (nrow, ncol, ncell, nlayers)\nresolution : 0.00005689511, 0.00005689511  (x, y)\nextent     : -119.6089, -119.4952, 37.68869, 37.80248  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nnames      :     lyr.1,     lyr.2,     lyr.3,     lyr.4 \nmin values : 0.6000000, 0.4117647, 0.2235294, 0.0000000 \nmax values :         1,         1,         1,         1 \n\n\nRGBA has 4 channels: red, green, blue, and alpha. The 4th channel, alpha, is like opacity. In this case, alpha is 0 in cells without contour lines and 1 where contours exist.\n\nggplot() + \n  geom_spatial_rgb(data = contours,\n                   mapping = aes(x, y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326) + \n  theme_void()\n\n\n\n\nContours reflect elevation, so let’s make it so! The next chunk stacks elevation and contours together and coverts to a dataframe to plot using geom_raster with ggplot2. Then, using the alpha channel, the dataframe is filtered to just the contour lines, and elevation values are used in the color scale.\n\ncontour_stack <- stack(elevation, contours)\ncontour_df <- as.data.frame(contour_stack, xy = TRUE)\n\nnames(contour_df) <- c(\"x\", \"y\", \"z\", \"r\", \"g\", \"b\", \"a\") # longitude, latitude, elevation, red, green, blue, alpha\n\ncontour_lines <- contour_df[contour_df$a != 0, ]\n\n## plot it!\nggplot(data = contour_lines)+\n  geom_raster(mapping = aes(x, y, fill = z))+\n  theme_void()+\n  theme(plot.background = element_rect(fill=\"black\"),\n        plot.title = element_text(color=\"white\"),\n        legend.position = \"none\")+\n  coord_sf(crs = 4326)+\n  scale_fill_scico(palette=\"imola\", direction = 1)+\n  ggtitle(paste(\"Half dome\", site$lat, site$lng))\n\n\n\n\nThis chart uses the scico package for color scale. Scico (https://github.com/thomasp85/scico) offers a selection of color palettes developed for scientific applications. They are perceptually uniform to represent data fairly, and universally readable by both color-vision deficient and color-blind people.\n\nscico_palette_show() # to see all palette options\n\n\n\n\nFor the purposes of the workshop, we’re avoiding large downloads. But if we were to increase the resolution even more, say to 1m, terrainr might break up the focal region into multiple tiles:\n\n## warning: running this code chunk will take over 5 minutes to complete\nwith_progress(\n  site_tiles_1 <- get_tiles(site_bbox, \n                            output_prefix = \"yosemite_np_10m\",\n                            services = c(\"elevation\", \"contours\"),\n                            resolution = 1)\n)\n\nsite_tiles_1\n\nIn which case they can be merged together using merge_rasters\n\nmerge_el <- terrainr::merge_rasters(site_tiles_1$elevation, \n                          tempfile(fileext = \".tif\"))\n\nmerge_co <- terrainr::merge_rasters(site_tiles_1$contours, \n                          tempfile(fileext = \".tif\"))"
  },
  {
    "objectID": "cdi/index.html#on-your-own",
    "href": "cdi/index.html#on-your-own",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "On your own",
    "text": "On your own\nUse the dataframe below to pull new starting coordinates and recreate the map in a new location, with new layer, or maybe a new color palette. Share what you made in the chat!\n\n# dataframe of cool places in the US\nplaces <- structure(list(name = c(\"half dome\", \"moki dugway\", \"badwater basin\", \n\"mount whitney\", \"monument valley navajo tribal park\", \"devil's lake\", \n\"cadillac mountain\", \"mcafee knob\", \"sleeping bear dunes \", \"grand canyon\", \n\"miguel's pizza at red river gorge\", \"between cumberland and blue ridge mountains\", \n\"missouri river floodplain and bluff\", \"shawangunk mountains\", \n\"lake winnipesaukee, new hampshire\", \"mount washington, new hampshire\", \n\"gravity hill, pa\", \"orcas island\", \"grand canyon of yellowstone\", \n\"mauna loa\", \"tetons\", \"rocky mountain biological station\"), \n    lat = c(37.746036, 37.302306, 36.250278, 36.578581, 36.983333, \n    43.420033, 44.35127, 37.392487, 44.878727, 36.2388, 37.783004, \n    37.1177098, 38.568762, 41.72469, 43.606096, 44.270504, 40.153283, \n    48.654167, 44.895556, 19.479444, 43.75, 38.958611), lng = c(-119.53294, \n    -109.968944, -116.825833, -118.291995, -110.100411, -89.737405, \n    -68.22649, -80.036298, -86.066458, -112.350427, -83.682861, \n    -81.132816, -90.883408, -74.204946, -71.338624, -71.303115, \n    -76.839954, -122.938333, -110.389444, -155.602778, -110.833333, \n    -106.987778)), class = \"data.frame\", row.names = c(NA, -22L\n))\n\n# randomly pick one row from the dataframe above\nsite <- places[sample(nrow(places), 1), ]\n\n# Or set your own by changing the values below and uncommenting\n# site <- data.frame(ï..name = \"skywalk @ grand canyon\", lat = 36.011827, lng = -113.810931) \n\n# convert to sf object and assign coordinate reference system \nsite_sf <- st_as_sf(site, coords = c(\"lng\",\"lat\"), crs = 4326)\n\n# buffer around point\nsite_bbox <- set_bbox_side_length(site_sf, 10, \"km\")\n\n# keep going! Using code from previous steps, make the rest of the map.\n\n# Create site name for use as filenames:\nsite_fn <- gsub(\" \", \"_\", gsub(\"[[:punct:]]\", \"\", site$name))\n\n# Download the appropriate data for your site from The National Map\nwith_progress(\n  site_tiles <- get_tiles(site_bbox, \n                          output_prefix = sprintf(\"%s_5m\", site_fn),\n                          services = c(\"elevation\", \"contours\"),\n                          resolution = 5)\n)\n\n# COnvert to a raster stack\nelevation <- raster(site_tiles$elevation)\ncontours <- stack(site_tiles$contours)\ncontour_stack <- stack(elevation, contours)\ncontour_df <- as.data.frame(contour_stack, xy = TRUE)\nnames(contour_df) <- c(\"x\", \"y\", \"z\", \"r\", \"g\", \"b\", \"a\") # longitude, latitude, elevation, red, green, blue, alpha\n\n# Remove zero elevations\ncontour_lines <- contour_df[contour_df$a != 0, ]\n\n## plot it!\nggplot(data = contour_lines)+\n  geom_raster(mapping = aes(x, y, fill = z))+\n  theme_void()+\n  theme(plot.background = element_rect(fill=\"black\"),\n        plot.title = element_text(color=\"white\"),\n        legend.position = \"none\")+\n  coord_sf(crs = 4326)+\n  # Change the color palette used by editing the `palette` argument below!!\n  scale_fill_scico(palette=\"imola\", direction = 1)+\n  ggtitle(paste(site$name, site$lat, site$lng))\n\n\n\n# If you want to save a higher resolution version (it makes the lines a little sharper)\n# Run this after printing your plot:\n# ggsave(sprintf(\"whimsical_topomap_terrainr_%s.png\", site_fn), width=5, height=8)"
  }
]