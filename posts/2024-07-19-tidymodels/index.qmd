---
title: "Converting New York's Forest Carbon Assessment to Tidymodels"
description: "Lessons, pain points, and benefits from a bottom-up rewrite of a production modeling pipeline"
author:
  - name: Mike Mahoney
    url: {}
date: "2024-07-19"
categories: ["tidymodels", "R", "Tutorials", "AGB", "Data science", "Spatial", "geospatial data", "machine learning"]
image: banner.jpg
format: 
  html:
    toc: true
engine: knitr
---

```{r include = FALSE}
options(
  warnPartialMatchArgs = FALSE,
  warnPartialMatchDollar = FALSE,
  warnPartialMatchAttr = FALSE
)
terra::terraOptions(progress=0, memfrac = 0.7)
```

We've been working this summer on version 2.0 of [the New York Forest Carbon Assessment](https://cafri-ny.org/new-york-forest-carbon-assessment/), incorporating changes in [how the US estimates its forest carbon resources](https://arxiv.org/abs/2405.04507) and building on our past work using [satellite imagery](https://doi.org/10.1016/j.foreco.2023.121348) and [LiDAR patchworks](https://doi.org/10.1016/j.jag.2022.103059) to create wall-to-wall estimates of forest carbon stocks across New York for the last 30 years.

One of the big themes of version 2.0 is operationalization. We're working to automate as much of our modeling and mapping pipelines as possible, to streamline generating annual maps as time goes on and to make it easier to experiment with new models and data sources in the future. In practice, this means I've been spending a lot of time merging individual scripts into [targets](https://books.ropensci.org/targets/) pipelines, and generally looking at ways to streamline our processes.

The biggest change to come from this rewriting is that, while version 1.0 of our modeling pipeline used an internal library for tuning and [creating stacked ensembles](https://www.mm218.dev/posts/2021/01/model-averaging/), version 2.0 shifts these tasks over to the [tidymodels framework](https://www.tidymodels.org/).^[A slightly less relevant change is that I wrote most of the pipeline while [beta testing Posit's new Positron IDE](https://github.com/posit-dev/positron), which I mention mostly as a humblebrag. I generally really like Positron -- this post was written using it -- though I have a really hard time specifying why. It just _feels_ nice to use.] Where we had previously only been using the [rsample](https://rsample.tidymodels.org/) and [yardstick](https://yardstick.tidymodels.org/) packages from tidymodels, we've now adopted the framework more-or-less wholesale in order to take advantage of a few advanced features.^[By the way -- if you work on a package I mention in this post, and it would be useful for you to say "Used in New York State's Forest Carbon Assessment" somewhere, drop me a line. We're part of New York's [Climate Act Scoping Plan](https://climate.ny.gov/resources/scoping-plan) and [partner with the state DEC](https://dec.ny.gov/nature/forests-trees/climate-change/climate-smart-farms-forests) on our mapping, and your work is helping make that possible.]

In this post, I want to walk through what that workflow looks like for us -- how we get from our field measurements to prepped data for models, then through hyperparameter tuning, into our final ensemble model construction. And then I want to talk about a few rough edges we ran into through the process, and why we're still using tidymodels despite those stubbed toes.

So, without further ado, let's start walking through how we're building our ensemble models using tidymodels.

## Building ensemble models using tidymodels

### What are we doing here?

My intent with this section is to give a pretty straightforward, if fast-paced, walkthrough of how we're using tidymodels to build our ensemble models. In order to do that, I need to load quite a few packages:^[I'm throwing this in a footnote because it doesn't _really_ fit anywhere else: There's a trendy sort of post where people talk about how the tidyverse or tidymodels ecosystem being made up of many packages is itself inherently bad. I think this is a bad opinion. We're asking these ecosystems to do a lot of complicated things for us, which typically means we're asking them to include a lot of functions and code. It's natural -- and good practice -- for these functions to get broken up into logical units, each providing an interface for users and the rest of the ecosystem to build upon. As such, I think that measuring supply-chain risk based on a pure count of dependencies is the wrong approach. If you're worried about dependencies changing their interfaces or getting archived and breaking your workflows, I personally think you should be more concerned about multitool-packages developed by non-tenured academics than about the ecosystem maintained by a small team of professional software engineers.]

```{r}
# Used to download our predictor data
library(rsi)
# Contains our "field data", used for cross-validation
library(spatialsample)

# Used to wrangle our predictor data
library(sf)
library(terra)
library(exactextractr)

# Tidymodels libraries used to actually fit and tune models
library(tidymodels)
library(hardhat)
library(bonsai)
library(finetune)
library(stacks)
```

In order to fit a stacked ensemble model, we first need to fit a number of "component" models. For our purposes today, we'll fit a [MARS model](https://bradleyboehmke.github.io/HOML/mars.html) using the [earth package](https://cran.r-project.org/package=earth) and a [gradient boosting model](https://bradleyboehmke.github.io/HOML/gbm.html) using [LightGBM](https://cran.r-project.org/package=lightgbm). While we don't need functions from these packages, we do need them to be installed so that tidymodels packages can find them:

```{r}
rlang::check_installed("earth")
rlang::check_installed("lightgbm")
```

We also need some data worth modeling. While the actual field data we use for our models is non-public,^[I'm realizing now, four-ish years into this job, that I don't actually know the right phrasing for FIA data. It's not "confidential", as no one in our group has a security clearance, but we have access to plot locations under an MOU with the Forest Service. I guess "non-public" is the best I've got at the moment.] we can demonstrate the workflow using pretty much any example data. For our purposes, we'll build our models to predict the amount of tree canopy in Boston in 2019, using data shipped with the spatialsample package:

```{r}
library(ggplot2)
ggplot(spatialsample::boston_canopy, aes(fill = canopy_percentage_2019)) + 
    geom_sf() + 
    scale_fill_distiller(palette = "Greens", direction = 1)
```

This will be our outcome variable for our models. 

### Data preprocessing

In order to actually fit the models, we're going to need predictor data too! We'll derive our predictors from Landsat imagery taken in the summer of 2019, when all the trees in Boston were at their greenest.

To actually download this data, we'll use the [rsi package](https://permian-global-research.github.io/rsi/index.html). In order to make the download go faster, we'll first use [future](https://future.futureverse.org/) to register a parallel backend, letting us download multiple pieces of data at the same time:

```{r}
library(future)
plan("multisession")
```

And we can then actually download our Landsat imagery using `rsi::get_landsat_imagery()`:

```{r}
landsat_imagery <- rsi::get_landsat_imagery(
    spatialsample::boston_canopy,
    "2019-06-01",
    "2019-09-01",
    platforms = c("landsat-8", "landsat-9"),
    output_filename = tempfile(fileext = ".tif")
)

terra::plot(terra::rast(landsat_imagery))
```

This winds up generating a median-composite image, made up of all of the "good" data available for this time period. 

We normally don't fit our models using these raw reflectance values, but rather use them to calculate a number of [spectral indices](https://github.com/awesome-spectral-indices/awesome-spectral-indices) that we then use as predictors. We can automate this process using rsi as well; for instance, we can use this code to calculate every spectral index possible using the bands in the raster we downloaded:

```{r}
landsat_indices <- rsi::calculate_indices(
    landsat_imagery,
    rsi::filter_bands(bands = names(terra::rast(landsat_imagery))),
    output_filename = tempfile(fileext = ".tif")
)

terra::plot(terra::rast(landsat_indices))
```

Now we've got a raster with a ton of spectral indices that we can use as predictors. We now need to associate those with our canopy percentage measurements from spatialsample. Generally we accomplish this using [exactextractr](https://github.com/isciences/exactextractr) to calculate the mean of all of these indices within our sample locations:

```{r}
cell_values <- exactextractr::exact_extract(
    terra::rast(landsat_indices),
    spatialsample::boston_canopy,
    "mean",
    progress = FALSE,
    colname_fun = \(values, ...) values
)

cell_values <- cbind(
    spatialsample::boston_canopy["canopy_percentage_2019"], 
    cell_values
    )

head(cell_values)[1:6]
```

We've now got index values at each of our sampled polygons! 

Before we can start using these to predict canopy coverage, we need to do a bit more pre-processing. For instance, for some reason a few of our polygons are missing values in at least one spectral index. While in the real world we'd probably want to debug exactly why this is happening, for our purposes today it's a lot easier to just drop those rows from our data frame:

```{r}
cell_values <- cell_values[
    complete.cases(sf::st_drop_geometry(cell_values)), 
]
```

Similarly, a few indices have non-finite values in at least one row, and we'll want to go ahead and drop those as well:

```{r}
infinite_cols <- vapply(
    sf::st_drop_geometry(cell_values), 
    \(x) any(is.infinite(x)),
    logical(1)
)
cell_values[infinite_cols] <- NULL
```

### Model fitting

We've now got our `cell_values` object fully ready for modeling workflows. We're now able to start the actual model-fitting process.

Our first step is to define the formula we're using to fit our models. Because I've got the `cell_values` dataframe structured with our outcome column in front, followed by all the predictor columns, I can create this formula using the neat `DF2formula()` utility:

```{r}
formula <- DF2formula(sf::st_drop_geometry(cell_values))
```

Did you know that's a function in base R? What a wild language.

Anyway, with our formula built, we're ready to start actually using tidymodels packages. Our first step is to create a recipe that uses our formula, and will pass that information on to the other tidymodels packages:

```{r}
recipe <- recipes::recipe(formula = formula, data = cell_values)
```

Up next, we'll create the model specifications that define what models we want to fit, and pass `hardhat::tune()` to every hyperparameter that we want to be tuned.^[There's a note in the tune documentation that some models -- specifically earth models, but presumbably LightGBM could do the same thing -- [can evalute more models than are fit, speeding up tuning times](https://tune.tidymodels.org/articles/extras/optimizations.html#sub-model-speed-ups). I don't actually see any information on how to combine this trick with tune itself, though, so we're just going to pretend I didn't see that part of the docs.] We'll also set the "engine" that we want to use (the package for fitting the models), and the mode of model:

```{r}
lightgbm_spec <- parsnip::boost_tree(
    tree_depth = hardhat::tune(),
    trees = hardhat::tune(),
    learn_rate = hardhat::tune(),
    mtry = hardhat::tune(),
    min_n = hardhat::tune(),
    loss_reduction = hardhat::tune()
  ) |>
    parsnip::set_engine(engine = "lightgbm") |>
    parsnip::set_mode(mode = "regression")

mars_spec <- parsnip::mars(
    num_terms = hardhat::tune(),
    prod_degree = hardhat::tune()
) |> 
    parsnip::set_engine("earth") |> 
    parsnip::set_mode("regression")
```

We've now got our recipe and our model specifications created, meaning we've defined all the preprocessing and modeling we're asking tidymodels to perform. 

```{r}
workflow_set <- workflowsets::workflow_set(
    list(recipe),
    list(
        lightgbm = lightgbm_spec,
        mars = mars_spec
    )
)
workflow_set
```

I'll talk about this a bit more later, but I think it's worth noting how complex the thing we just did is. The `workflow_set()` function just created a "workflowset", which is a tibble that contains multiple "workflows"^[Which come from a package, workflows, that we won't ever call explicitly here.] which each contain our "recipe" and "model specification" which has some arguments "marked for tuning"^[With parameter values determined by the dials package which, again, we won't call explicitly here]. The code we've written is pretty minimal -- something like 20 lines of code to create the workflowset itself -- but it winds up producing quite a number of abstractions that we need to reason around to actually understand what we're doing.

But with our workflowset created, we're now able to actually tune our component models! Before we do so, **we need to set our future strategy back to sequential processing**. The LightGBM implementation in bonsai uses parallel processing by default, and our tuning code will also try and run in parallel if we have a future backend registered, resulting in thread contention and making tuning never finish. As such, we'll run `plan("sequential")` before tuning:

```{r}
plan("sequential")
```

And then we'll perform our tuning. We need to tune each row of our workflowset -- each unique recipe/model combination -- separately. 

Luckily, this is pretty easy using `workflowsets::workflow_map()`, which can apply a function to each workflow separately. Here, we'll use this to use `finetune::tune_race_anova()` to evaluate our models against 10 different sets of hyperparameters^[Which is a small number chosen to run quickly for this blog post; in practice we tune with a much, much larger grid.] -- making a point to save the predictions and workflows from each set, as we'll need those to build our ensemble model later on.

```{r}
resamples <- spatialsample::spatial_clustering_cv(cell_values, 10)
tuning <- workflowsets::workflow_map(
    workflow_set,
    "tune_race_anova",
    resamples = resamples,
    grid = 10,
    metrics = yardstick::metric_set(yardstick::rmse),
    control = finetune::control_race(save_pred = TRUE, save_workflow = TRUE)
)
```

Our `tuning` object now contains the outputs from the tuning process. We could use this to, for example, find the best evaluated set of hyperparameters for each of our models:

```{r}
tuning$result |> 
    lapply(tune::select_best, metric = "rmse")
```

But we aren't actually all that interested in just-the-best. Because we're going to be combining these models into ensembles, we can actually use all the models fit using _good enough_ hyperparameter sets. That process can happen more or less automatically thanks to [stacks](https://stacks.tidymodels.org/), which will fit a lasso regression to determine which of the candidate models are better than useless and combine them into a final prediction:

```{r}
ensemble <- stacks::stacks() |>
  stacks::add_candidates(tuning) |>
  stacks::blend_predictions() |>
  stacks::fit_members()

ensemble
```

Now, the best thing about stacks^[I mean, in my opinion.] is that predicting with this model is dead simple. Even though predicting with our ensemble requires predicting with each of the selected earth and LightGBM models, and then predicting with the final lasso regression, from our perspective as users we can just call a single, typical `predict()`:

```{r}
predict(ensemble, cell_values) |> head()
```

For our carbon mapping workflows, however, we wind up generating predictions using the same 30 meter raster we grabbed our original predictor values from. Ensembles from stacks also work pretty well with this workflow:

```{r}
terra::predict(
    terra::rast(landsat_indices),
    ensemble
) |>
    terra::plot()
```

We've also been using [bundle](https://rstudio.github.io/bundle/) to help us serialize these ensemble models and load them in new R sessions. The flexibility of bundle is another great plus of leaning in with tidymodels.

## Some complaints

That said, as I mentioned at the start, I've certainly had a few unpleasant experiences through this transition, and I want to talk about them now.

I want to be clear at the start -- I'm not intending for this to be some sort of "tidymodels skeptic" post. We're still using tidymodels, and I think our codebase is more maintainable and -- with some caveats -- understandable following this ground-up rewrite. These are just the things that bit us during that rewriting process.

Another disclaimer here is that I interned with the tidymodels team back in 2022^[Go [star spatialsample on GitHub](https://github.com/tidymodels/spatialsample)] and deeply enjoyed my time with the team. As a result, my experience with this rewrite was pretty nonstandard; I was already pretty familiar with some corners of the ecosystem (especially the parts touching rsample), am pretty willing to push past frustration with other packages based on my trust in the people developing them, and could DM a team member if I was ever particularly stuck. So this isn't the most objective outsider experience you could ever imagine; it's just the one I had.

### Learning one thing requires learning everything

A recurring challenge I have with the tidymodels framework is that often, to understand one part of tidymodels, it can feel like you need to know all of it. 

For example: if we want to automatically tune a hyperparameter, we need to set it to `tune()` inside of our model specification. But what does `tune()` actually do -- how does it determine the hyperparameter values worth searching?

Well, the `tune()` function itself comes from the [hardhat](https://hardhat.tidymodels.org/) package, which describes itself as a "developer focused package designed to ease the creation of new modeling packages". That means that if I'm trying to figure out what this `tune()` function actually does, running `?tune()` winds up getting me the help page [of a developer-focused package](https://hardhat.tidymodels.org/reference/tune.html), and I start wondering how much of that developer-focused documentation I need to read and learn in order to tune my models.

For its part, the `tune()` help page actually points you to `tune::tune_grid()` and `tune::tune_bayes()` for more information on how these parameter values are being set. I don't see any section here that explicitly says how parameter ranges are set when using `tune()` but the "Parameter Grids" section of `tune_grid()` and "Parameter Ranges and Values" section of `tune_bayes` do both discuss tuning parameter ranges. These sections don't mention `tune()`,^[In the context of grid creation -- `tune_grid()` mentions `tune()` in the context of user-provided grids.] but _do_ mention that `dials::finalize()` "can be used to derive the data-dependent parameters".

We don't call `dials::finalize()` ourselves -- we don't actually load `dials` at all -- but if we go to the dials website, we could read the [Getting Started vignette](https://dials.tidymodels.org/articles/dials.html)^[Not linked from the `finalize()` page, but in the top navigation bar.] which starts to explain how parameter ranges are estimated. The "Unknown Values" section sort of explains how some ranges -- like the max value for `mtry` and so on -- are calculated, but I'm still a bit lost on how our `hardhat::tune()` gets turned into a dials function (if it does) and when these parameter ranges are set. 

So I haven't entirely figured out how these grids are made, but I do have five tabs open from three different package websites. At this point, I'm starting to get a little bit buried by the different layers going on here -- and I don't know how much of hardhat, dials, or tune I need to understand before things start making a bit more sense. I also don't have an obvious first point of attack to answer this question, as the packages all link back to each other as places to get further information.^[For what it's worth, a full explanation of how tuning works is in [the tidymodels book](https://www.tmwr.org/tuning), but I didn't see that linked anywhere in this chain.]

A bit of speculation: I think the issue I run into here is that the boundaries between the packages -- where the logical breaks between concepts are _for developers_ -- don't line up with where the logical breaks between concepts are for me, as a user. There's no one package that owns the concept of "tuning a hyperparameter" -- tune owns the grid searching, hardhat owns the infrastructure, dials owns the grid construction. As a result, each package's documentation explains the piece of tuning that package covers, but none of them explain the concept as a whole.

### Hidden options can cause delayed failures

This particular issue is more likely my fault -- the behavior I'm about to complain about is documented, and is a valid design for an interface to have, but I got badly bitten by it and want to whinge.

The default arguments to tuning functions -- specifically, those in `control_grid()` -- are set (I think) to create small, efficient objects. This is a sensible goal to have. The problem is that it also makes these objects less useful, in a way that you can't fix after the fact. You'll only learn that your pipeline is broken after already spending a lot of time running computationally intensive tuning code.

To be more specific: say that you tried to tune the same workflow that we used above, but didn't know that you needed to specify that you wanted to save the workflow:

```{r}
tuning <- workflowsets::workflow_map(
    workflow_set,
    "tune_race_anova",
    resamples = resamples,
    grid = 10,
    metrics = yardstick::metric_set(yardstick::rmse)
)
```

If we then want to fit the best model, the obvious choice would be to use `tune::fit_best()`. But if we call it on our tuning results:

```{r}
try(
    tuning$result |> 
        lapply(tune::fit_best, metric = "rmse")
)
```

We get an error. Now, if we had read the documentation for `fit_best()`, we'd see that the documentation for the `x` argument ends by saying "The control option `save_workflow = TRUE` should have been used" -- so this _is_ documented. But if you missed that, or if you're writing your pipeline as you go,^[Meaning you didn't look up the code you needed post-tuning until you were done tuning.] you're just told -- in past tense -- that you did the wrong thing and need to try again, throwing out the computational time you spent tuning the first time around.^[If you're tuning a workflow, I think there _is_ a workaround where you can use `select_best()`, `finalize_workflow()`, `fit()`, and `extract_workflow()` to get your fitted workflow without needing to retune everything here. I'm not entirely sure-- I'm guessing based off of the "Details" and "See also" sections of the `fit_best()` documentation, and this doesn't seem to work with workflowsets.]

Now, the version of this that actually bit me came when I tried to add my tuned models to an ensemble using `add_candidate()`. This function has a similar pattern to `fit_best()`, except it wants you to have saved both your workflow and your predictions:

```{r}
try(
    stacks::stacks() |>
        stacks::add_candidates(tuning)
)
```

And again, this is documented, for instance in the ["define candidate ensemble members"](https://stacks.tidymodels.org/articles/basics.html#define-candidate-ensemble-members) section of one vignette^[Though isn't spelled out in either other vignette, or the README] and in [one of the argument definitions for `add_candidates()`](https://stacks.tidymodels.org/reference/add_candidates.html). But I didn't see it when I skimmed the stacks documentation initially, and as a result I wound up wasting about ~26 hours spent tuning without saving workflows or predictions. The defaults here are good for making your objects smaller, but mean that tidymodels workflows fail _extremely_ slowly. 

Part of the issue with this particular example is that the way these options get set is pretty indirect, and I get a bit lost in the documentation. My tuning here is controlled by `workflowsets::workflow_map()`, which has [five documented arguments](https://workflowsets.tidymodels.org/reference/workflow_map.html): the workflow set, the tuning function to run, a flag for verbosity and a random seed, and then `...`, documented as "Options to pass to the modeling function". The word "control" doesn't appear on this page.

A minor nit is that I don't think of `tune_*` as being _modeling_ functions, but rather _tuning_ functions, so `...` doesn't jump out at me as a place to set my `save_pred` and `save_workflow` arguments. But I think a bigger problem I had is that if I _do_ click over into the documentation for my tuning function, in this case [`finetune::tune_race_anova`](https://finetune.tidymodels.org/reference/tune_race_anova.html), I still don't see obvious places to set these arguments.

Instead, the `tune_*` functions have a `control` argument which itself takes the output of another function. There are a surprising number of these control functions -- at least [`control_race()`](https://finetune.tidymodels.org/reference/control_race.html) and [`control_sim_anneal()`](https://finetune.tidymodels.org/reference/control_sim_anneal.html) from finetune, plus [`control_bayes()`](https://tune.tidymodels.org/reference/control_bayes.html) and [`control_grid()`](https://tune.tidymodels.org/reference/control_grid.html) from tune^[I _think_ it's most fair to not include `control_resamples()` or `control_last_fit()` here, because I _think_ they're not used for tuning, but I don't actually know. A particular oddity is that `control_grid()` is listed in the tune reference as a developer function, but is used throughout the documentation, while `control_last_fit()` is listed as a "function for tuning" and isn't used in any examples.] -- which I _think_ are paired 1:1 with tuning functions.

I'll be honest, I don't actually know what these functions do at a fundamental level,^[Creating a list with a finite set of named elements, with some default values? The print method just says `<x> control object`.] or if they're sharing some plumbing under the hood which makes this indirection much more efficient for the developers. All I know is that I need to go through a decent number of man pages and a bit of guesswork to land on this final chunk of code:

```r
workflowsets::workflow_map(
    workflow_set,
    "tune_race_anova",
    resamples = resamples,
    grid = grid,
    metrics = yardstick::metric_set(yardstick::rmse),
    control = control_race(verbose = TRUE, save_workflow = TRUE, save_pred = TRUE)
  )
```

So, in order to override the default arguments so that I can fit my models or create an ensemble after tuning, I need to pass the new values to `control_race()`, and then pass the output from `control_race()` to `tune_race_anova` by providing it as a named argument `metrics` to `...`. That's a lot of deep magic to learn -- and a lot of concepts to be thinking about at the same time in order to use this function.

In some sense, this is another flavor of "learning one thing requires learning everything". Because a lot of functions that you'd use later in a workflow require specific non-default arguments in earlier steps, you need to know every function you intend on using when you start writing a pipeline. And as understanding the arguments to these functions generally requires understanding multiple other functions in multiple other packages, it becomes difficult to learn the framework as you go, or to think about modeling pipelines in smaller chunks.

## We're still using tidymodels

With those complaints now vented, I wanted to close by saying that the pipeline I was building -- the one that originally motivated this post -- is still using tidymodels, and is going to keep using the framework for the forseeable future. There's a few really nice features of the framework that motivated the switch:

* First, [finetune](https://finetune.tidymodels.org/) is really cool. We've been using finetune to conduct a much more thorough grid search than would be possible if we needed to see every set of hyperparameters through to the end, and to do so faster than we could do the smaller searches we had been using.
* Second, [stacks](https://stacks.tidymodels.org/) is also great. A generic interface to fit ensemble models ([something I've written about before](https://www.mm218.dev/posts/2021/01/model-averaging/)), which easily accepts new models and writes its own predict methods? Sign me up. Wanting to use these two packages then motivated us to use tune, and deal with the pain points I mentioned above.^[We had been hand-rolling our own ensemble code, which was _mostly_ painless. The hardest part initially was writing a predict method that would generate predictions from all of our component models, and then combine those into a final ensemble prediction. The function we wrote to handle this could easily extend to use multiple models from the same package -- if we wanted to use add another LightGBM to the mix, for instance -- but adding new packages required adding new complicated chunks to the predict method. We were interested in making future extensions easier, so stacks seemed like an obvious choice -- but our home-rolled grid search code was a lot more straightforward than tune, so we were initially resistant to making the jump.]
* Third, even before we adopted the full tidymodels framework, we were already using packages like [rsample](https://rsample.tidymodels.org/) and [yardstick](https://yardstick.tidymodels.org/) (not to mention my extensions for these packages, [spatialsample](https://spatialsample.tidymodels.org/) and [waywiser](https://docs.ropensci.org/waywiser/)^[The waywiser package was originally developed explicitly for the NYS carbon mapping project, as it happens; we use just about every function in waywiser as part of our assessment process.]) to assess our models. We've always found a number of individual tidymodels packages to be extremely useful; most of our pain points (and most of the complaints above) come when we need to hand things off between two packages.
* And finally, I do think our final pipeline here is more readable, and will likely be more maintainable moving forward. Only time will tell, of course, but I'd rather be reading code that included a lot of `tune_race_anova()` and `boost_tree()` than the model-specific interfaces and nested `lapply()` calls that it replaced.

So given all that, we're still using the tidymodels framework. By my estimation, the benefits have been worth the growing pains so far, and with our modeling pipelines now operational we're hoping that we're mostly finished with the worst of the surprises. We're just making extra sure to highlight the bandaids we've had to add to the pipeline, wherever we got caught on a rough edge -- and hopefully this post helps flag a few of them for you as well!