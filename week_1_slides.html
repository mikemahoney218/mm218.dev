<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MLCA Week 1:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Mike Mahoney" />
    <meta name="date" content="2021-03-30" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"xcde59eef1834b49956f54eef08195d6","expires":1}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <script src="libs/js-cookie/js.cookie.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":true}) })</script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# MLCA Week 1:
## What Are We Doing Here?
### Mike Mahoney
### 2021-03-30

---


















class: center, middle

# Syllabus

All content for this class lives on the course website: 
[https://mlca.mm218.dev/](https://mlca.mm218.dev/)

This includes the syllabus, schedule, course handouts, and weekly
assignments.

---
class: middle

Each week you should expect:

* A course handout (these slides) which you should read through before that 
  week's class.
* A one-hour class period for discussing and answering any questions you have
  about the topic of the week.
* A short application-focused R assignment, due the following class.
  
&lt;br /&gt;
  
There is no assignment due this week; Assignment 1 (described at the end of this
handout) is due by the start of class 2.

&lt;br /&gt;

Each assignment is worth the same number of points. Course grades are based 
solely on assignments.

---

class: center, middle, inverted

# Notes

---

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[

There are two cultures in the use of statistical modeling to reach conclusions 
from data. One assumes that the data are generated by a given stochastic data 
model. The other uses algorithmic models and treats the data mechanism as 
unknown. The statistical community has been committed to the almost exclusive 
use of data models. This commitment has led to irrelevant theory, questionable 
conclusions, and has kept statisticians from working on a large range of 
interesting current problems.

.tr[
— [Leo Breiman: Statistical Modeling: The Two Cultures.](https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full)
]
]

---

# We're going to spend the next thirteen weeks talking about a lot of different models.

--
&lt;br /&gt;
&lt;br /&gt;
# So: What _is_ a model, anyway?

---

# Imagine you work with plants.

&lt;br /&gt;

You're interested in how sunlight impacts plant growth. 

&lt;br /&gt;

You decide to run a simple experiment. You: 

* collect a number of plants,
* give them different amounts of sunlight throughout the day,
* and measure how fast they all grow.

&lt;br /&gt;

You apply for a grant with a local funding agency. They love the idea, but the 
chairperson is convinced that speaking to plants is more important for growth
than sunlight. 

You promise to record how much noise each plant is exposed to in 
exchange for a million dollars.

---

# You collect data.

It looks like this:



.panelset[
.panel[.panel-name[Table]


```r
head(plants)
```

```
##   sunlight    growth     noise
## 1        1 -1.241903 0.5468262
## 2        2  1.079290 0.6623176
## 3        3  9.234833 0.1716985
## 4        4  4.282034 0.6330554
## 5        5  5.517151 0.3118697
## 6        6 12.860260 0.7245543
```
]

.panel[.panel-name[Growth ~ Sunlight]

![](week_1_slides_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

]

.panel[.panel-name[Growth ~ Noise]

![](week_1_slides_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

]

.panel[.panel-name[Noise ~ Sunlight]

![](week_1_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

]
]

--

&lt;br /&gt;
Just by looking at the data, you can already tell that plants grow faster when
they get more sun, and noise doesn't seem to be related at all!

But "just looking" doesn't get you a million bucks, so you decide to fit a 
model.

---
class: middle

In R, you fit a linear regression:


```r
plants_model &lt;- lm(growth ~ sunlight + noise, data = plants)
plants_model
```

```
## 
## Call:
## lm(formula = growth ~ sunlight + noise, data = plants)
## 
## Coefficients:
## (Intercept)     sunlight        noise  
##      0.9792       0.9343      -0.1952
```

&lt;br /&gt; 

Which you might have seen written in other classes as:

&lt;br /&gt; 

`$$\operatorname{growth} = \alpha + \beta_{1}(\operatorname{sunlight}) + \beta_{2}(\operatorname{noise}) + \epsilon$$`

Where `\(\alpha\)` is the intercept, representing plant growth if both sunlight and
noise were 0, `\(\beta\)` the coefficients, representing how strongly sunlight and
noise are related to plant growth, and `\(\epsilon\)` some amount of error that your
variables can't explain.

---

# This is a model.

&lt;br /&gt;

At the simplest level, a model is an explicit hypothesis about how some 
combination of variables relate to an outcome of interest. 

&lt;br /&gt;

Our equation tells a story of how we think `\(\operatorname{growth}\)` 
is connected to `\(\operatorname{sunlight}\)` and `\(\operatorname{noise}\)` -- as 
sunlight increases, growth increases a lot; as noise increases, growth decreases
a little.

&lt;br /&gt;

Models are stories we tell about how the world works. They're helpful tools to 
try and collapse complex systems down into shapes and symbols our brains can 
understand.

---

That tool can be used in a lot of different ways.

For instance, imagine if we wanted to know what was more important for plant 
growth, sunlight or noise. We could look at our model statistics:


```r
summary(plants_model)
```

```
## 
## Call:
## lm(formula = growth ~ sunlight + noise, data = plants)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.5490 -2.0789 -0.2725  2.3269  9.0540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.9792     0.8579   1.141    0.256    
## sunlight      0.9343     0.0472  19.796   &lt;2e-16 ***
## noise        -0.1953     1.0984  -0.178    0.859    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.578 on 117 degrees of freedom
## Multiple R-squared:  0.7701,	Adjusted R-squared:  0.7662 
## F-statistic:   196 on 2 and 117 DF,  p-value: &lt; 2.2e-16
```

---

When we're looking to associate a cause with its effect, we're doing 
**attribution**. 

Attribution tends to look at effect sizes and statistical significance to make 
claims around causality, and cares a lot about making parsimonious models.


```r
summary(plants_model)
```

```
## 
## Call:
## lm(formula = growth ~ sunlight + noise, data = plants)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.5490 -2.0789 -0.2725  2.3269  9.0540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.9792     0.8579   1.141    0.256    
## sunlight      0.9343     0.0472  19.796   &lt;2e-16 ***
## noise        -0.1953     1.0984  -0.178    0.859    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.578 on 117 degrees of freedom
## Multiple R-squared:  0.7701,	Adjusted R-squared:  0.7662 
## F-statistic:   196 on 2 and 117 DF,  p-value: &lt; 2.2e-16
```

---

So we look at our results and determine that noise has absolutely nothing to do
with plant growth, and we decide to take it out. We refit our model:


```r
smaller_plants_model &lt;- lm(growth ~ sunlight, plants)
```

And now it looks like this:


```r
summary(smaller_plants_model)
```

```
## 
## Call:
## lm(formula = growth ~ sunlight, data = plants)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.5411 -2.0369 -0.3238  2.3366  9.1080 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.88495    0.67145   1.318     0.19    
## sunlight     0.93415    0.04699  19.879   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.563 on 118 degrees of freedom
## Multiple R-squared:  0.7701,	Adjusted R-squared:  0.7681 
## F-statistic: 395.2 on 1 and 118 DF,  p-value: &lt; 2.2e-16
```

---

This is attribution in action.

We could use this model (built on experimental data) to claim that sunlight 
_causes_ plant growth based on its p value and effect size.


```r
summary(smaller_plants_model)
```

```
## 
## Call:
## lm(formula = growth ~ sunlight, data = plants)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.5411 -2.0369 -0.3238  2.3366  9.1080 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.88495    0.67145   1.318     0.19    
## sunlight     0.93415    0.04699  19.879   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.563 on 118 degrees of freedom
## Multiple R-squared:  0.7701,	Adjusted R-squared:  0.7681 
## F-statistic: 395.2 on 1 and 118 DF,  p-value: &lt; 2.2e-16
```

---

We can use that model to tell us what value of growth we could expect at every
single value of sunlight there could ever be. If we graphed that relationship, 
we'd get a line like this:


```r
ggplot(plants, aes(sunlight, growth)) + 
  geom_smooth(method = "lm", formula = y ~ x) + 
  theme(axis.text = element_blank())
```

![](week_1_slides_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

---

If we plotted our data on top of that, we'd see the expected relationship is a 
pretty good match for what we measured, give or take some noise (from variables
we didn't measure, or error):


```r
ggplot(plants, aes(sunlight, growth)) + 
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point() + 
  theme(axis.text = element_blank())
```

![](week_1_slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
---

This process -- of trying to model the true relationship between variables from
the noisy one we can capture in the real world -- is called **estimation**.

Estimation tries to maximize the goodness-of-fit of the model, most commonly 
using `\(R^2\)`. Parsimony tends to be important here, too.


```r
ggplot(plants, aes(sunlight, growth)) + 
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point() + 
  theme(axis.text = element_blank())
```

![](week_1_slides_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

---
class: middle

Estimation and attribution are two sides of the same coin: 

* estimation aims to determine how some outcome varies over the different levels
  of some input variables, while
* attribution aims to determine which of those variables are causing the 
  variation (and which are only correlated).

&lt;br /&gt;

This course is about prediction, which is a different coin. 

If estimation and attribution are different sides of a penny, prediction is 
maybe a dime.

---
class: center middle

# Prediction

---
class: middle

At the simplest level, prediction is about trying to guess a _specific_ value 
`\(Y\)` given some number of known values `\((X)\)` -- in our example, trying to guess 
unknown `\(\operatorname{growth}\)` values based on measured 
`\(\operatorname{sunlight}\)`.

&lt;br /&gt; 

Where estimation and attribution are concerned with parsimony, significance, and
goodness-of-fit, prediction models care about one thing: **accuracy**.

&lt;br /&gt;

The best prediction model is the one that makes the best predictions -- that has
the lowest rate of errors out of all the models you're considering. 

---

Let's talk a little about what that means. For instance, you can do prediction 
with models designed for other purposes. 

For instance, we can predict plant growth using our simple linear model:


```r
plants$pred &lt;- predict(smaller_plants_model, plants)

ggplot(plants, aes(sunlight, growth)) + 
  geom_point() + 
  geom_point(aes(y = pred), color = "red") + 
  theme(axis.text = element_blank())
```

![](week_1_slides_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
---

Our predictions fall right along that line of best fit. 

In the world of the linear model, "predicting" is the same thing as estimation.
We're estimating how our response varies with our predictors -- here, we're
saying it increases linearly.


```r
plants$pred &lt;- predict(smaller_plants_model, plants)

ggplot(plants, aes(sunlight, growth)) + 
  geom_point() + 
  geom_point(aes(y = pred), color = "red") + 
  theme(axis.text = element_blank())
```

![](week_1_slides_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---

But with prediction, the question we're asking is "what's the value of 
`\(\operatorname{growth}\)`?"

In an ideal world, our predictions would look like this:


```r
ggplot(plants, aes(sunlight, growth)) + 
  geom_point() + 
  geom_point(color = "red", shape = 21, fill = NA) + 
  theme(axis.text = element_blank())
```

![](week_1_slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

Knowing the underlying relationship between our variables isn't the goal when 
we're assessing predictions. The only thing that matters is accuracy.

---

We can assess accuracy in a few different ways -- one of the most common being
the root mean square error, expressed in algebra as:

&lt;br /&gt;

`$$\operatorname{rmse} = \sqrt{(\frac{1}{n})\sum_{i=1}^{n}(\hat{y} - y_{i})^{2}}$$`

&lt;br /&gt;

Or in words as "the square root of the mean of the squared errors", or in code as:


```r
sqrt(mean((plants$pred - plants$growth)^2))
```

```
## [1] 3.533489
```

&lt;br /&gt;

Our linear model has an RMSE of about 3.5, measured in the same units as our 
made-up "growth" measurement. If all your predictions are exactly correct, your
RMSE is 0.

---
class: middle

The title of this course is Machine Learning for Prediction: Concepts and Applications in R.

&lt;br /&gt;

"Machine learning" (ML) is a broad term that refers to any program which can use 
input data to make predictions, which is a definition that includes things like
linear regression.

&lt;br /&gt;

People don't usually mean linear regression when they say ML. What they usually 
mean, and what 75% of this class will focus on, are algorithms that don't bother 
with estimation or attribution at all -- which try to predict a value without 
worrying about the true relationship between your variables.

---
class: middle

# These are pure prediction algorithms.

These tools are extremely powerful, and are easier to use than you'd expect. But 
while they walk like traditional statistics and quack like traditional 
statistics, they work in a different way than the majority of statistics used in 
environmental science. 

By making use of weak correlations and almost completely ignoring estimation and 
attribution, these methods are able to optimize entirely for predictive 
accuracy. 

The end result is more powerful models and more accurate predictions than we've 
ever been able to make before. 


---
class: middle

But this power is not without limits, and trying to apply traditional thinking 
-- about estimation, attribution, parsimony or significance -- is risky business 
at best.

&lt;br /&gt;

Pure prediction algorithms do not care about statistical significance. 

&lt;br /&gt;

They don't expect parsimony; in fact, getting rid of weakly-correlated variables 
will often make your predictions worse. 

&lt;br /&gt;

And, most importantly, they don't pretend to search for any long-term, provable, 
scientific truth. 

&lt;br /&gt;

We'll spend the rest of the semester talking about what exactly that means.

---
.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[

This was not meant to be an “emperor has no clothes” kind of story, rather “the 
emperor has nice clothes but they’re not suitable for every occasion.” Where 
they are suitable, the pure prediction algorithms can be stunningly successful. 
When one reads an enthusiastic AI-related story in the press, there’s usually
one of these algorithms, operating in enormous scale, doing the heavy lifting. 
Regression methods have come a long and big way since the time of Gauss.

.tr[
— [Bradley Efron: Prediction, Estimation, and Attribution.](https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1762613?journalCode=uasa20)
]
]

---
class: middle center title-slide

# Assignment 1

(Due week 2)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
